<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-04-05T16:37:26-07:00</updated><id>http://localhost:4000/</id><title type="html">Ashok R. Dinasarapu</title><subtitle>personal description</subtitle><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><entry><title type="html">Building a real-time big data pipeline (part 1: Kafka, Zookeeper)</title><link href="http://localhost:4000/posts/2020/01/blog-post-kafka/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 1: Kafka, Zookeeper)" /><published>2020-04-05T00:00:00-07:00</published><updated>2020-04-05T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/01/blog-kafka</id><content type="html" xml:base="http://localhost:4000/posts/2020/01/blog-post-kafka/">&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org&quot;&gt;Kafka&lt;/a&gt; is used for building real-time data pipelines and streaming apps.&lt;/p&gt;

&lt;p&gt;What is Kafka? &lt;a href=&quot;https://success.docker.com/article/getting-started-with-kafka&quot;&gt;Getting started with kafka&lt;/a&gt; says &lt;em&gt;Kafka is a distributed append log; in a simplistic view it is like a file on a filesystem. Producers can append data (echo ‘data’ » file.dat), and consumers subscribe to a certain file (tail -f file.dat)&lt;/em&gt;. In addition, Kafka provides an ever-increasing counter and a timestamp for each consumed message. Kafka uses Zookeeper to store metadata about producers, topics and partitions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kafka for local development of applications&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;There are multiple ways of running Kafka locally for development of apps but the easiest method is by &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose&lt;/code&gt;. To download Docker Desktop, go to &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt; and Sign In with your Docker ID.&lt;/p&gt;

&lt;p&gt;Docker compose facilitates installing &lt;code class=&quot;highlighter-rouge&quot;&gt;Kafka&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Zookeeper&lt;/code&gt; with the help of &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '2'  
services:  
  zookeeper:  
    image: wurstmeister/zookeeper  
    ports:  
      - &quot;2181:2181&quot;  
  kafka:  
   image: wurstmeister/kafka  
    ports:  
      - &quot;9092:9092&quot;  
    environment:  
     KAFKA_ADVERTISED_HOST_NAME: localhost  
     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  
     KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;true&quot;  
    volumes:  
     - /var/run/docker.sock:/var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;1. Start the Kafka service&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Open a terminal, go to the directory where you have the &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file, and execute the following command. This command starts the docker-compose engine, and it downloads the images and runs them.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose up -d  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Starting kafka_example_zookeeper_1 ... done  
Starting kafka_example_kafka_1     ... done  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To list running docker containers, run the following command&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose ps  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Name				Command				State	Ports  
kafka_example_kafka_1		start-kafka.sh			Up	0.0.0.0:9092-&amp;gt;9092/tcp                              
kafka_example_zookeeper_1	/bin/sh -c /usr/sbin/sshd  ...	Up	0.0.0.0:2181-&amp;gt;2181/tcp, 22/tcp, 2888/tcp, 3888/tcp  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can shut down docker-compose by executing the following command in another terminal.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose down  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stopping kafka_example_zookeeper_1 ... done  
Stopping kafka_example_kafka_1     ... done  
Removing kafka_example_zookeeper_1 ... done  
Removing kafka_example_kafka_1     ... done  
Removing network kafka_example_default  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using the following command check the ZooKeeper logs to verify that ZooKeeper is working and healthy.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose logs zookeeper | grep -i binding  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next, check the Kafka logs to verify that broker is working and healthy.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose logs kafka | grep -i started  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. Create a Kafka topic&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Kafka cluster stores streams of records in categories called topics. Each record in a topic consists of a key, a value, and a timestamp. A topic can have zero, one, or many consumers that subscribe to the data written to it.&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose exec&lt;/code&gt; to execute a command in a running container. For example, &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose exec&lt;/code&gt; command by default allocates a TTY, so that you can use such a command to get an interactive prompt. Go into directory where &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file present, and execute it as&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose exec kafka bash  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;(for zookeeper &lt;code class=&quot;highlighter-rouge&quot;&gt;$docker-compose exec zookeeper bash&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Change the directory to /opt/kafka/bin where you find scripts such as &lt;code class=&quot;highlighter-rouge&quot;&gt;kafka-topics.sh&lt;/code&gt;.&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd /opt/kafka/bin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create, list or delete&lt;/strong&gt; existing topics:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \  
 --create \  
 --topic mytopic \  
 --partitions 1 \  
 --replication-factor 1 \  
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Figure 1&lt;/b&gt;. Kafka topic partitions layout (&lt;a href=&quot;https://cloudblogs.microsoft.com/opensource/2018/07/09/how-to-data-processing-apache-kafka-spark/&quot;&gt;cloudblogs.microsoft.com&lt;/a&gt;). Each partition in a topic is an ordered, immutable sequence of records that is continually appended to a structured commit log.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-partitions.png&quot; alt=&quot;Partitions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-partition.png&quot; alt=&quot;Partition&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \
 --list \
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If necessary, delete a topic using the following command.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \
 --delete \
 --topic mytopic \
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3. Kafka Producer and Consumer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A Kafka producer is an object that consists of a pool of buffer space that holds records that haven’t yet been transmitted to the server.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Figure 2&lt;/b&gt;. Relationship between &lt;a href=&quot;https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a&quot;&gt;Kafka Components&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-producer-consumer.png&quot; alt=&quot;Producer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka broker (a.k.a Kafka server/node) is the server node in the cluster, mainly responsible for hosting partitions of Kafka Topics, transferring messages from Kafka Producer to Kafka Consumer and, providing data replication and partitioning within a Kafka Cluster.&lt;/p&gt;

&lt;p&gt;The following is a producer command line to read data from standard input and write it to a Kafka Topic.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-producer.sh \
 --broker-list localhost:9092 \
 --topic mytopic
  
&amp;gt;Hello  
&amp;gt;World  
^C
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The following is a command line to read data from a Kafka topic and write it to standard output.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-consumer.sh \
 --bootstrap-server localhost:9092 \
 --topic mytopic \
 --from-beginning
  
Hello  
World  
^CProcessed a total of 2 messages  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Another way of reading data from a Kafka topic is by simply using a Spring Boot application&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The following demonstrates how to receive messages from Kafka Topic. First in this blog I create a Spring Kafka Consumer, which is able to listen the messages sent to a Kafka Topic. Then I create a Spring Kafka Producer, which is able to send messages to a Kafka Topic.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Figure 3&lt;/b&gt;. Kafka Producer and Consumer in Java (&lt;a href=&quot;https://blog.clairvoyantsoft.com/benchmarking-kafka-e7b7c289257d&quot;&gt;blog.clairvoyantsoft.com&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-producer-consumer-java.png&quot; alt=&quot;Java&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first step to create a simple &lt;strong&gt;Spring Boot Maven Application&lt;/strong&gt; is &lt;a href=&quot;https://spring.io/guides/gs/spring-boot/&quot;&gt;Starting with Spring Initializr&lt;/a&gt; and make sure to have spring-kafka dependency to &lt;code class=&quot;highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;  
  &amp;lt;groupId&amp;gt;org.springframework.kafka&amp;lt;/groupId&amp;gt;  
  &amp;lt;artifactId&amp;gt;spring-kafka&amp;lt;/artifactId&amp;gt;  
&amp;lt;/dependency&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SpringBootKafkaApplication.java&lt;/code&gt; class (The Spring Initializr creates the following simple application class for you)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@SpringBootApplication  
public class SpringBootKafkaApplication {  
	public static void main(String[] args) {  
		SpringApplication.run(SpringBootApplication.class, args);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Configure Kafka through application.yml configuration file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Spring Boot, properties are kept in the &lt;code class=&quot;highlighter-rouge&quot;&gt;application.properties&lt;/code&gt; file under the classpath. The application.properties file is located in the &lt;code class=&quot;highlighter-rouge&quot;&gt;src/main/resources&lt;/code&gt; directory. Change application.properties file to &lt;code class=&quot;highlighter-rouge&quot;&gt;application.yml&lt;/code&gt;, then add the following content.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spring:  
  kafka:  
consumer:  
  bootstrap-servers: localhost:9092  
  group-id: group_test1  
  auto-offset-reset: earliest  
  key-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
  value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a Spring Kafka Consumer class&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Create a class called &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaConsumer.java&lt;/code&gt; and add a method with the @KakfaListener annotation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Service  
public class KafkaConsumer {  
	@KafkaListener(id = &quot;group_test1&quot;, topics = &quot;mytopic&quot;)  
	public void consumeMessage(String message) {  
		System.out.println(&quot;Consumed message: &quot; + message);  
	}  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;How to run Spring Boot web application in Eclipse?&lt;/p&gt;

&lt;p&gt;In eclipse Project Explorer, right click the project name -&amp;gt; select “Run As” -&amp;gt; “Maven Build…”&lt;br /&gt;
In the goals, enter &lt;code class=&quot;highlighter-rouge&quot;&gt;spring-boot:run&lt;/code&gt;&lt;br /&gt;
then click Run button.&lt;/p&gt;

&lt;p&gt;If you have Spring Tool Suite (STS) plug-in, you see a “Spring Boot App” option under Run As.&lt;/p&gt;

&lt;p&gt;Start Kafka and Zookeeper: &lt;code class=&quot;highlighter-rouge&quot;&gt;$docker-compose up -d&lt;/code&gt;&lt;br /&gt;
Produce message using the Kafka console producer: &lt;code class=&quot;highlighter-rouge&quot;&gt;$docker-compose exec kafka bash&lt;/code&gt;&lt;br /&gt;
Once inside the container &lt;code class=&quot;highlighter-rouge&quot;&gt;cd /opt/kafka/bin&lt;/code&gt;&lt;br /&gt;
Run the following console producer which will enable you to send messages to Kafka:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-producer.sh \
 --broker-list localhost:9092 \
 --topic mytopic
  
&amp;gt;Hello  
&amp;gt;World  
^C  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Try sending a few messages like above (Hello, World etc) and watch the application standard output in the Eclipse shell where you are running your Spring Boot application.&lt;/p&gt;

&lt;p&gt;Eclipse Console:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  .   ____          _            __ _ _  
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \  
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \  
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )  
  '  |____| .__|_| |_|_| |_\__, | / / / /  
 =========|_|==============|___/=/_/_/_/  

:: Spring Boot ::        (v2.2.4.RELEASE)  
2020-01-26 14:26:55.205  INFO 11137 --- [           main] c.e.d.SpringBootKafkaConsumerApplication : Starting   
…  
…  
…
o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=simpleconsumer] Setting newly assigned partitions: test1-0  
2020-01-26 14:26:56.384  INFO 11137 --- [econsumer-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=simpleconsumer] Found no committed offset for partition test1-0  
2020-01-26 14:26:56.408  INFO 11137 --- [econsumer-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-1, groupId=simpleconsumer] Resetting offset for partition test1-0 to offset 2.  
2020-01-26 14:26:56.477  INFO 11137 --- [econsumer-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : simpleconsumer: partitions assigned: [test1-0]  
Got message: hello  
Got message: world  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The following code demonstrates how to send and receive messages from Kafka topic. The above &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaConsumer.java&lt;/code&gt; receives messages that were sent to a Kafka topic. The followng &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaProducer.java&lt;/code&gt; send messages to a Kafka topic.&lt;/p&gt;

&lt;p&gt;Make sure to have spring-web dependency to &lt;code class=&quot;highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;  
  &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;  
  &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;  
&amp;lt;/dependency&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Add two new java classes &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaController.java&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaProducer.java&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@RestController  
@RequestMapping(value=&quot;/kafka&quot;)  
public class KafkaController {  
	private final KafkaProducer producer;  

	@Autowired  
	KafkaController(KafkaProducer producer){  
		this.producer = producer;  
	}  

	@PostMapping(value=&quot;/publish&quot;)  
	public void messagePrint(@RequestParam(value=&quot;message&quot;, required = false) String message) {  
		this.producer.sendMessage(message);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Service  
public class KafkaProducer {  
	private static final String TOPIC = &quot;mytopic&quot;;  

	@Autowired  
	private KafkaTemplate&amp;lt;String, String&amp;gt; kafkaTemplate;  
	
	public void sendMessage(String message) {  
		kafkaTemplate.send(TOPIC, message);  
		System.out.println(&quot;Produced message: &quot; + message);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Update &lt;code class=&quot;highlighter-rouge&quot;&gt;application.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server:
  port: 8080
spring:
  kafka:
   consumer:
    bootstrap-servers: localhost:9092
    group-id: group_test1
    auto-offset-reset: earliest
    key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
   producer:  
    bootstrap-servers: localhost:9092  
    key-serializer: org.apache.kafka.common.serialization.StringSerializer  
    value-serializer: org.apache.kafka.common.serialization.StringSerializer  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Run Spring Boot web application (see How to run Spring Boot web application in Eclipse?)&lt;/p&gt;

&lt;p&gt;Make POST request using &lt;a href=&quot;https://www.getpostman.com&quot;&gt;Postman&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Select &lt;code class=&quot;highlighter-rouge&quot;&gt;POST&lt;/code&gt; and use the API &lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:8080/kafka/publish&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;Body&lt;/strong&gt;: form-data  &lt;strong&gt;KEY&lt;/strong&gt;: message  &lt;strong&gt;VALUE&lt;/strong&gt;: hello&lt;/p&gt;

&lt;p&gt;Finally click &lt;strong&gt;send&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;See Eclipse Console for messages:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;...  
...  
...  
2020-01-27 13:12:06.911  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.3.1  
2020-01-27 13:12:06.912  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 18a913733fb71c01  
2020-01-27 13:12:06.912  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1580148726911  
2020-01-27 13:12:06.947  INFO 31822 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: 6R8O95IPSfGoifR4zzwM6g  
Produced message: hello  
Consumed message: hello
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="big data" /><category term="apache kafka" /><category term="real time data pipelines" /><category term="java" /><category term="docker" /><category term="Spring Boot" /><category term="YAML" /><category term="Zookeeper" /><category term="Bioinformatics" /><category term="Emory University" /><summary type="html">Kafka is used for building real-time data pipelines and streaming apps.</summary></entry><entry><title type="html">Quantitative proteomics: TMT-based quantitation of proteins</title><link href="http://localhost:4000/posts/2020/01/blog-post-tmt/" rel="alternate" type="text/html" title="Quantitative proteomics: TMT-based quantitation of proteins" /><published>2020-04-04T00:00:00-07:00</published><updated>2020-04-04T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/01/blog-tmt</id><content type="html" xml:base="http://localhost:4000/posts/2020/01/blog-post-tmt/">&lt;p&gt;Quantification of proteins using isobaric labeling (tandem mass tag or TMT) starts with the reduction of disulfide bonds in proteins with Dithiothreitol (DTT).&lt;/p&gt;

&lt;p&gt;Alkylation with iodoacetamide (IAA) after cystine reduction results in the covalent addition of a carbamidomethyl group that prevents the formation of disulfide bonds. Then, overnight digestion of the proteins using trypsin or trypsin/LyC mixture, Tandem Mass Tag (TMT) labeling on Lysine residues, pooling of all samples and the fractionation of peptide mixture. Finally, LC-MS/MS data acquisition and database search for protein identification and quantification.&lt;/p&gt;

&lt;p&gt;Fractionation prior to LC-MS/MS analysis effectively detects low abundance proteins (&amp;lt;100ng/mL), which is the concentration range of most clinical biomarkers.&lt;/p&gt;

&lt;p&gt;TMT-based approach allows multiplexing of samples. TMT 6-plex reagents produces a series of different reporter ions with nominal masses from 126 to 131 Da at 1 Da intervals. Several TMT reagents are commercially available including TMTzero, TMT duplex, TMT 6-plex, and TMT 10-plex &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. They have the same chemical structure but contain different numbers and combinations of 13C and 15N isotopes in the reporter group. The overall calibrated intensities of reporter ions equal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Isobaric labeling reagent (TMT) structure&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;a. An amine-specific reactive group – an N-hydroxysuccinimide ester, which reacts with primary amines i.e unblocked N-terminals and lysine side chains.&lt;br /&gt;
b. A mass reporter group for quantification – the reporter groups are partially fragmented from the peptide during precursor fragmentation in the mass spectrometer.&lt;br /&gt;
c. A mass normalizer group to link the reactive and reporter groups – mass normalizer group ensures that the peptide complexity in the MS1 spectra does not increase with multiplexing.&lt;/p&gt;

&lt;p&gt;Unless otherwise noted, every analysis utilizes an MS3-based TMT-centric mass spectrometry method. MS2-based TMT yields the highest precision but lowest accuracy due to ratio compression, which MS3-based TMT can partly rescue &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. To 10 precursors selected for MS2/MS3 analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are the correction factors used for TMT?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TMT reporter ion signals need to be adjusted to account for isotopic impurities in each TMT variant.  For TMT-10plex labeling, different batches of TMT reagents have slightly different isotope impurities that need to be included for database search to correct the reporter ion ratio. The information of isotope impurity can be found in the reagent kit. However, I do not normally correct for isotopic impurities of TMT reagents, as this is typically a «1.2% shift.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Database search for protein identification and quantification&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The resulting TMT-MS3 data (.raw files) are processed using MaxQuant with an integrated Andromeda search engine (v.1.6.7.0). Tandem mass spectra were searched against the Uniprot database.&lt;/p&gt;

&lt;p&gt;Download and install  &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src/master/&quot;&gt;MaxQuant&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Specification of Various Parameters in MaxQuant (All the other parameters in MaxQuant are set to the default values for processing orbitrap-type data)&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw data&lt;/strong&gt; pane&lt;br /&gt;
Using Load, select all raw files from all batches. Update Experiment (a unique value for each of the samples) and Fraction (if available) column values using set experiment and set fractions buttons.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Group-specific parameters&lt;/strong&gt; pane&lt;br /&gt;
Change Type as Reporter ion MS3 and then select “10plex TMT”. Update isobaric impurities values, if available. For Modifications, select Variable modifications as Oxidation (M), Acetyl (Protein N-term); Deamidation (NQ) and Fixed Modifications as Carbamidomethyl© and, TMT labeled N- terminus and lysine residue. Specify Trypsin/P as a cleavage enzyme using Digestion and allow up to 2 missing cleavages. Set Label-free quantification as None.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global parameters&lt;/strong&gt; pane&lt;br /&gt;
For Sequences select a reference proteome database Uniprot FASTA file and update Max. peptide mass [Da] as 6000. For Protein quantification select Use only unmodified peptides and a list of modifications such as Oxidation (M), Acetyl (Protein N-term) and Deamidation. Select Match between runs in Identification tab.&lt;/p&gt;

&lt;p&gt;Update the following two parameters for MS/MS analyzer:&lt;br /&gt;
a. FTMS MS/MS match tolerance: 0.05 Da&lt;br /&gt;
b. ITMS MS/MS match tolerance: 0.6 Da&lt;/p&gt;

&lt;p&gt;For Folder location select tmp directory (optional).&lt;/p&gt;

&lt;p&gt;Further data processing is performed using &lt;code class=&quot;highlighter-rouge&quot;&gt;Perseus v1.6.12.0&lt;/code&gt; &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. The search results in &lt;code class=&quot;highlighter-rouge&quot;&gt;ProteinGroups.txt&lt;/code&gt; generated by MaxQuant are directly processed by Perseus software. MaxQuant reports the TMT-MS3 quantitative relative abundance metrics in the columns titled “Reporter intensity corrected”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;br /&gt;
Data normalization and analysis in multiple TMT experimental designs &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
Multiplexed Protein Quantification Using the Isobaric TMT … &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
Isobaric matching between runs and novel PSM-level normalization in MaxQuant … &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;AWS Windows instance for &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src&quot;&gt;MaxQuant/Perseus&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Bąchor, R.; Waliczek, M.; Stefanowicz, P.; Szewczuk, Z. Trends in the Design of New Isobaric Labeling Reagents for Quantitative Proteomics. Molecules 2019, 24, 701.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;A. Hogrebe, L. von Stechow, D.B. Bekker-Jensen, B.T. Weinert, C.D. Kelstrup, J.V. Olsen Benchmarking common quantification strategies for large-scale phosphoproteomics Nat. Commun., 9 (2018), p. 1045.&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=maxquant:start&quot;&gt;MaxQuant&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=perseus:start&quot;&gt;Perseus&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://pwilmart.github.io/TMT_analysis_examples/multiple_TMT_MQ.html&quot;&gt;TMT Batch Correction&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://assets.thermofisher.com/TFS-Assets/CMD/Reference-Materials/PP-TMT-Multiplexed-Protein-Quantification-HUPO2015-EN.pdf&quot;&gt;TMT analysis using Proteome Discoverer&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://doi.org/10.1101/2020.03.30.015487&quot;&gt;Isobaric matching between runs and novel PSM-level normalization&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="Proteomics" /><category term="TMT" /><category term="tandem mass tag" /><category term="MS3 spectra" /><category term="MaxQuant" /><category term="isobaric labeling" /><category term="Perseus" /><category term="TMT 10plex" /><category term="10plex" /><category term="Bioinformatics" /><category term="Emory University" /><summary type="html">Quantification of proteins using isobaric labeling (tandem mass tag or TMT) starts with the reduction of disulfide bonds in proteins with Dithiothreitol (DTT).</summary></entry><entry><title type="html">Quantitative proteomics: label-free quantitation of proteins</title><link href="http://localhost:4000/posts/2018/04/blog-post-lfq/" rel="alternate" type="text/html" title="Quantitative proteomics: label-free quantitation of proteins" /><published>2020-04-04T00:00:00-07:00</published><updated>2020-04-04T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/04/blog-lfq</id><content type="html" xml:base="http://localhost:4000/posts/2018/04/blog-post-lfq/">&lt;p&gt;Liquid chromatography (LC) coupled with mass spectrometry (MS) has been widely used for protein expression quantification.&lt;/p&gt;

&lt;p&gt;Protein quantification by tandem-MS (MS/MS) uses integrated peak intensity from the parent-ion mass (MS1) or features from fragment-ions (MS2). MS1 methods use the iBAQ (intensity Based Absolute Quantification) algorithm (a protein’s total non-normalised intensities are divided by the number of measurable tryptic peptides). Untargeted label-free quantitation (LFQ) of proteins, aims to determine the relative amount of proteins in two or more biological samples.&lt;/p&gt;

&lt;p&gt;Mass spectrometer generated &lt;code class=&quot;highlighter-rouge&quot;&gt;raw&lt;/code&gt; files are used for label-free quantitation of proteins. Base peak chromatograms are inspected visually using &lt;code class=&quot;highlighter-rouge&quot;&gt;RawMeat&lt;/code&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, which  is a data quality assessment tool designed for Thermo instruments. All raw files are processed together in a single run by &lt;code class=&quot;highlighter-rouge&quot;&gt;MaxQuant v1.6.14&lt;/code&gt; &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; with defaul parameters except the following&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Raw data&lt;/code&gt; pane&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load all raw data samples of a single run.&lt;/li&gt;
  &lt;li&gt;Select a sample file and edit &lt;code class=&quot;highlighter-rouge&quot;&gt;Set experiment&lt;/code&gt; to assign a unique ID to each sample. If you don´t numerate the biological replicates MaxQuant will put them together in the output.&lt;/li&gt;
  &lt;li&gt;Select a sample file and edit &lt;code class=&quot;highlighter-rouge&quot;&gt;Set fractions&lt;/code&gt; to assign fraction value. If you don’t have a fractionation set all file or samples with 1.&lt;/li&gt;
  &lt;li&gt;Number of processors: 4&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Group-specific parameters&lt;/code&gt; pane&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Type: Standard and Multiplicity: 1&lt;/li&gt;
  &lt;li&gt;Modifications:&lt;br /&gt;
  a. Variable modifications: Oxidation(M); Acetyl (Protein N-term); Deamidation (NQ)&lt;br /&gt;
  b. Fixed modifications: Carbamidomethyl ( C )&lt;/li&gt;
  &lt;li&gt;Digestion: trypsin&lt;/li&gt;
  &lt;li&gt;Instrument: Orbitrap&lt;/li&gt;
  &lt;li&gt;Label-free quantification: LFQ (LFQ min. ratio count: 2)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Global parameters&lt;/code&gt; pane&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sequences:&lt;br /&gt;
 a. Add D:\Proteomics\HUMAN.fasta (from UNIPROT)&lt;br /&gt;
 b. Identifier rule: Uniprot identifier&lt;br /&gt;
 c. Min. peptide length: 6&lt;br /&gt;
 d. Max. peptide mass [Da]: 6000&lt;/li&gt;
  &lt;li&gt;Protein quantification:&lt;br /&gt;
 a. Label min. ratio count: 1&lt;br /&gt;
 b. Peptides for quantification: Unique+razor&lt;br /&gt;
 c. Modifications used in protein quantification: Oxidation(M); Acetyl (Protein N-term); Deamidation (NQ)&lt;br /&gt;
 d. Discard unmodified counterpart peptides: FALSE&lt;/li&gt;
  &lt;li&gt;Tables&lt;br /&gt;
 a. Write msScans tabls: TRUE&lt;/li&gt;
  &lt;li&gt;MS/MS analyzer&lt;br /&gt;
 a. FTMS MS/MS match tolerance: 0.05 Da&lt;br /&gt;
 b. ITMS MS/MS match tolerance: 0.6 Da&lt;/li&gt;
  &lt;li&gt;Identification:&lt;br /&gt;
 a. Match between runs: TRUE&lt;br /&gt;
 b. Find dependent peptides: FALSE&lt;br /&gt;
 c. Razor protein FDR: TRUE&lt;/li&gt;
  &lt;li&gt;Label free quantification&lt;br /&gt;
 a. iBAQ: TRUE&lt;br /&gt;
 b. Separate LFQ in parameter groups: TRUE&lt;/li&gt;
  &lt;li&gt;Folder locations&lt;br /&gt;
 a. Temporary folder: D:\tmp&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Database searches are performed using the &lt;code class=&quot;highlighter-rouge&quot;&gt;Andromeda&lt;/code&gt; search engine (a peptide search engine based on probabilistic scoring) with the UniProt-SwissProt human canonical database as a reference and a contaminants database of common laboratory contaminants. MaxQuant reports summed intensity for each protein, as well as its iBAQ value. Proteins that share all identified peptides are combined into a single protein group. Peptides that match multiple protein groups (“razor” peptides) are assigned to the protein group with the most unique peptides. MaxQuant employs the MaxLFQ algorithm for label-free quantitation (LFQ). Quantification will be performed using razor and unique peptides, including those modified by acetylation (protein N-terminal), oxidation (Met) and deamidation (NQ). &lt;code class=&quot;highlighter-rouge&quot;&gt;PTXQC&lt;/code&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; is used for general quality control of proteomics data, which takes MaxQuant result files.&lt;/p&gt;

&lt;p&gt;Data processing is performed using `Perseus v1.6.12.0)&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. In brief, protein group LFQ intensities are log2-transformed to reduce the effect of outliers. To overcome the obstacle of missing LFQ values, missing values are imputed before fit the models. Hierarchical clustering is performed on Z-score normalized, log2-transformed LFQ intensities. Log ratios are calculated as the difference in average log2 LFQ intensity values between experimental and control groups. Two-tailed, Student’s t test calculations are used in statistical tests. A protein is considered statistically significant if its fold change is ≥ 2 and FDR ≤ 0.01. All the identified differentially expressed proteins are used in protein network or pathway analysis. In addition to the above analytical considerations, good experimental design helps effectively identify true differences in the presence of variability from various sources and also avoids bias during data acquisition.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;AWS Windows instance for &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src&quot;&gt;MaxQuant/Perseus&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proteomicsresource.washington.edu/protocols06/&quot;&gt;RawMeat&lt;/a&gt; is a nice Thermo raw file diagnostic tool developed by the now defunct Vast Scientific&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=maxquant:start&quot;&gt;MaxQuant&lt;/a&gt; is a quantitative proteomics software package designed for analyzing large mass-spectrometric data sets&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/cbielow/PTXQC&quot;&gt;PTXQC&lt;/a&gt;, an R-based quality control pipeline called Proteomics Quality Control&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=perseus:start&quot;&gt;Perseus&lt;/a&gt; is software package for shotgun proteomics data analyses&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="Proteomics" /><category term="LFQ" /><category term="label free quantitation" /><category term="iBAC" /><category term="MaxQuant" /><category term="Perseus" /><category term="Emory University" /><category term="Bioinformatics" /><summary type="html">Liquid chromatography (LC) coupled with mass spectrometry (MS) has been widely used for protein expression quantification.</summary></entry><entry><title type="html">Spatial RNA-seq data analysis using Space Ranger on SGE Cluster</title><link href="http://localhost:4000/posts/2020/03/blog-spatial-gene-expression/" rel="alternate" type="text/html" title="Spatial RNA-seq data analysis using Space Ranger on SGE Cluster" /><published>2020-03-06T00:00:00-08:00</published><updated>2020-03-06T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/03/blog-spatial-expression</id><content type="html" xml:base="http://localhost:4000/posts/2020/03/blog-spatial-gene-expression/">&lt;p&gt;Running &lt;a href=&quot;https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/what-is-space-ranger&quot;&gt;spaceranger&lt;/a&gt; as cluster mode that uses Sun Grid Engine (SGE) as queuing.&lt;/p&gt;

&lt;p&gt;There are 2 steps to analyze Spatial RNA-seq data&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger mkfastq&lt;/code&gt; demultiplexes raw base call (&lt;code class=&quot;highlighter-rouge&quot;&gt;BCL&lt;/code&gt;) files generated by Illumina sequencers into FASTQ files.&lt;br /&gt;
&lt;strong&gt;Step 2&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger count&lt;/code&gt; takes FASTQ files from &lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger mkfastq&lt;/code&gt; and performs alignment, filtering, barcode counting, and UMI counting.&lt;/p&gt;

&lt;p&gt;Running pipelines on cluster requires the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Load Space Ranger module (&lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger-1.0.0&lt;/code&gt;)&lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or, download and uncompress spaceranger at your &lt;code class=&quot;highlighter-rouge&quot;&gt;$HOME&lt;/code&gt; directory and add PATH in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Update job config file (&lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger-1.0.0/external/martian/jobmanagers/config.json&lt;/code&gt;) for threads and memory. For example&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;threads_per_job&quot;: 8,&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;memGB_per_job&quot;: 64,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Update template file (&lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger-1.0.0/external/martian/jobmanagers/sge.template&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -pe smp __MRO_THREADS__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;##$ -l mem_free=__MRO_MEM_GB__G&lt;/code&gt; (comment this line if your cluster do not support it!)&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -q b.q&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -S /bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -m abe&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -M &amp;lt;e-mail&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd __MRO_JOB_WORKDIR__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;source ../spaceranger-1.0.0/sourceme.bash&lt;/code&gt; (update with complete path)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For clusters whose job managers do not support memory requests, it is possible to request memory 
in the form of cores via the &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore&lt;/code&gt; command-line option. This option scales up the number 
of threads requested via the &lt;code class=&quot;highlighter-rouge&quot;&gt;__MRO_THREADS__&lt;/code&gt; variable according to how much memory a stage requires&lt;/em&gt;.&lt;br /&gt;
Read more at &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&quot;&gt;Cluster Mode&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. Download spatial gene expression, image file and reference genome datasets from &lt;a href=&quot;https://www.10xgenomics.com/resources/datasets/&quot;&gt;10XGenomics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;. Create &lt;code class=&quot;highlighter-rouge&quot;&gt;sge.sh&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TR=&quot;$HOME/refdata-cellranger-mm10-3.0.0&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Output files will appear in the out/ subdirectory within this pipeline output directory.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd $HOME/10xgenomics/out&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For pipeline output directory, the &lt;code class=&quot;highlighter-rouge&quot;&gt;--id&lt;/code&gt; argument is used i.e Adult_Mouse_Brain.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/V1_Adult_Mouse_Brain_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spaceranger count --disable-ui \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=Adult_Mouse_Brain \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--sample=V1_Adult_Mouse_Brain \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--image=$DATA_DIR/V1_Adult_Mouse_Brain_image.tif \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--slide=V19L01-041 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--area=C1 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=5000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;. Execute a command in screen and, detach and reconnect&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;screen&lt;/code&gt; command to get in/out of the system while keeping the processes running.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;screen -S screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bash sge.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want to exit the terminal without killing the running process, simply press &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+A+D&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To reconnect to the screen: &lt;code class=&quot;highlighter-rouge&quot;&gt;screen -R screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7&lt;/strong&gt;. Monitor work progress through a web browser&lt;/p&gt;

&lt;p&gt;Open &lt;code class=&quot;highlighter-rouge&quot;&gt;_log&lt;/code&gt; file present in output folder &lt;code class=&quot;highlighter-rouge&quot;&gt;Adult_Mouse_Brain&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see serving UI as &lt;code class=&quot;highlighter-rouge&quot;&gt;http://cluster.university.edu:3600?auth=rlSdT_QLzQ9O7fxEo-INTj1nQManinD21RzTAzkDVJ8&lt;/code&gt;, then type the following from your laptop&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ssh -NT -L 9000:cluster.university.edu:3600 user@cluster.university.edu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;user@cluster.university.edu's password:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then access the UI using the following URL in your web browser
&lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:9000/&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/what-is-space-ranger&quot;&gt;10XGenomics- Visium spatial RNA-seq&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="10XGenomics" /><category term="SPATIAL GENE EXPRESSION" /><category term="Visium Spatial Gene Expression Solution" /><category term="brightfield microscope image" /><category term="Space Ranger" /><category term="SGE Cluster" /><category term="Cluster Computing" /><summary type="html">Running spaceranger as cluster mode that uses Sun Grid Engine (SGE) as queuing.</summary></entry><entry><title type="html">Building a real-time big data pipeline (part 3: Spark, SQL)</title><link href="http://localhost:4000/posts/2020/02/blog-post-spark-sql/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 3: Spark, SQL)" /><published>2020-02-15T00:00:00-08:00</published><updated>2020-02-15T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/02/blog-spark-sql</id><content type="html" xml:base="http://localhost:4000/posts/2020/02/blog-post-spark-sql/">&lt;p&gt;Apache Spark is an open-source cluster computing system that provides high-level API in Java, Scala, Python and R.&lt;/p&gt;

&lt;p&gt;Spark also packaged with higher-level libraries for SQL, machine learning, streaming, and graphs. Spark SQL is Spark’s package for working with structured data &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;1-start-hadoop-copy-a-csv-file-to-hdfs&quot;&gt;1. Start Hadoop, Copy a &lt;code class=&quot;highlighter-rouge&quot;&gt;csv&lt;/code&gt; file to HDFS&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;The Hadoop Distributed File System (HDFS) is the primary data storage system used by Hadoop applications. It employs a &lt;code class=&quot;highlighter-rouge&quot;&gt;NameNode&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;DataNode&lt;/code&gt; architecture to implement a distributed file system that provides high-performance access to data across highly scalable Hadoop clusters&lt;/em&gt;.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd hadoop-3.1.3/
$bash sbin/start-dfs.sh
$hadoop fs -mkdir -p /user/adinasarapu
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;-copyFromLocal&lt;/code&gt; command to move one or more files from local location to HDFS.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$hadoop fs -copyFromLocal *.csv /user/adinasarapu  

$hadoop fs -ls /user/adinasarapu  
-rw-r--r--   1 adinasarapu supergroup     1318 2020-02-13 21:41 /user/adinasarapu/samples.csv  
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-14 09:33 /user/adinasarapu/survey.csv  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;2--read-a-csv-file-into-spark-data-frame&quot;&gt;2.  Read a &lt;code class=&quot;highlighter-rouge&quot;&gt;csv&lt;/code&gt; file into Spark Data Frame&lt;/h2&gt;

&lt;p&gt;A Spark Data Frame can be constructed from an array of data different sources such as Hive tables, Structured Data files (ex.csv), external databases (eg. MySQL), or existing RDDs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Start the Spark&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$spark-shell&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Options used while reading csv file into a Spark DataFrame:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df = spark.read.format(&quot;csv&quot;)
	.option(&quot;header&quot;, &quot;true&quot;)
	.option(&quot;inferSchema&quot;,&quot;true&quot;)
	.option(&quot;nullValue&quot;,&quot;NA&quot;)
	.option(&quot;mode&quot;,&quot;failfast&quot;)
	.load(&quot;hdfs://localhost:9000/user/adinasarapu/samples.csv&quot;)  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;For more details visit &lt;a href=&quot;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&quot;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Select &amp;amp; Filter the Spark DataFrame&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val sel = df.select(&quot;Sample&quot;,&quot;p16&quot;,&quot;Age&quot;,&quot;Race&quot;).filter($&quot;Anatomy&quot;.like(&quot;BOT&quot;))  

scala&amp;gt; sel.show  
+------+--------+---+-----+  
|Sample|     p16|Age| Race|  
+------+--------+---+-----+  
|GHN-48|Negative| 68|white|  
|GHN-57|Negative| 50|white|  
|GHN-62|Negative| 71|white|  
|GHN-39|Positive| 51|white|  
|GHN-60|Positive| 41|white|  
|GHN-64|Positive| 49|white|  
|GHN-65|Positive| 63|white|  
|GHN-69|Positive| 56|white|  
|GHN-70|Positive| 68|white|  
|GHN-71|Positive| 59|white|  
|GHN-77|Positive| 53|   AA|  
|GHN-82|Positive| 67|white|  
|GHN-43|Positive| 65|white|  
+-------------------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Spark Data Frame Schema&lt;/strong&gt;:&lt;br /&gt;
Schema is definition for the column name and it’s data type. In Spark, the data source defines the schema, and we infer it from the source. Spark Data Frame always uses Spark types (&lt;code class=&quot;highlighter-rouge&quot;&gt;org.apache.spark.sql.types&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;To check the Schema of Spark Data Frame use the following command.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; println(df.schema)  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Alternatively, a user can define the schema explicitly  and read the data using user defined schema definition (when data source is csv or json files).&lt;/p&gt;

&lt;p&gt;If you know the schema of the file ahead and do not want to use the inferSchema option for column names and types, use user-defined custom column names and type using schema option.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; import org.apache.spark.sql.types._

scala&amp;gt; val sampleSchema = new StructType()
	.add(&quot;Sample&quot;,StringType,true)
	.add(&quot;p16&quot;,StringType,true)
	.add(&quot;Age&quot;,IntegerType,true)
	.add(&quot;Race&quot;,StringType,true)
	.add(&quot;Sex&quot;,StringType,true)
	.add(&quot;Anatomy&quot;,StringType,true)
	.add(&quot;Smoking&quot;,StringType,true)
	.add(&quot;Radiation&quot;,StringType,true)
	.add(&quot;Chemo&quot;,StringType,true)  

scala&amp;gt; val df = spark.read.format(&quot;csv&quot;)
	.option(&quot;header&quot;, &quot;true&quot;)
	.option(&quot;schema&quot;,&quot;sampleSchema&quot;)
	.option(&quot;nullValue&quot;,&quot;NA&quot;)
	.option(&quot;mode&quot;,&quot;failfast&quot;)
	.load(&quot;hdfs://localhost:9000/user/adinasarapu/samples.csv&quot;)  

scala&amp;gt; df.printSchema()  
root  
 |-- Sample: string (nullable = true)  
 |-- p16: string (nullable = true)  
 |-- Age: string (nullable = true)  
 |-- Race: string (nullable = true)  
 |-- Sex: string (nullable = true)  
 |-- Anatomy: string (nullable = true)  
 |-- Smoking: string (nullable = true)  
 |-- Radiation: string (nullable = true)  
 |-- Chemo: string (nullable = true)  

scala&amp;gt; val df = df.select(&quot;sample&quot;,&quot;Age&quot;,&quot;Sex&quot;,&quot;Anatomy&quot;)
	.filter($&quot;Anatomy&quot;.contains(&quot;BOT&quot;) and $&quot;Age&quot; &amp;gt; 55)  

scala&amp;gt; df.show  
+------+---+------+-------+  
|sample|Age|   Sex|Anatomy|  
+------+---+------+-------+  
|GHN-48| 68|female|    BOT|  
|GHN-62| 71|  male|    BOT|  
|GHN-65| 63|  male|    BOT|  
|GHN-69| 56|  male|    BOT|  
|GHN-70| 68|  male|    BOT|  
|GHN-71| 59|  male|    BOT|  
|GHN-82| 67|  male|    BOT|  
|GHN-43| 65|  male|    BOT|  
+------+---+------+-------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Write the resulting Data Frame back to HDFS&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.write.option(&quot;header&quot;,&quot;true&quot;)
	.csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples_filtered.csv&quot;)

$hadoop fs -ls  
-rw-r--r--   1 adinasarapu supergroup     1318 2020-02-13 21:41 samples.csv  
drwxr-xr-x   - adinasarapu supergroup     0 2020-02-14 10:39 samples_filtered.csv  
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-14 09:33 survey.csv  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;overwrite&lt;/strong&gt; – mode is used to overwrite the existing file, alternatively, you can use &lt;code class=&quot;highlighter-rouge&quot;&gt;SaveMode.Overwrite&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.write.option(&quot;header&quot;,&quot;true&quot;)
	.mode(&quot;overwrite&quot;)
	.csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples_filtered.csv&quot;)  

OR  

scala&amp;gt; import org.apache.spark.sql.SaveMode  

scala&amp;gt; df.write.option(&quot;header&quot;,&quot;true&quot;)
	.mode(SaveMode.Overwrite)
	.csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples_filtered.csv&quot;)  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;3-global-temp-views&quot;&gt;3. Global Temp Views&lt;/h2&gt;

&lt;p&gt;Convert Spark Data Frame into temporary view that is available for only that spark session (Local)  or across spark sessions (Global) within the current application. The session-scoped view serve as a temporary table on which SQL queries can be made.&lt;/p&gt;

&lt;p&gt;There are two broad categories of Data Frame methods to create a view:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Global Temp View: Visible to the current application across the Spark sessions.&lt;br /&gt;
a). createGlobalTempView&lt;br /&gt;
b). createOrReplaceGlobalTempView&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Local Temp View: Visible to the current Spark session.&lt;br /&gt;
a). createOrReplaceTempView&lt;br /&gt;
b). createTempView&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;“The life of a Spark Application starts and finishes with the Spark Driver. The Driver is the process that clients use to submit applications in Spark. The Driver is also responsible for planning and coordinating the execution of the Spark program and returning status and/or results (data) to the client. The Driver can physically reside on a client or on a node in the cluster. The Spark Driver is responsible for creating the SparkSession.”&lt;/em&gt; - Data Analytics with Spark Using Python&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Spark &lt;strong&gt;Application&lt;/strong&gt; and Spark &lt;strong&gt;Session&lt;/strong&gt; are two different things. You can have multiple sessions in a single Spark Application. Spark session internally creates a Spark &lt;strong&gt;Context&lt;/strong&gt;. Spark Context represents connection to a Spark &lt;strong&gt;Cluster&lt;/strong&gt;. It also keeps track of all the RDDs, Cached data as well as the configurations. You can’t have more than one Spark Context in a single JVM. That means, one instance of an Application can have only one connection to the Cluster and hence a single Spark Context. In standard applications you may not have to create multiple sessions. However, if you are developing an application that needs to support multiple interactive users you might want to create one Spark Session for each user session. Ideally we should be able to create multiple connections to Spark Cluster for each user. But creating multiple Contexts is not yet supported by Spark.”&lt;/em&gt; - Learning Journal&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“We can have multiple spark contexts by setting spark.driver.allowMultipleContexts to true. But having multiple spark contexts in the same jvm is not encouraged and is not considered as a good practice as it makes it more unstable and crashing of 1 spark context can affect the other.”&lt;/em&gt; -  A tale of Spark Session and Spark Context&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Created a local temporary table view&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.createOrReplaceTempView(&quot;sample_tbl&quot;)  

scala&amp;gt; spark.catalog.listTables.show  
+----------+--------+-----------+---------+-----------+  
|      name|database|description|tableType|isTemporary|  
+----------+--------+-----------+---------+-----------+  
|sample_tbl|    null|       null|TEMPORARY|       true|  
+----------+--------+-----------+---------+-----------+  

scala&amp;gt; df.cache()  

scala&amp;gt; val resultsDF = spark.sql(&quot;SELECT * FROM sample_tbl WHERE Age &amp;gt; 70&quot;)  

scala&amp;gt; resultsDF.show  
+------+--------+---+-----+------+-------+-------+---------+-----+  
|Sample|     p16|Age| Race|   Sex|Anatomy|Smoking|Radiation|Chemo|  
+------+--------+---+-----+------+-------+-------+---------+-----+  
|GHN-62|Negative| 71|white|  male|    BOT|  never|        Y|    N|  
|GHN-73|Positive| 72|white|female| Tonsil|  never|        Y|    Y|  
+------+--------+---+-----+------+-------+-------+---------+-----+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Created a global temporary table view&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scala&amp;gt; df.createOrReplaceGlobalTempView(&quot;sample_gtbl&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sample_gtbl&lt;/code&gt; belongs to system database called &lt;code class=&quot;highlighter-rouge&quot;&gt;global_temp&lt;/code&gt;. This qualified name should be used to access &lt;code class=&quot;highlighter-rouge&quot;&gt;GlobalTempView(global_temp.sample_gtbl)&lt;/code&gt; or else it throws an error &lt;code class=&quot;highlighter-rouge&quot;&gt;Table or view not found&lt;/code&gt;.  When you run &lt;code class=&quot;highlighter-rouge&quot;&gt;spark.catalog.listTables.show&lt;/code&gt;, if you don’t specify the database for the &lt;code class=&quot;highlighter-rouge&quot;&gt;listTables()&lt;/code&gt; function it will point to default database. Try this instead:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;scala&amp;gt; spark.catalog.listTables(&quot;global_temp&quot;).show&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-----------+-----------+-----------+---------+-----------+  
|       name|   database|description|tableType|isTemporary|  
+-----------+-----------+-----------+---------+-----------+  
|sample_gtbl|global_temp|       null|TEMPORARY|       true|  
| sample_tbl|       null|       null|TEMPORARY|       true|  
+-----------+-----------+-----------+---------+-----------+  

scala&amp;gt; val resultsDF = spark.sql(&quot;SELECT * FROM global_temp.sample_gtbl WHERE Age &amp;gt; 70&quot;)  

scala&amp;gt; resultsDF.show  
+------+---+----+-------+  
|sample|Age| Sex|Anatomy|  
+------+---+----+-------+  
|GHN-62| 71|male|    BOT|  
+------+---+----+-------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;4-read-a-mysql-table-data-file-into-spark-data-frame&quot;&gt;4. Read a MySQL table data file into Spark Data Frame&lt;/h2&gt;

&lt;p&gt;At the command line, log in to &lt;code class=&quot;highlighter-rouge&quot;&gt;MySQL&lt;/code&gt; as the root user:&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$mysql -u root -p&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Type the MySQL root password, and then press Enter.&lt;/p&gt;

&lt;p&gt;To create a new MySQL user account, run the following command:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$mysql&amp;gt; CREATE USER 'adinasarapu'@'localhost' IDENTIFIED BY 'xxxxxxx';  

$mysql&amp;gt; GRANT ALL PRIVILEGES ON *.* TO 'adinasarapu'@'localhost';  

$mysql -u adinasarapu -p`  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;Type the MySQL user’s  password, and then press Enter.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$mysql&amp;gt; SHOW DATABASES;

$mysql&amp;gt; CREATE DATABASE meta;  

$mysql&amp;gt; SHOW DATABASES;  
+--------------------+  
| Database           |  
+--------------------+  
| information_schema |  
| meta               |  
| mysql              |  
| performance_schema |  
+--------------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To work with the new database, type the following command.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql&amp;gt; USE meta;  
mysql&amp;gt; CREATE TABLE samples (  
	-&amp;gt;  Sample VARCHAR(20) NOT NULL,  
	-&amp;gt;  Age INT,  
	-&amp;gt;  Race VARCHAR(20) NOT NULL,  
	-&amp;gt;  Sex VARCHAR(20) NOT NULL,  
	-&amp;gt;  Anatomy VARCHAR(20) NOT NULL,  
	-&amp;gt;  Smoking VARCHAR(20) NOT NULL,  
	-&amp;gt;  Radiation VARCHAR(20) NOT NULL,  
	-&amp;gt;  Chemo VARCHAR(20) NOT NULL,  
	-&amp;gt;  PRIMARY KEY ( Sample )  
-&amp;gt; );  
mysql&amp;gt; LOAD DATA INFILE '/Users/adinasarapu/spark_example/samples.csv'  
	INTO TABLE samples FIELDS TERMINATED BY ','  
	LINES TERMINATED BY '\n' IGNORE 1 ROWS;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If you encounter the following error&lt;br /&gt;
ERROR 1290 (HY000): The MySQL server is running with the &lt;code class=&quot;highlighter-rouge&quot;&gt;--secure-file-priv&lt;/code&gt; option so it cannot execute this statement&lt;/p&gt;

&lt;p&gt;Set &lt;code class=&quot;highlighter-rouge&quot;&gt;local_infile&lt;/code&gt; variable as true.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql&amp;gt; SET GLOBAL local_infile = true;

mysql&amp;gt; SHOW GLOBAL VARIABLES LIKE 'local_infile';  
+---------------+-------+  
| Variable_name | Value |  
+---------------+-------+  
| local_infile  | ON    |  
+---------------+-------+  

mysql&amp;gt; USE meta;  

mysql&amp;gt; LOAD DATA LOCAL INFILE '/Users/adinasarapu/spark_example/samples.csv'  
	INTO TABLE samples FIELDS TERMINATED BY ','  
	LINES TERMINATED BY '\n' IGNORE 1 ROWS;  

mysql&amp;gt; SELECT * FROM samples;  
+--------+------+------+-------+---------+---------+-----------+---------+  
| Sample | Age  | Race | Sex   | Anatomy | Smoking | Radiation | Chemo   |  
+--------+------+------+-------+---------+---------+-----------+---------+  
| GHN-39 |    0 | 51   | white | male    | BOT     | never     | Y       |  
| GHN-40 |    0 | 66   | white | male    | Tonsil  | former    | Y       |  
| GHN-43 |    0 | 65   | white | male    | BOT     | former    | Y       |  
| GHN-48 |    0 | 68   | white | female  | BOT     | current   | Y       |  
| GHN-53 |    0 | 58   | white | male    | Larynx  | current   | Y       |  
| ...		...		...		...		...	 |  
| ...		...		...		...		...	 |  
+------------------------------------------------------------------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a new MySQL table from Spark&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; import org.apache.spark.sql.SaveMode  
scala&amp;gt; df.write.mode(SaveMode.Append).jdbc(url,&quot;newsamples&quot;,prop)  

mysql&amp;gt; USE meta;  
mysql&amp;gt; SHOW tables;  
+----------------+  
| Tables_in_meta |  
+----------------+  
| newsamples     |  
| samples        |  
+----------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://hadoop.apache.org&quot;&gt;Apache Hadoop&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.learningjournal.guru/courses/spark/spark-foundation-training/&quot;&gt;Learning Journal&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://medium.com/@achilleus/spark-session-10d0d66d1d24&quot;&gt;A tale of Spark Session and Spark Context&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.a2hosting.com.co/kb/developer-corner/mysql/managing-mysql-databases-and-users-from-the-command-line&quot;&gt;MySQL&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="big data" /><category term="apache spark" /><category term="real time data pipelines" /><category term="scala" /><category term="hadoop" /><category term="MySQL" /><category term="bioinformatics" /><category term="Hadoop Distributed File System" /><summary type="html">Apache Spark is an open-source cluster computing system that provides high-level API in Java, Scala, Python and R.</summary></entry><entry><title type="html">Building a real-time big data pipeline (part 2: Spark, Hadoop)</title><link href="http://localhost:4000/posts/2020/02/blog-post-spark/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 2: Spark, Hadoop)" /><published>2020-02-09T00:00:00-08:00</published><updated>2020-02-09T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/02/blog-spark</id><content type="html" xml:base="http://localhost:4000/posts/2020/02/blog-post-spark/">&lt;p&gt;Apache Spark is a general-purpose, in-memory cluster computing engine  for large scale data processing.&lt;/p&gt;

&lt;p&gt;Apache Spark provides APIs in Java, Scala, Python, and R &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. It also supports Spark SQL for structured data processing, MLlib for machine learning, GraphX for computing graphs, and Spark Streaming for stream computing. The spark core has two parts. (1). Computing engine and (2). Spark Core APIs (Scala, Java, Python and R).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Apache Spark Ecosystem&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------+-----------+-------+----------+
| SQL	 | Streaming | MLlib |	GraphX 	|
|---------------------------------------|
|	Spark Core API			|	
|---------------------------------------|
| Scala	| Python    |	Java |	R	|
|---------------------------------------|	
|	Compute Engine			|
+---------------------------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Spark Computing Engine&lt;/strong&gt;: Hadoop MapReduce vs Spark&lt;/p&gt;

&lt;p&gt;Apache Hadoop&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; offers distributed storage (HDFS), resource manager (YARN) and computing framework (MapReduce). MapReduce reads and writes from disk, which slows down the processing speed and overall efficiency.&lt;/p&gt;

&lt;p&gt;Apache Spark is a distributed processing engine comes with it’s own Spark Standalone cluster manager. However, we can also plugin a cluster manager of our choice such as Apache Hadoop YARN (the resource manager in Hadoop 2), Apache Mesos, or Kubernetes. When Spark applications run on YARN, resource management, scheduling, and security are controlled by YARN. Similarly, for the storage system we can use Hadoop’s HDFS, Amazon S3, Google cloud storage or Apache Cassandra. The compute engine provides some basic functionality like memory management, task scheduling, fault recovery and most importantly interacting with the cluster manager and storage system. Spark also has a local mode, where the driver and executors run as threads on your computer instead of a cluster, which is useful for developing your applications from a personal computer.  Spark runs applications up to 100 times faster in memory and 10 times faster on disk than Hadoop by reducing the number of read-write operations to disk and storing intermediate data in-memory&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark Core APIs&lt;/strong&gt;
Spark core consists of structured API and unstructured API. Structured API consists of Data Frames and Data Sets. Unstructured APIs consists of RDDs&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. These core APIs (available as Scala, Java, Python and R) facilitate the execution of high-level operators with RDD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Spark libraries&lt;/strong&gt; such as Spark SQL, Spark Streaming, MLlib and Graphx directly depend on Spark Core APIs to achieve distributed processing.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Figure&lt;/b&gt;. Spark is fully compatible with the Hadoop eco-system and works smoothly with HDFS (&lt;a href=&quot;https://towardsdatascience.com&quot;&gt;https://towardsdatascience.com&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/hadoop.png&quot; alt=&quot;Spark&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Apache Spark reads data from source and load it into a Spark. There are 3 alternatives to hold data in Spark. 1) Data Frame 2) Data Set and 3) RDD. We can create RDDs using one of the two methods. 1.Load some data from a source or 2. Create an RDD by transforming another RDD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RDD: Resilient Distributed Dataset&lt;/strong&gt;&lt;br /&gt;
Spark RDD is a resilient, partitioned, distributed and immutable collection of data&lt;sup id=&quot;fnref:4:1&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Basically, it is read-only partition collection of records. This set of data is spread across multiple machines over cluster.  &lt;br /&gt;
&lt;strong&gt;Resilient&lt;/strong&gt; – RDDs are fault tolerant. If any bug or loss found, RDD has the capability to recover the loss. &lt;br /&gt;
&lt;strong&gt;Partitioned&lt;/strong&gt; – Spark breaks the RDD into smaller chunks of data. These pieces are called partitions.&lt;br /&gt;
&lt;strong&gt;Distributed&lt;/strong&gt; – Instead of keeping these partitions on a single machine, Spark spreads them across the cluster.&lt;br /&gt;
&lt;strong&gt;Immutable&lt;/strong&gt; – Once defined, you can’t change them i.e Spark RDD is a read-only data structure.&lt;/p&gt;

&lt;p&gt;For “RDDs vs DataFrames and Datasets - When to use them and why”, see reference &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Hadoop installation&lt;/p&gt;

&lt;p&gt;See tutorials on&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quickprogrammingtips.com/big-data/how-to-install-hadoop-on-mac-os-x-el-capitan.html&quot;&gt;How to Install and Configure Hadoop on Mac&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/HADOOP2/GettingStartedWithHadoop&quot;&gt;Getting Started With Hadoop&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Update your &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bash_profile&lt;/code&gt; file, which is a configuration file for configuring user environments.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$vi ~/.bash_profile  
export HADOOP_HOME=/Users/adinasarapu/Documents/hadoop-3.1.3  
export PATH=$PATH:$HADOOP_HOME/bin  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Start Hadoop&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;start-dfs.sh&lt;/code&gt; - Starts the Hadoop DFS daemons, the namenode and datanodes. Use this before start-mapred.sh&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;stop-dfs.sh&lt;/code&gt; - Stops the Hadoop DFS daemons.&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;start-mapred.sh&lt;/code&gt; - Starts the Hadoop Map/Reduce daemons, the jobtracker and tasktrackers.&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;stop-mapred.sh&lt;/code&gt; - Stops the Hadoop Map/Reduce daemons.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$bash sbin/start-dfs.sh 
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [Ashoks-MacBook-Pro.local]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Verify Hadoop installation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ jps
61073 ResourceManager
82025 SecondaryNameNode
61177 NodeManager
81882 DataNode
82303 Jps
81774 NameNode
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create user&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$hadoop fs -mkdir -p /user/adinasarapu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Move file to HDFS (Hadoop Distributed File System)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$hadoop fs -copyFromLocal samples.csv /user/adinasarapu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now the data file is at HDFS distributed storage. The file location at HDFS is &lt;code class=&quot;highlighter-rouge&quot;&gt;hdfs://localhost:9000/user/adinasarapu/samples.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;List files moved&lt;/strong&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$ hadoop fs -ls /user/adinasarapu&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Found 3 items
-rw-r--r--   1 adinasarapu supergroup  110252495 2020-02-08 17:04 /user/adinasarapu/flist.txt
-rw-r--r--   1 adinasarapu supergroup       1318 2020-02-09 14:47 /user/adinasarapu/samples.csv
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-09 08:21 /user/adinasarapu/survey.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Apache Spark installation&lt;/p&gt;

&lt;p&gt;For basic configuration see tutorial on &lt;a href=&quot;https://medium.com/luckspark/installing-spark-2-3-0-on-macos-high-sierra-276a127b8b85&quot;&gt;Installing Apache Spark … on macOS&lt;/a&gt;&lt;br /&gt;
Update your &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bash_profile&lt;/code&gt; file, which is a configuration file for configuring user environments.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$vi ~/.bash_profile  
export SPARK_HOME=/Users/adinasarapu/Documents/spark-3.0.0-preview2-bin-hadoop3.2  
export PATH=$PATH:$SPARK_HOME/bin  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Start Spark shell&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$spark-shell  
20/02/09 15:03:23 ..  
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties  
Setting default log level to &quot;WARN&quot;.  
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).  
Spark context Web UI available at http://192.168.0.5:4040  
Spark context available as 'sc' (master = local[*], app id = local-1581278612106).  
Spark session available as 'spark'.  
Welcome to  
      ____              __  
     / __/__  ___ _____/ /__  
    _\ \/ _ \/ _ `/ __/  '_/  
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-preview2  
      /_/  
           
Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_102)  
Type in expressions to have them evaluated.  
Type :help for more information.  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Read data from distributed storage (HDFS)&lt;/strong&gt;: csv file&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df = spark.read.options(Map(  
	&quot;header&quot; -&amp;gt; &quot;true&quot;,  
	&quot;inferSchema&quot;-&amp;gt;&quot;true&quot;,  
	&quot;nullValue&quot;-&amp;gt;&quot;NA&quot;,  
	&quot;timestampFormat&quot;-&amp;gt;&quot;MM-dd-yyyy&quot;,  
	&quot;mode&quot;-&amp;gt;&quot;failfast&quot;)).csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples.csv&quot;)  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Check the number of partitions&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.rdd.getNumPartitions  
res4: Int = 1  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Set/increase the number of partitions to 3&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df2 = df.repartition(3).toDF    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Recheck the number of partitions&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df2.rdd.getNumPartitions  
res5: Int = 3  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;SQL like operation&lt;/strong&gt; &lt;br /&gt;
Data Frame follows row and column structure like a database table. Data Frame compiles down to RDDs. RDDs are immutable; once loaded you can’t modify it. However, you can perform Transformations and and Actions. Spark Data Frames carries the same legacy from RDDs. Like RDDs, Spark Data Frames are immutable. You can perform transformation and actions on Data Frames.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.select(&quot;Sample&quot;,&quot;Age&quot;,&quot;Sex&quot;,&quot;Anatomy&quot;).filter(&quot;Age &amp;lt; 55&quot;).show  
  
+------+---+------+-------+  
|Sample|Age|   Sex|Anatomy|  
+------+---+------+-------+  
|GHN-57| 50|female|    BOT|  
|GHN-39| 51|  male|    BOT|  
|GHN-60| 41|  male|    BOT|  
|GHN-64| 49|  male|    BOT|  
|GHN-77| 53|  male|    BOT|  
|GHN-66| 52|  male| Tonsil|  
|GHN-67| 54|  male| Tonsil|  
|GHN-68| 51|  male| Tonsil|  
|GHN-80| 54|  male| Tonsil|  
|GHN-83| 54|  male| Tonsil|  
+------+---+------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df1 = df.select($&quot;Sex&quot;,$&quot;Radiation&quot;)    

scala&amp;gt; df1.show  
+------+---------+  
|   Sex|Radiation|  
+------+---------+  
|female|        Y|  
|female|        Y|  
|  male|        Y|  
|  male|        N|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        N|  
|  male|        N|  
|  male|  Unknown|  
|  male|        Y|  
|female|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        N|  
+------+---------+  
only showing top 20 rows  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df2 = df1.select($&quot;Sex&quot;,   
		(when($&quot;Radiation&quot; === &quot;Y&quot;,1).otherwise(0)).alias(&quot;Yes&quot;),  
		(when($&quot;Radiation&quot; === &quot;N&quot;,1).otherwise(0)).alias(&quot;No&quot;),  
		(when($&quot;Radiation&quot; === &quot;Unknown&quot;,1).otherwise(0)).alias(&quot;Unknown&quot;))    

scala&amp;gt; df2.show  
+------+-------+-------+-----------+  
|   Sex|    Yes|     No|    Unknown|  
+------+-------+-------+-----------+  
|female|      1|      0|          0|  
|female|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      0|      1|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      0|      1|          0|  
|  male|      0|      1|          0|  
|  male|      0|      0|          1|  
|  male|      1|      0|          0|  
|female|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      0|      1|          0|  
+------+-------+-------+-----------+  
only showing top 20 rows  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Scala user-defined function (UDF)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parseSex(g: String) = {  
 	g.toLowerCase match {   
			case &quot;male&quot;  =&amp;gt; “Male”  
			case &quot;female&quot; =&amp;gt; “Female”   
			case _ =&amp;gt; “Other”  
	}  
}   

scala&amp;gt; val parseSexUDF = udf(parseSex _)  

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df3 = df2.select((parseSexUDF($&quot;Sex&quot;)).alias(&quot;Sex&quot;),$&quot;Yes&quot;,$&quot;No&quot;,$&quot;Unknown&quot;)  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df4 = df3.groupBy(&quot;Sex&quot;).agg(sum($&quot;Yes&quot;), sum($&quot;No&quot;), sum($&quot;Unknown&quot;))    

scala&amp;gt; df4.show  
+------+------------+------------+----------------+  
|   Sex|    sum(Yes)|     sum(No)|    sum(Unknown)|     
+------+------------+------------+----------------+  
|Female|           3|           0|               0|  
|  Male|          18|           5|               1|  
+------+------------+------------+----------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df5 = df4.filter($&quot;Sex&quot; =!= &quot;Unknown&quot;)  

scala&amp;gt; df5.collect()  
scala&amp;gt; df5.show  
+----+------------+------------+----------------+  
| Sex|    sum(Yes)|     sum(No)|    sum(Unknown)|  
+----+------------+------------+----------------+  
|Male|          18|           5|               1|  
+----+------------+------------+----------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://hadoop.apache.org&quot;&gt;Apache Hadoop&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://dx.doi.org/10.1093%2Fgigascience%2Fgiy098&quot;&gt;Bioinformatics applications on Apache Spark&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.learningjournal.guru/courses/spark/spark-foundation-training/&quot;&gt;Learning Journal&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html&quot;&gt;RDDs vs Data Frames and Data Sets&lt;/a&gt; A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets - When to use them and why&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="big data" /><category term="apache spark" /><category term="real time data pipelines" /><category term="scala" /><category term="hadoop" /><category term="YARN" /><category term="bioinformatics" /><category term="Emory University" /><category term="Hadoop Distributed File System" /><summary type="html">Apache Spark is a general-purpose, in-memory cluster computing engine for large scale data processing.</summary></entry><entry><title type="html">ATAC-seq peak calling with MACS2</title><link href="http://localhost:4000/posts/2019/12/blog-post-atacseq/" rel="alternate" type="text/html" title="ATAC-seq peak calling with MACS2" /><published>2019-12-05T00:00:00-08:00</published><updated>2019-12-05T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/12/blog-atacseq</id><content type="html" xml:base="http://localhost:4000/posts/2019/12/blog-post-atacseq/">&lt;p&gt;ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing) is a next-generation sequencing approach for the analysis of open chromatin regions to assess the genome-wise chromatin accessibility.&lt;/p&gt;

&lt;p&gt;ATAC-seq achieves this by simultaneously fragmenting and tagging genomic DNA with sequencing adapters using the hyperactive Tn5 transposase enzyme &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Other global chromatin accessibility methods include FAIRE-seq and DNase-seq. This document aims to provide accessibility.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pre-processing of raw sequencing reads&lt;/strong&gt; – before mapping the raw reads to the genome, trim the adapter sequences. Poor read quality or sequencing
errors often lead to low mapping rate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapping/alignment of sequencing reads to a reference genome&lt;/strong&gt; – use Burrows-Wheeler Aligner (BWA) for mapping of sequencing reads. The output alignment file will be saved as a sequence alignment/map (SAM) format or binary version of SAM called BAM. Mark the duplicate reads using Picard &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and exclude reads mapping to mitochondrial DNA and other chromosomes from analysis together with low quality reads (MAPQ&amp;lt;10 and reads in Encode black list regions) using SAMtools &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Filtering and shifting of the mapped reads&lt;/strong&gt; - shift the read position +4 and -5 bp in the BAM file before peak calling &lt;a href=&quot;https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/&quot;&gt;adjust the reads alignment&lt;/a&gt;. When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site. Picard CollectInsertSizeMetrics will be used to compute the fragment sizes on alignment shifted BAM files.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Identification and visualization of the ATAC-seq peaks&lt;/strong&gt; – use MACS2 for peak calling with the parameters nomodel or BAMPE &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and identify the differentially enriched peaks using the MACS2 &lt;code class=&quot;highlighter-rouge&quot;&gt;bdgdiff&lt;/code&gt; module. Individual peaks separated by &amp;lt;100 bp will be join together. For peak annotation and functional analysis use the R package ChIPpeakAnno or HOMER &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. First, ATAC-seq peaks will be categorized into different groups based on the nearest RefSeq gene i.e. promoter, untranslated regions (UTRs), intron and exon. Second, peaks that are within 5 kb upstream and 3 kb downstream of the Transcription Start Site (TSS) are associated to the nearest genes. Finally, these genes are then analyzed for over-represented gene ontology (GO) terms and KEGG pathways using ChIPpeakAnno. Visualize all sequencing tracks using the Integrated Genomic Viewer (IGV) &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Scripts are available for HPC &lt;a href=&quot;https://bitbucket.org/adinasarapu/atac_seq-data-analysis/src/master/&quot;&gt;Cluster&lt;/a&gt;.&lt;br /&gt;
For further reading: &lt;a href=&quot;https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/&quot;&gt;ATAC-seq-data-analysis-from-FASTQ-to-peaks&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/24097267&quot;&gt;ATAC-seq&lt;/a&gt; - Nature Methods. 2013; 10:1213–1218.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://broadinstitute.github.io/picard/&quot;&gt;PICARD&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.htslib.org&quot;&gt;HTSLIB&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/taoliu/MACS&quot;&gt;MACS&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-237&quot;&gt;ChIPpeakAnno&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://homer.ucsd.edu/homer/ngs/&quot;&gt;HOMER&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://software.broadinstitute.org/software/igv/home&quot;&gt;IGV&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing)" /><category term="MACS2" /><category term="BWA" /><category term="HOMER" /><summary type="html">ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing) is a next-generation sequencing approach for the analysis of open chromatin regions to assess the genome-wise chromatin accessibility.</summary></entry><entry><title type="html">Single cell RNA-seq data analysis using CellRanger and Seurat on Cluster</title><link href="http://localhost:4000/posts/2019/01/blog-post-sc-ranseq/" rel="alternate" type="text/html" title="Single cell RNA-seq data analysis using CellRanger and Seurat on Cluster" /><published>2019-11-18T00:00:00-08:00</published><updated>2019-11-18T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/01/blog-sc-rnaseq</id><content type="html" xml:base="http://localhost:4000/posts/2019/01/blog-post-sc-ranseq/">&lt;p&gt;Running &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger&quot;&gt;cellranger&lt;/a&gt; as cluster mode that uses Sun Grid Engine (SGE) as queuing system allows highly parallelizable jobs.&lt;/p&gt;

&lt;p&gt;There are 4 steps to analyze Chromium Single Cell data&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger mkfastq&lt;/code&gt; demultiplexes raw base call (BCL) files generated by Illumina sequencers into FASTQ files.&lt;br /&gt;
&lt;strong&gt;Step 2&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count&lt;/code&gt; takes FASTQ files from cellranger mkfastq and performs alignment, filtering, barcode counting, and UMI counting. When doing large studies involving multiple GEM wells, run cellranger count on FASTQ data from each of the GEM wells individually, and then pool the results using cellranger aggr, as described &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
&lt;strong&gt;Step 3&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger aggr&lt;/code&gt; aggregates outputs from multiple runs of cellranger count.&lt;br /&gt;
&lt;strong&gt;Step 4&lt;/strong&gt;: Downstream/Secondary analysis using R package &lt;a href=&quot;https://satijalab.org/seurat/&quot;&gt;Seurat v3.0&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Running pipelines on cluster requires the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Load Cell Ranger module (&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger-3.1.0&lt;/code&gt;)&lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or, download and uncompress cellranger at your &lt;code class=&quot;highlighter-rouge&quot;&gt;$HOME&lt;/code&gt; directory and add PATH in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Update job config file (&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger-3.1.0/martian-cs/v3.2.3/jobmanagers/config.json&lt;/code&gt;) for threads and memory. For example&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;threads_per_job&quot;: 20,&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;memGB_per_job&quot;: 150,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Update template file (&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger-3.1.0/martian-cs/v3.2.3/jobmanagers/sge.template&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -pe smp __MRO_THREADS__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;##$ -l mem_free=__MRO_MEM_GB__G&lt;/code&gt; (comment this line if your cluster do not support it!)&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -q b.q&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -S /bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -m abe&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -M &amp;lt;e-mail&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd __MRO_JOB_WORKDIR__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;source $HOME/cellranger-3.1.0/sourceme.bash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For clusters whose job managers do not support memory requests, it is possible to request memory 
in the form of cores via the &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore&lt;/code&gt; command-line option. This option scales up the number 
of threads requested via the &lt;code class=&quot;highlighter-rouge&quot;&gt;__MRO_THREADS__&lt;/code&gt; variable according to how much memory a stage requires. 
see more at &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&quot;&gt;Cluster Mode&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. Download single cell gene expression and reference genome datasets from &lt;a href=&quot;https://www.10xgenomics.com/resources/datasets/&quot;&gt;10XGenomics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;. Create &lt;code class=&quot;highlighter-rouge&quot;&gt;sge.sh&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TR=&quot;$HOME/refdata-cellranger-GRCh38-3.0.0&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 3′ Gene expression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Output files will appear in the out/ subdirectory within this pipeline output directory.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd $HOME/10xgenomics/out&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For pipeline output directory, the &lt;code class=&quot;highlighter-rouge&quot;&gt;--id&lt;/code&gt; argument is used i.e 10XGTX_v3.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/pbmc_10k_v3_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count --disable-ui \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=10XGTX_v3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--sample=pbmc_10k_v3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--expect-cells=10000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=5000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 5′ gene expression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/vdj_v1_hs_nsclc_5gex_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-either-force-cells-or-expect-cells&quot;&gt;use either –force-cells or –expect-cells&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=10XGTX_v5 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--sample=vdj_v1_hs_nsclc_5gex \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--force-cells=7802 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=2000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 5′ gene expression and cell surface protein (Feature Barcoding/Antibody Capture Assay)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LIBRARY=$HOME/vdj_v1_hs_pbmc2_5gex_protein_fastqs/vdj_v1_hs_pbmc2_5gex_protein_library.csv&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;FEATURE_REF=$HOME/vdj_v1_hs_pbmc2_5gex_protein_fastqs/vdj_v1_hs_pbmc2_5gex_protein_feature_ref.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--libraries=${LIBRARY} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--feature-ref=${FEATURE_REF} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=PBMC_5GEX \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--expect-cells=9000 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=5000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;. Execute a command in screen and, detach and reconnect&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;screen&lt;/code&gt; command to get in/out of the system while keeping the processes running.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;screen -S screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bash sge.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want to exit the terminal without killing the running process, simply press &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+A+D&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To reconnect to the screen: &lt;code class=&quot;highlighter-rouge&quot;&gt;screen -R screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7&lt;/strong&gt;. Monitor work progress through a web browser&lt;/p&gt;

&lt;p&gt;Open &lt;code class=&quot;highlighter-rouge&quot;&gt;_log&lt;/code&gt; file present in output folder &lt;code class=&quot;highlighter-rouge&quot;&gt;PBMC_5GEX&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see serving UI as &lt;code class=&quot;highlighter-rouge&quot;&gt;http://cluster.university.edu:3600?auth=rlSdT_QLzQ9O7fxEo-INTj1nQManinD21RzTAzkDVJ8&lt;/code&gt;, then type the following from your laptop&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ssh -NT -L 9000:cluster.university.edu:3600 user@cluster.university.edu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;user@cluster.university.edu's password:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then access the UI using the following URL in your web browser
&lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:9000/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8&lt;/strong&gt;. Single Cell Integration in Seurat v3.0&lt;/p&gt;

&lt;p&gt;Seurat is an R package designed for QC, analysis, and exploration of single cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. Seurat starts by reading cellranger data (barcodes.tsv.gz, features.tsv.gz and matrix.mtx.gz)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pbmc.data &amp;lt;- Read10X(data.dir = &quot;~/PBMC_5GEX/outs/filtered_feature_bc_matrix/&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/overview/welcome&quot;&gt;10XGenomics&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://satijalab.org/seurat/&quot;&gt;Seurat&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="10XGenomics" /><category term="Chromium Single Cell Gene Expression" /><category term="Single cell RNA-sequencing (scRNA-seq)" /><category term="Single Cell" /><category term="Cell Ranger" /><category term="Seurat" /><category term="Cluster Computing" /><category term="Feature Barcoding" /><summary type="html">Running cellranger as cluster mode that uses Sun Grid Engine (SGE) as queuing system allows highly parallelizable jobs.</summary></entry><entry><title type="html">Taxonomic and diversity profiling of the microbiome - 16S rRNA gene amplicon sequence data</title><link href="http://localhost:4000/posts/2019/01/blog-post-qiime2/" rel="alternate" type="text/html" title="Taxonomic and diversity profiling of the microbiome - 16S rRNA gene amplicon sequence data" /><published>2019-01-01T00:00:00-08:00</published><updated>2019-01-01T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/01/blog-qiime2</id><content type="html" xml:base="http://localhost:4000/posts/2019/01/blog-post-qiime2/">&lt;p&gt;The 16S ribosomal RNA (rRNA) gene of Bacteria codes for the RNA component of the 30S subunit. Different bacterial species have one to multiple copies of the 16S rRNA gene, and each with 9 hypervariable regions, V1-V9. High-throughput sequencing of 16S rRNA gene (a “marker gene”) amplicons has become a widely used method to study bacterial phylogeny and species classification.&lt;/p&gt;

&lt;p&gt;Quantitative Insights Into Microbial Ecology “QIIME” 2 (release 2018.11)&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is a widely used package to identity abundance of microbes using 16s rRNA. Briefly, feature table containing counts of each unique sequence in the samples will be constructed using &lt;code class=&quot;highlighter-rouge&quot;&gt;qiime dada2 denoise-paired&lt;/code&gt; method. A feature is essentially any unit of observation, e.g., an OTU (Operational Taxonomic Unit), a sequence variant, a gene or a metabolite. In QIIME2 (currently), most features will be OTUs or sequence variants (alternatively, for OTUs, use QIIME2 plugin &lt;code class=&quot;highlighter-rouge&quot;&gt;q2-vsearch&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Data produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact typically has the &lt;code class=&quot;highlighter-rouge&quot;&gt;.qza&lt;/code&gt; file extension when output data stored in a file. Visualizations are another type of data (&lt;code class=&quot;highlighter-rouge&quot;&gt;.qzv&lt;/code&gt; file extension) generated by QIIME 2, which can be viewed using a web interface &lt;a href=&quot;https://view.qiime2.org&quot;&gt;https://view.qiime2.org&lt;/a&gt; (at Firefox web browser) without requiring a QIIME installation. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), we must create a QIIME 2 artifact by importing our &lt;code class=&quot;highlighter-rouge&quot;&gt;fastq.gz&lt;/code&gt; data files.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Scripts are available for AWS &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_qiime2/src&quot;&gt;Cloud&lt;/a&gt; and HPC &lt;a href=&quot;https://bitbucket.org/adinasarapu/cluster_qiime2/src&quot;&gt;Cluster&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://qiime2.org&quot;&gt;https://qiime2.org&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="microbiome" /><category term="computing" /><category term="qiime2" /><category term="16s rRNA amplicon" /><category term="DADA2" /><category term="OTU" /><summary type="html">The 16S ribosomal RNA (rRNA) gene of Bacteria codes for the RNA component of the 30S subunit. Different bacterial species have one to multiple copies of the 16S rRNA gene, and each with 9 hypervariable regions, V1-V9. High-throughput sequencing of 16S rRNA gene (a “marker gene”) amplicons has become a widely used method to study bacterial phylogeny and species classification.</summary></entry><entry><title type="html">Taxonomic and functional profiling of the microbiome - whole genome shotgun metagenomics</title><link href="http://localhost:4000/posts/2018/07/blog-post-metagenomics/" rel="alternate" type="text/html" title="Taxonomic and functional profiling of the microbiome - whole genome shotgun metagenomics" /><published>2018-07-27T00:00:00-07:00</published><updated>2018-07-27T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/07/blog-metagenomics</id><content type="html" xml:base="http://localhost:4000/posts/2018/07/blog-post-metagenomics/">&lt;p&gt;This workflow consists of taxonomic and functional profiling of shotgun metagenomics sequencing (MGS) reads using MetaPhlAn2 and HUMAnN2, respectively. To perform taxonomic (phyla, genera or species level) profiling of the MGS data, the MetaPhlAn2 pipeline was run on a high performance multicore cluster computing environment.&lt;/p&gt;

&lt;p&gt;MetaPhlAn2 provides microbial (bacterial, archaeal, viral, and eukaryotic) taxonomic profiling allowing the quantification of individual species across metagenomic samples. MetaPhlAn2 relies on ~1M unique clade-specific marker genes identified from ~17,000 reference genomes. Microbial reads, aligned by MetaPhlAn2, belonging to clades with no sequenced genomes available are reported as an “unclassified” subclade of the closest ancestor with available sequence data.
HUMAnN2 utilizes the MetaCyc database as well as the UniRef gene family catalog to characterize the microbial pathways present in samples. HUMAnN2 relies on programs such as BowTie (for accelerated nucleotide-level searches) and Diamond (for accelerated translated searches) to compute the abundance of gene families and metabolic pathways present. HUMAnN2 generates three outputs: 1) gene families based on UniRef proteins and their abundances reported in reads per kilobase, 2) MetaCyc pathways and their coverage, and 3) MetaCyc pathways and their abundances reported in reads per kilobase.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Scripts are available at &lt;a href=&quot;https://bitbucket.org/adinasarapu/shotgun_metagenomics/src&quot;&gt;shotgun_metagenomics&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="metagenomics" /><category term="MetaPhlAn2" /><category term="HUMAnN2" /><category term="taxonomic profiling" /><category term="functional profiling" /><category term="shotgun metagenomics sequencing" /><summary type="html">This workflow consists of taxonomic and functional profiling of shotgun metagenomics sequencing (MGS) reads using MetaPhlAn2 and HUMAnN2, respectively. To perform taxonomic (phyla, genera or species level) profiling of the MGS data, the MetaPhlAn2 pipeline was run on a high performance multicore cluster computing environment.</summary></entry></feed>