<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-07-05T13:39:33-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Bioinformatics | Big Data</title><subtitle>personal description</subtitle><author><name>Ashok R. Dinasarapu</name></author><entry><title type="html">Building a real-time big data pipeline (part 4: Kafka, Spark Streaming)</title><link href="http://localhost:4000/posts/2020/07/blog-post-kafka-spark-streaming/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 4: Kafka, Spark Streaming)" /><published>2020-07-04T00:00:00-07:00</published><updated>2020-07-04T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/07/blog-kafka-spark-streaming</id><content type="html" xml:base="http://localhost:4000/posts/2020/07/blog-post-kafka-spark-streaming/">&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org/&quot;&gt;Apache Kafka&lt;/a&gt; is a scalable, high performance and low latency platform for handling of real-time data feeds. Kafka allows reading and writing streams of data like a messaging system; written in Scala and Java.&lt;/p&gt;

&lt;p&gt;Kafka requires &lt;a href=&quot;https://zookeeper.apache.org/&quot;&gt;Apache Zookeeper&lt;/a&gt; to run. Kafka (v2.5.0) and zookeeper were installed using docker. See my other blog for installations &lt;a href=&quot;https://adinasarapu.github.io/posts/2020/01/blog-post-kafka/&quot;&gt;Kafka and Zookeeper with Docker&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once we start Zookeeper and Kafka locally, we can proceed to create our first topic, named “mytopic”:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \  
   --create \  
   --topic mytopic \  
   --partitions 1 \  
   --replication-factor 1 \  
   --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Spark Streaming is an extension of the core Apache Spark platform that enables scalable, high throughput, fault tolerant processing of data streams; written in Scala but offers Java, Python APIs to work with. Spark uses Hadoop’s client libraries for HDFS and YARN. It’s very important to assemble the compatible versions of all of these.&lt;/p&gt;

&lt;p&gt;Download, and unzip, Spark (v3.0.0) then add env variables to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bash_profile&lt;/code&gt; as&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export SPARK_HOME=&quot;/Users/adinasarapu/spark-3.0.0-preview2-bin-hadoop3.2&quot;  
export PATH=&quot;$PATH:$SPARK_HOME/bin&quot;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once the right package of Spark is unpacked, your will need to add the following jars in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$SPARK_HOME/jars&lt;/code&gt; directory. It’s important to choose the right package depending upon the broker available and features desired. We can pull the following dependencies from Maven Central:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;1. commons-pool2-2.8.0.jar  
2. kafka-clients-2.5.0.jar  
3. spark-sql-kafka-0-10_2.12-3.0.0-preview2.jar  
4. spark-token-provider-kafka-0-10_2.12-3.0.0-preview2.jar  
5. spark-streaming-kafka-0-10_2.12-3.0.0-preview2.jar  
6. kafka_2.12-2.5.0.jar  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Start the spark-shell using the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$spark-shell  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create a simple application in Scala using Spark which will integrate with the Kafka topic we created earlier. The application will read the messages as posted.&lt;/p&gt;

&lt;p&gt;We need to initialize the StreamingContext which is the entry point for all Spark Streaming applications:&lt;/p&gt;

&lt;p&gt;Stop &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SparkContext&lt;/code&gt; and create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StreamingContext&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sc.stop  

import org.apache.spark.SparkConf  
import org.apache.spark.streaming.StreamingContext  
import org.apache.spark.streaming.Seconds  

val conf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;kafkaReceiver&quot;)  

val streamingContext = new StreamingContext(conf, Seconds(10))  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, we can connect to the Kafka topic from the StreamingContext. Please note that we’ve to provide deserializers for key and value here. For common data types like String, the deserializer is available by default. However, if we wish to retrieve custom data types, we’ll have to provide custom deserializers.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import org.apache.kafka.clients.consumer.ConsumerRecord  
import org.apache.kafka.common.serialization.StringDeserializer  
import org.apache.spark.streaming.kafka010._  
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent  
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe  

val kafkaParams = Map[String, Object](  
	&quot;bootstrap.servers&quot; -&amp;gt; &quot;localhost:9092&quot;,  
	&quot;key.deserializer&quot; -&amp;gt; classOf[StringDeserializer],  
	&quot;value.deserializer&quot; -&amp;gt; classOf[StringDeserializer],  
	&quot;group.id&quot; -&amp;gt; &quot; spark-streaming-consumer-group&quot;,  
	&quot;auto.offset.reset&quot; -&amp;gt; &quot;latest&quot;,  
	&quot;enable.auto.commit&quot; -&amp;gt; (false: java.lang.Boolean))  

val topics = Array(&quot;mytopic&quot;)  

val stream = KafkaUtils.createDirectStream[String, String](  
	streamingContext,  
	PreferConsistent,  
	Subscribe[String, String](topics, kafkaParams))  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here, we’ve obtained InputDStream which is an implementation of Discretized Streams or DStreams, the basic abstraction provided by Spark Streaming. Internally DStreams is nothing but a continuous series of RDDs.&lt;/p&gt;

&lt;p&gt;We’ll now perform a series of operations on the InputDStream to obtain our messages. As this is a stream processing application, we would want to keep this running:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;stream.map(record=&amp;gt;(record.value().toString)).print  
streamingContext.start  
streamingContext.awaitTermination()  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once we start this application and post some messages in the Kafka topic we created earlier, … 
Then start kafka producer…&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;bash-4.4# ./kafka-console-producer.sh  
	--broker-list localhost:9092  
	--topic mytopic
&amp;gt;hello  
&amp;gt;here is my message  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;we should see the messages like …&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;-------------------------------------------  
Time: 1593889650000 ms  
-------------------------------------------  
hello  


-------------------------------------------  
Time: 1593889670000 ms  
-------------------------------------------  
here is my message  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, to exit from scala shell&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; :q  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can shut down docker-compose by executing the following command in another terminal.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# exit  

$docker-compose down  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="big data" /><category term="apache kafka" /><category term="real time data pipelines" /><category term="Scala" /><category term="docker" /><category term="Spark Streaming" /><category term="YAML" /><category term="Zookeeper" /><category term="Bioinformatics" /><category term="Emory University" /><summary type="html">Apache Kafka is a scalable, high performance and low latency platform for handling of real-time data feeds. Kafka allows reading and writing streams of data like a messaging system; written in Scala and Java.</summary></entry><entry><title type="html">Building a real-time big data pipeline (part 3: Hadoop, Spark and SQL)</title><link href="http://localhost:4000/posts/2020/02/blog-post-spark-sql/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 3: Hadoop, Spark and SQL)" /><published>2020-06-22T00:00:00-07:00</published><updated>2020-06-22T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/02/blog-spark-sql</id><content type="html" xml:base="http://localhost:4000/posts/2020/02/blog-post-spark-sql/">&lt;p&gt;Apache Spark is an open-source cluster computing system that provides high-level API in Java, Scala, Python and R.&lt;/p&gt;

&lt;p&gt;Spark also packaged with higher-level libraries for SQL, machine learning, streaming, and graphs. Spark SQL is Spark’s package for working with structured data &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;1-hadoop---copy-a-csv-file-to-hdfs&quot;&gt;1. Hadoop - copy a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.csv&lt;/code&gt; file to HDFS&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;The Hadoop Distributed File System (HDFS) is the primary data storage system used by Hadoop applications. It employs a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NameNode&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataNode&lt;/code&gt; architecture to implement a distributed file system that provides high-performance access to data across highly scalable Hadoop clusters&lt;/em&gt;.&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd hadoop-3.1.3/
$bash sbin/start-dfs.sh
$hadoop fs -mkdir -p /user/adinasarapu
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-copyFromLocal&lt;/code&gt; command to move one or more files from local location to HDFS.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$hadoop fs -copyFromLocal *.csv /user/adinasarapu  

$hadoop fs -ls /user/adinasarapu  
-rw-r--r--   1 adinasarapu supergroup     1318 2020-02-13 21:41 /user/adinasarapu/samples.csv  
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-14 09:33 /user/adinasarapu/survey.csv  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hadoop fs&lt;/code&gt; is more generic command that allows you to interact with multiple file systems like local, HDFS etc. This can be used when you are dealing with different file systems such as local FS, (S)FTP, S3, and others.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hdfs dfs&lt;/code&gt; is the command that is specific to HDFS.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$hdfs dfs -ls /user/adinasarapu  
-rw-r--r--   1 adinasarapu supergroup       1318 2020-02-15 10:02 samples.csv  
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-15 10:02 survey.csv  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Figure&lt;/b&gt;. Hadoop &lt;a href=&quot;http://hadoop.apache.org&quot;&gt;filesystem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/hadoop-fs.png&quot; alt=&quot;HDFS&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For Hadoop Architecture in Detail – &lt;a href=&quot;https://data-flair.training/blogs/hadoop-architecture/&quot;&gt;HDFS, Yarn &amp;amp; MapReduce&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$bash start-yarn.sh  
Starting resourcemanager  
Starting nodemanagers  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Check the list of Java processes running in your system by using the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jps&lt;/code&gt;. If you are able to see the Hadoop daemons running after executing the jps command, we can safely assume that the Hadoop cluster is running.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$jps  
96899 NodeManager  
91702 SecondaryNameNode  
96790 ResourceManager  
97240 Jps  
91437 NameNode  
91550 DataNode  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We can stop all the daemons using the command &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stop-all.sh&lt;/code&gt;. We can also start or stop each daemon separately.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$bash stop-all.sh  
WARNING: Stopping all Apache Hadoop daemons as adinasarapu in 10 seconds.  
WARNING: Use CTRL-C to abort.  
Stopping namenodes on [localhost]  
Stopping datanodes  
Stopping secondary namenodes [Ashoks-MacBook-Pro.local]  
Stopping nodemanagers  
Stopping resourcemanager  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Web UI&lt;/b&gt;&lt;br /&gt;
for HDFS: http://localhost:9870&lt;br /&gt;
for YARN Resource Manager: http://localhost:8088&lt;/p&gt;

&lt;p&gt;Note: Hadoop can be installed in 3 different modes: standalone mode, pseudo-distributed mode and fully distributed mode. In fully distributed mode, replace the ‘’localhost’ with actual host name of machine on cluster.&lt;/p&gt;

&lt;h2 id=&quot;2--read-a-csv-file-from-hdfs-into-spark-dataframe&quot;&gt;2.  Read a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;csv&lt;/code&gt; file (from HDFS) into Spark DataFrame&lt;/h2&gt;

&lt;p&gt;A Spark DataFrame can be constructed from an array of data sources such as Hive tables, Structured Data files (ex.csv), external databases (eg. MySQL), or existing RDDs.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Start Hadoop and Spark&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$bash start-dfs.sh  
$spark-shell  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Read csv file into a Spark DataFrame&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df = spark.read.format(&quot;csv&quot;)
	.option(&quot;header&quot;, &quot;true&quot;)
	.option(&quot;inferSchema&quot;,&quot;true&quot;)
	.option(&quot;nullValue&quot;,&quot;NA&quot;)
	.option(&quot;mode&quot;,&quot;failfast&quot;)
	.load(&quot;hdfs://localhost:9000/user/adinasarapu/samples.csv&quot;)  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For more details visit &lt;a href=&quot;https://sparkbyexamples.com/spark/spark-read-csv-file-into-dataframe/&quot;&gt;Spark Read CSV file into DataFrame&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Select &amp;amp; filter the Spark DataFrame&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val sel = df.select(&quot;Sample&quot;,&quot;p16&quot;,&quot;Age&quot;,&quot;Race&quot;).filter($&quot;Anatomy&quot;.like(&quot;BOT&quot;))  

What does dollar sign do here in scala? So, basically, you are making it a variable(of type Column) with $&quot;&quot; in Spark.  

scala&amp;gt; sel.show  
+------+--------+---+-----+  
|Sample|     p16|Age| Race|  
+------+--------+---+-----+  
|GHN-48|Negative| 68|white|  
|GHN-57|Negative| 50|white|  
|GHN-62|Negative| 71|white|  
|GHN-39|Positive| 51|white|  
|GHN-60|Positive| 41|white|  
|GHN-64|Positive| 49|white|  
|GHN-65|Positive| 63|white|  
|GHN-69|Positive| 56|white|  
|GHN-70|Positive| 68|white|  
|GHN-71|Positive| 59|white|  
|GHN-77|Positive| 53|   AA|  
|GHN-82|Positive| 67|white|  
|GHN-43|Positive| 65|white|  
+-------------------------+  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Spark DataFrame Schema&lt;/strong&gt;:&lt;br /&gt;
Schema is definition for the column name and it’s data type. In Spark, the data source defines the schema, and we infer it from the source. Spark Data Frame always uses Spark types (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;org.apache.spark.sql.types&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;To check the Schema of Spark DataFrame use the following command.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; println(df.schema)  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Alternatively, a user can define the schema explicitly and read the data using user defined schema definition (when data source is csv or json files).&lt;/p&gt;

&lt;p&gt;If you know the schema of the file ahead and do not want to use the inferSchema option for column names and types, use user-defined custom column names and type using schema option.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; import org.apache.spark.sql.types._

scala&amp;gt; val sampleSchema = new StructType()
	.add(&quot;Sample&quot;,StringType,true)
	.add(&quot;p16&quot;,StringType,true)
	.add(&quot;Age&quot;,IntegerType,true)
	.add(&quot;Race&quot;,StringType,true)
	.add(&quot;Sex&quot;,StringType,true)
	.add(&quot;Anatomy&quot;,StringType,true)
	.add(&quot;Smoking&quot;,StringType,true)
	.add(&quot;Radiation&quot;,StringType,true)
	.add(&quot;Chemo&quot;,StringType,true)  

scala&amp;gt; val df = spark.read.format(&quot;csv&quot;)
	.option(&quot;header&quot;, &quot;true&quot;)
	.option(&quot;schema&quot;,&quot;sampleSchema&quot;)
	.option(&quot;nullValue&quot;,&quot;NA&quot;)
	.option(&quot;mode&quot;,&quot;failfast&quot;)
	.load(&quot;hdfs://localhost:9000/user/adinasarapu/samples.csv&quot;)  

scala&amp;gt; df.printSchema()  
root  
 |-- Sample: string (nullable = true)  
 |-- p16: string (nullable = true)  
 |-- Age: string (nullable = true)  
 |-- Race: string (nullable = true)  
 |-- Sex: string (nullable = true)  
 |-- Anatomy: string (nullable = true)  
 |-- Smoking: string (nullable = true)  
 |-- Radiation: string (nullable = true)  
 |-- Chemo: string (nullable = true)  

scala&amp;gt; val df = df.select(&quot;sample&quot;,&quot;Age&quot;,&quot;Sex&quot;,&quot;Anatomy&quot;)
	.filter($&quot;Anatomy&quot;.contains(&quot;BOT&quot;) and $&quot;Age&quot; &amp;gt; 55)  

scala&amp;gt; df.show  
+------+---+------+-------+  
|sample|Age|   Sex|Anatomy|  
+------+---+------+-------+  
|GHN-48| 68|female|    BOT|  
|GHN-62| 71|  male|    BOT|  
|GHN-65| 63|  male|    BOT|  
|GHN-69| 56|  male|    BOT|  
|GHN-70| 68|  male|    BOT|  
|GHN-71| 59|  male|    BOT|  
|GHN-82| 67|  male|    BOT|  
|GHN-43| 65|  male|    BOT|  
+------+---+------+-------+  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Write the resulting DataFrame back to HDFS&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.write.option(&quot;header&quot;,&quot;true&quot;)
	.csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples_filtered.csv&quot;)

$hadoop fs -ls  
-rw-r--r--   1 adinasarapu supergroup     1318 2020-02-13 21:41 samples.csv  
drwxr-xr-x   - adinasarapu supergroup     0 2020-02-14 10:39 samples_filtered.csv  
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-14 09:33 survey.csv  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;overwrite&lt;/strong&gt; – mode is used to overwrite the existing file, alternatively, you can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SaveMode.Overwrite&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.write.option(&quot;header&quot;,&quot;true&quot;)
	.mode(&quot;overwrite&quot;)
	.csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples_filtered.csv&quot;)  

OR  

scala&amp;gt; import org.apache.spark.sql.SaveMode  

scala&amp;gt; df.write.option(&quot;header&quot;,&quot;true&quot;)
	.mode(SaveMode.Overwrite)
	.csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples_filtered.csv&quot;)  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To list all the files within a hdfs directory using Hadoop command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$hdfs dfs -ls /user/adinasarapu  

OR  

$hadoop fs -ls /user/adinasarapu  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To list all the files within a hdfs directory using Scala/Spark.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; import java.net.URI  
scala&amp;gt; import org.apache.hadoop.conf.Configuration
scala&amp;gt; import org.apache.hadoop.fs.{FileSystem, Path}  
  
scala&amp;gt; val uri = new URI(&quot;hdfs://localhost:9000&quot;)  
scala&amp;gt; val fs = FileSystem.get(uri,new Configuration())  
scala&amp;gt; val filePath = new Path(&quot;/user/adinasarapu/&quot;)  
scala&amp;gt; val status = fs.listStatus(filePath)  
scala&amp;gt; status.map(sts =&amp;gt; sts.getPath).foreach(println)  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hdfs://localhost:9000/user/adinasarapu/samples.csv  
hdfs://localhost:9000/user/adinasarapu/select.csv  
hdfs://localhost:9000/user/adinasarapu/survey.csv  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;3-temporary-views&quot;&gt;3. Temporary Views&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;“The life of a Spark Application starts and finishes with the Spark Driver. The Driver is the process that clients use to submit applications in Spark. The Driver is also responsible for planning and coordinating the execution of the Spark program and returning status and/or results (data) to the client. The Driver can physically reside on a client or on a node in the cluster. The Spark Driver is responsible for creating the SparkSession.”&lt;/em&gt; - Data Analytics with Spark Using Python&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“Spark &lt;strong&gt;Application&lt;/strong&gt; and Spark &lt;strong&gt;Session&lt;/strong&gt; are two different things. You can have multiple sessions in a single Spark Application. Spark session internally creates a Spark &lt;strong&gt;Context&lt;/strong&gt;. Spark Context represents connection to a Spark &lt;strong&gt;Cluster&lt;/strong&gt;. It also keeps track of all the RDDs, Cached data as well as the configurations. You can’t have more than one Spark Context in a single JVM. That means, one instance of an Application can have only one connection to the Cluster and hence a single Spark Context. In standard applications you may not have to create multiple sessions. However, if you are developing an application that needs to support multiple interactive users you might want to create one Spark Session for each user session. Ideally we should be able to create multiple connections to Spark Cluster for each user. But creating multiple Contexts is not yet supported by Spark.”&lt;/em&gt; - Learning Journal&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Convert Spark DataFrame into temporary view that is available for only that spark session (local)  or across spark sessions (global) within the current application. The session-scoped view serve as a temporary table on which SQL queries can be made. There are two broad categories of DataFrame methods to create a view:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Local Temp View: Visible to the current Spark session.
a). createOrReplaceTempView
b). createTempView&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Global Temp View: Visible to the current application across the Spark sessions.&lt;br /&gt;
a). createGlobalTempView&lt;br /&gt;
b). createOrReplaceGlobalTempView&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;“We can have multiple spark contexts by setting spark.driver.allowMultipleContexts to true. But having multiple spark contexts in the same JVM is not encouraged and is not considered as a good practice as it makes it more unstable and crashing of 1 spark context can affect the other.”&lt;/em&gt; -  A tale of Spark Session and Spark Context&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create a local temporary table view&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.createOrReplaceTempView(&quot;sample_tbl&quot;)  

spark.catalog.listTables() tries to fetch every table’s metadata first and then show the requested table names.  

scala&amp;gt; spark.catalog.listTables.show  
+----------+--------+-----------+---------+-----------+  
|      name|database|description|tableType|isTemporary|  
+----------+--------+-----------+---------+-----------+  
|sample_tbl|    null|       null|TEMPORARY|       true|  
+----------+--------+-----------+---------+-----------+  

scala&amp;gt; df.cache()  

There are two function calls for caching an RDD: cache() and persist(level: StorageLevel). The difference among them is that cache() will cache the RDD into memory, whereas persist(level) can cache in memory, on disk, or off-heap memory according to the caching strategy specified by level. persist() without an argument is equivalent with cache().  

scala&amp;gt; val resultsDF = spark.sql(&quot;SELECT * FROM sample_tbl WHERE Age &amp;gt; 70&quot;)  

scala&amp;gt; resultsDF.show  
+------+--------+---+-----+------+-------+-------+---------+-----+  
|Sample|     p16|Age| Race|   Sex|Anatomy|Smoking|Radiation|Chemo|  
+------+--------+---+-----+------+-------+-------+---------+-----+  
|GHN-62|Negative| 71|white|  male|    BOT|  never|        Y|    N|  
|GHN-73|Positive| 72|white|female| Tonsil|  never|        Y|    Y|  
+------+--------+---+-----+------+-------+-------+---------+-----+  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a global temporary table view&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scala&amp;gt; df.createOrReplaceGlobalTempView(&quot;sample_gtbl&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sample_gtbl&lt;/code&gt; belongs to system database called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;global_temp&lt;/code&gt;. This qualified name should be used to access &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GlobalTempView(global_temp.sample_gtbl)&lt;/code&gt; or else it throws an error &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Table or view not found&lt;/code&gt;.  When you run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spark.catalog.listTables.show&lt;/code&gt;, if you don’t specify the database for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;listTables()&lt;/code&gt; function it will point to default database. Try this instead:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scala&amp;gt; spark.catalog.listTables(&quot;global_temp&quot;).show&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-----------+-----------+-----------+---------+-----------+  
|       name|   database|description|tableType|isTemporary|  
+-----------+-----------+-----------+---------+-----------+  
|sample_gtbl|global_temp|       null|TEMPORARY|       true|  
| sample_tbl|       null|       null|TEMPORARY|       true|  
+-----------+-----------+-----------+---------+-----------+  

scala&amp;gt; val resultsDF = spark.sql(&quot;SELECT * FROM global_temp.sample_gtbl WHERE Age &amp;gt; 70&quot;)  

scala&amp;gt; resultsDF.show  
+------+---+----+-------+  
|sample|Age| Sex|Anatomy|  
+------+---+----+-------+  
|GHN-62| 71|male|    BOT|  
+------+---+----+-------+  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;4-read-a-mysql-table-data-file-into-spark-dataframe&quot;&gt;4. Read a MySQL table data file into Spark DataFrame&lt;/h2&gt;

&lt;p&gt;At the command line, log in to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MySQL&lt;/code&gt; as the root user:&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$mysql -u root -p&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Type the MySQL root password, and then press Enter.&lt;/p&gt;

&lt;p&gt;To create a new MySQL user account, run the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$mysql&amp;gt; CREATE USER 'adinasarapu'@'localhost' IDENTIFIED BY 'xxxxxxx';  

$mysql&amp;gt; GRANT ALL PRIVILEGES ON *.* TO 'adinasarapu'@'localhost';  

$mysql -u adinasarapu -p`  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Type the MySQL user’s  password, and then press Enter.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$mysql&amp;gt; SHOW DATABASES;

$mysql&amp;gt; CREATE DATABASE meta;  

$mysql&amp;gt; SHOW DATABASES;  
+--------------------+  
| Database           |  
+--------------------+  
| information_schema |  
| meta               |  
| mysql              |  
| performance_schema |  
+--------------------+  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To work with the new database, type the following command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql&amp;gt; USE meta;  
mysql&amp;gt; CREATE TABLE samples (  
	-&amp;gt;  Sample VARCHAR(20) NOT NULL,  
	-&amp;gt;  p16 VARCHAR(20) NOT NULL,  
	-&amp;gt;  Age INT,  
	-&amp;gt;  Race VARCHAR(20) NOT NULL,  
	-&amp;gt;  Sex VARCHAR(20) NOT NULL,  
	-&amp;gt;  Anatomy VARCHAR(20) NOT NULL,  
	-&amp;gt;  Smoking VARCHAR(20) NOT NULL,  
	-&amp;gt;  Radiation VARCHAR(20) NOT NULL,  
	-&amp;gt;  Chemo VARCHAR(20) NOT NULL,  
	-&amp;gt;  PRIMARY KEY ( Sample )  
-&amp;gt; );  
mysql&amp;gt; LOAD DATA LOCAL INFILE '/Users/adinasarapu/spark_example/samples.csv'  
	INTO TABLE samples FIELDS TERMINATED BY ','  
	LINES TERMINATED BY '\n' IGNORE 1 ROWS;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you encounter the following error&lt;br /&gt;
ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides&lt;/p&gt;

&lt;p&gt;Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;local_infile = 1&lt;/code&gt; to the [mysqld] section of the /etc/my.cnf file and restart the mysqld service.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$sudo vi /etc/my.cnf  

$mysql -u root -p --local_infile=1

mysql&amp;gt; SHOW GLOBAL VARIABLES LIKE 'local_infile';  
+---------------+-------+  
| Variable_name | Value |  
+---------------+-------+  
| local_infile  | ON    |  
+---------------+-------+  

mysql&amp;gt; USE meta;  

mysql&amp;gt; LOAD DATA LOCAL INFILE '/Users/adinasarapu/spark_example/samples.csv'  
	INTO TABLE samples FIELDS TERMINATED BY ','  
	LINES TERMINATED BY '\n' IGNORE 1 ROWS;  

mysql&amp;gt; SELECT * FROM samples;  
+--------+----------+-----+-------+--------+---------+---------+-----------+---------+  
| Sample | p16      | Age | Race  | Sex    | Anatomy | Smoking | Radiation | Chemo   |  
+--------+----------+-----+-------+--------+---------+---------+-----------+---------+  
| GHN-39 | Positive |  51 | white | male   | BOT     | never   | Y         | Y       |  
| GHN-40 | Positive |  66 | white | male   | Tonsil  | former  | Y         | Y       |  
| GHN-43 | Positive |  65 | white | male   | BOT     | former  | Y         | Y       |  
| GHN-48 | Negative |  68 | white | female | BOT     | current | Y         | Y       |  
| GHN-53 | Unknown  |  58 | white | male   | Larynx  | current | Y         | Y       |  
| GHN-57 | Negative |  50 | white | female | BOT     | current | Y         | Y       |  
| GHN-84 | Positive |  56 | white | male   | Tonsil  | never   | Y         | N       |  
| ...    | ...      |  .. | ...   | ...    | ...     | ...     | ...       | ...     |  
+--------+----------+-----+-------+--------+---------+---------+-----------+---------+  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a new MySQL table from Spark DataFrame&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; import org.apache.spark.sql.SaveMode  
scala&amp;gt; val prop = new java.util.Properties  
scala&amp;gt; prop.setProperty(&quot;driver&quot;, &quot;com.mysql.cj.jdbc.Driver&quot;)  
scala&amp;gt; prop.setProperty(&quot;user&quot;, &quot;root&quot;)  
scala&amp;gt; prop.setProperty(&quot;password&quot;, &quot;pw&quot;)  
scala&amp;gt; val url = &quot;jdbc:mysql://localhost:3306/meta&quot;  
scala&amp;gt; df.write.mode(SaveMode.Append).jdbc(url,&quot;newsamples&quot;,prop)  

If you see MySQL JDBC Driver - Time Zone Issue, change url to  
scala&amp;gt; val url = &quot;jdbc:mysql://localhost/meta?useUnicode=true&amp;amp;useJDBCCompliantTimezoneShift=true&amp;amp;useLegacyDatetimeCode=false&amp;amp;serverTimezone=UTC&quot;  

mysql&amp;gt; USE meta;  
mysql&amp;gt; SHOW tables;  
+----------------+  
| Tables_in_meta |  
+----------------+  
| newsamples     |  
| samples        |  
+----------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, exit mysql, scala and hadoop&lt;/p&gt;

&lt;p&gt;mysql&amp;gt;exit&lt;br /&gt;
scala&amp;gt;:q&lt;br /&gt;
bash stop-all.sh&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://hadoop.apache.org&quot;&gt;Apache Hadoop&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.learningjournal.guru/courses/spark/spark-foundation-training/&quot;&gt;Learning Journal&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://medium.com/@achilleus/spark-session-10d0d66d1d24&quot;&gt;A tale of Spark Session and Spark Context&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.a2hosting.com.co/kb/developer-corner/mysql/managing-mysql-databases-and-users-from-the-command-line&quot;&gt;MySQL&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="big data" /><category term="apache spark" /><category term="real time data pipelines" /><category term="scala" /><category term="hadoop" /><category term="MySQL" /><category term="bioinformatics" /><category term="Hadoop Distributed File System" /><category term="Emory Uiversity" /><summary type="html">Apache Spark is an open-source cluster computing system that provides high-level API in Java, Scala, Python and R.</summary></entry><entry><title type="html">Building a real-time big data pipeline (part 1: Kafka, Spring Boot)</title><link href="http://localhost:4000/posts/2020/01/blog-post-kafka/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 1: Kafka, Spring Boot)" /><published>2020-04-05T00:00:00-07:00</published><updated>2020-04-05T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/01/blog-kafka</id><content type="html" xml:base="http://localhost:4000/posts/2020/01/blog-post-kafka/">&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org&quot;&gt;Kafka&lt;/a&gt; is used for building real-time data pipelines and streaming apps.&lt;/p&gt;

&lt;p&gt;What is Kafka? &lt;a href=&quot;https://success.docker.com/article/getting-started-with-kafka&quot;&gt;Getting started with kafka&lt;/a&gt; says &lt;em&gt;Kafka is a distributed append log; in a simplistic view it is like a file on a filesystem. Producers can append data (echo ‘data’ » file.dat), and consumers subscribe to a certain file (tail -f file.dat)&lt;/em&gt;. In addition, Kafka provides an ever-increasing counter and a timestamp for each consumed message. Kafka uses Zookeeper to store metadata about producers, topics and partitions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kafka for local development of applications&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;There are multiple ways of running Kafka locally for development of apps but the easiest method is by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose&lt;/code&gt;. To download Docker Desktop, go to &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt; and Sign In with your Docker ID.&lt;/p&gt;

&lt;p&gt;Docker compose facilitates installing &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Kafka&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Zookeeper&lt;/code&gt; with the help of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '2'  
services:  
  zookeeper:  
    image: wurstmeister/zookeeper  
    ports:  
      - &quot;2181:2181&quot;  
  kafka:  
   image: wurstmeister/kafka  
    ports:  
      - &quot;9092:9092&quot;  
    environment:  
     KAFKA_ADVERTISED_HOST_NAME: localhost  
     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  
     KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;true&quot;  
    volumes:  
     - /var/run/docker.sock:/var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;1. Start the Kafka service&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Open a terminal, go to the directory where you have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file, and execute the following command. This command starts the docker-compose engine, and it downloads the images and runs them.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose up -d  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Starting kafka_example_zookeeper_1 ... done  
Starting kafka_example_kafka_1     ... done  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To list running docker containers, run the following command&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose ps  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Name				Command				State	Ports  
kafka_example_kafka_1		start-kafka.sh			Up	0.0.0.0:9092-&amp;gt;9092/tcp                              
kafka_example_zookeeper_1	/bin/sh -c /usr/sbin/sshd  ...	Up	0.0.0.0:2181-&amp;gt;2181/tcp, 22/tcp, 2888/tcp, 3888/tcp  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can shut down docker-compose by executing the following command in another terminal.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose down  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stopping kafka_example_zookeeper_1 ... done  
Stopping kafka_example_kafka_1     ... done  
Removing kafka_example_zookeeper_1 ... done  
Removing kafka_example_kafka_1     ... done  
Removing network kafka_example_default  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using the following command check the ZooKeeper logs to verify that ZooKeeper is working and healthy.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose logs zookeeper | grep -i binding  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, check the Kafka logs to verify that broker is working and healthy.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose logs kafka | grep -i started  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. Create a Kafka topic&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Kafka cluster stores streams of records in categories called topics. Each record in a topic consists of a key, a value, and a timestamp. A topic can have zero, one, or many consumers that subscribe to the data written to it.&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose exec&lt;/code&gt; to execute a command in a running container. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose exec&lt;/code&gt; command by default allocates a TTY, so that you can use such a command to get an interactive prompt. Go into directory where &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file present, and execute it as&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose exec kafka bash  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(for zookeeper &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$docker-compose exec zookeeper bash&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Change the directory to /opt/kafka/bin where you find scripts such as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kafka-topics.sh&lt;/code&gt;.&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd /opt/kafka/bin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Create, list or delete&lt;/strong&gt; existing topics:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \  
 --create \  
 --topic mytopic \  
 --partitions 1 \  
 --replication-factor 1 \  
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;b&gt;Figure 1&lt;/b&gt;. Kafka topic partitions layout (&lt;a href=&quot;https://cloudblogs.microsoft.com/opensource/2018/07/09/how-to-data-processing-apache-kafka-spark/&quot;&gt;cloudblogs.microsoft.com&lt;/a&gt;). Each partition in a topic is an ordered, immutable sequence of records that is continually appended to a structured commit log.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-partitions.png&quot; alt=&quot;Partitions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-partition.png&quot; alt=&quot;Partition&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \
 --list \
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If necessary, delete a topic using the following command.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \
 --delete \
 --topic mytopic \
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3. Kafka Producer and Consumer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A Kafka producer is an object that consists of a pool of buffer space that holds records that haven’t yet been transmitted to the server.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Figure 2&lt;/b&gt;. Relationship between &lt;a href=&quot;https://medium.com/@kavimaluskam/start-your-real-time-pipeline-with-apache-kafka-39e30129892a&quot;&gt;Kafka Components&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-producer-consumer.png&quot; alt=&quot;Producer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kafka broker (a.k.a Kafka server/node) is the server node in the cluster, mainly responsible for hosting partitions of Kafka Topics, transferring messages from Kafka Producer to Kafka Consumer and, providing data replication and partitioning within a Kafka Cluster.&lt;/p&gt;

&lt;p&gt;The following is a producer command line to read data from standard input and write it to a Kafka Topic.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-producer.sh \
 --broker-list localhost:9092 \
 --topic mytopic
  
&amp;gt;Hello  
&amp;gt;World  
^C
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following is a command line to read data from a Kafka topic and write it to standard output.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-consumer.sh \
 --bootstrap-server localhost:9092 \
 --topic mytopic \
 --from-beginning
  
Hello  
World  
^CProcessed a total of 2 messages  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Another way of reading data from a Kafka topic is by simply using a Spring Boot application&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The following demonstrates how to receive messages from Kafka Topic. First in this blog I create a Spring Kafka Consumer, which is able to listen the messages sent to a Kafka Topic. Then I create a Spring Kafka Producer, which is able to send messages to a Kafka Topic.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Figure 3&lt;/b&gt;. Kafka Producer and Consumer in Java (&lt;a href=&quot;https://blog.clairvoyantsoft.com/benchmarking-kafka-e7b7c289257d&quot;&gt;blog.clairvoyantsoft.com&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka-producer-consumer-java.png&quot; alt=&quot;Java&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first step to create a simple &lt;strong&gt;Spring Boot Maven Application&lt;/strong&gt; is &lt;a href=&quot;https://spring.io/guides/gs/spring-boot/&quot;&gt;Starting with Spring Initializr&lt;/a&gt; and make sure to have spring-kafka dependency to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;  
  &amp;lt;groupId&amp;gt;org.springframework.kafka&amp;lt;/groupId&amp;gt;  
  &amp;lt;artifactId&amp;gt;spring-kafka&amp;lt;/artifactId&amp;gt;  
&amp;lt;/dependency&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SpringBootKafkaApplication.java&lt;/code&gt; class (The Spring Initializr creates the following simple application class for you)&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@SpringBootApplication  
public class SpringBootKafkaApplication {  
	public static void main(String[] args) {  
		SpringApplication.run(SpringBootApplication.class, args);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Configure Kafka through application.yml configuration file&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In Spring Boot, properties are kept in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;application.properties&lt;/code&gt; file under the classpath. The application.properties file is located in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src/main/resources&lt;/code&gt; directory. Change application.properties file to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;application.yml&lt;/code&gt;, then add the following content.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spring:  
  kafka:  
consumer:  
  bootstrap-servers: localhost:9092  
  group-id: group_test1  
  auto-offset-reset: earliest  
  key-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
  value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create a Spring Kafka Consumer class&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Create a class called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KafkaConsumer.java&lt;/code&gt; and add a method with the @KakfaListener annotation.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Service  
public class KafkaConsumer {  
	@KafkaListener(id = &quot;group_test1&quot;, topics = &quot;mytopic&quot;)  
	public void consumeMessage(String message) {  
		System.out.println(&quot;Consumed message: &quot; + message);  
	}  
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;How to run Spring Boot web application in Eclipse?&lt;/p&gt;

&lt;p&gt;In eclipse Project Explorer, right click the project name -&amp;gt; select “Run As” -&amp;gt; “Maven Build…”&lt;br /&gt;
In the goals, enter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spring-boot:run&lt;/code&gt;&lt;br /&gt;
then click Run button.&lt;/p&gt;

&lt;p&gt;If you have Spring Tool Suite (STS) plug-in, you see a “Spring Boot App” option under Run As.&lt;/p&gt;

&lt;p&gt;Run the following console producer which will enable you to send messages to Kafka:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-producer.sh \
 --broker-list localhost:9092 \
 --topic mytopic
  
&amp;gt;Hello  
&amp;gt;World  
^C  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Try sending a few messages like above (Hello, World etc) and watch the application standard output in the Eclipse shell where you are running your Spring Boot application.&lt;/p&gt;

&lt;p&gt;Eclipse Console:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  .   ____          _            __ _ _  
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \  
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \  
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )  
  '  |____| .__|_| |_|_| |_\__, | / / / /  
 =========|_|==============|___/=/_/_/_/  

:: Spring Boot ::        (v2.2.4.RELEASE)  
2020-01-26 14:26:55.205  INFO 11137 --- [           main] c.e.d.SpringBootKafkaConsumerApplication : Starting   
…  
…  
…
o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=simpleconsumer] Setting newly assigned partitions: test1-0  
2020-01-26 14:26:56.384  INFO 11137 --- [econsumer-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=simpleconsumer] Found no committed offset for partition test1-0  
2020-01-26 14:26:56.408  INFO 11137 --- [econsumer-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-1, groupId=simpleconsumer] Resetting offset for partition test1-0 to offset 2.  
2020-01-26 14:26:56.477  INFO 11137 --- [econsumer-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : simpleconsumer: partitions assigned: [test1-0]  
Got message: hello  
Got message: world  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The following code demonstrates how to send and receive messages from Kafka Topic. The above &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KafkaConsumer.java&lt;/code&gt; receives messages that were sent to a Kafka Topic. The followng &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KafkaProducer.java&lt;/code&gt; send messages to a Kafka Topic.&lt;/p&gt;

&lt;p&gt;Make sure to have spring-web dependency to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;  
  &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;  
  &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;  
&amp;lt;/dependency&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add two new java classes &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KafkaController.java&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KafkaProducer.java&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@RestController  
@RequestMapping(value=&quot;/kafka&quot;)  
public class KafkaController {  
	private final KafkaProducer producer;  

	@Autowired  
	KafkaController(KafkaProducer producer){  
		this.producer = producer;  
	}  

	@PostMapping(value=&quot;/publish&quot;)  
	public void messagePrint(@RequestParam(value=&quot;message&quot;, required = false) String message) {  
		this.producer.sendMessage(message);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Service  
public class KafkaProducer {  
	private static final String TOPIC = &quot;mytopic&quot;;  

	@Autowired  
	private KafkaTemplate&amp;lt;String, String&amp;gt; kafkaTemplate;  
	
	public void sendMessage(String message) {  
		kafkaTemplate.send(TOPIC, message);  
		System.out.println(&quot;Produced message: &quot; + message);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Update &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;application.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server:
  port: 8080
spring:
  kafka:
   consumer:
    bootstrap-servers: localhost:9092
    group-id: group_test1
    auto-offset-reset: earliest
    key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
   producer:  
    bootstrap-servers: localhost:9092  
    key-serializer: org.apache.kafka.common.serialization.StringSerializer  
    value-serializer: org.apache.kafka.common.serialization.StringSerializer  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run Spring Boot web application (see How to run Spring Boot web application in Eclipse?)&lt;/p&gt;

&lt;p&gt;Make POST request using &lt;a href=&quot;https://www.getpostman.com&quot;&gt;Postman&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Select &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;POST&lt;/code&gt; and use the API &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:8080/kafka/publish&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;Body&lt;/strong&gt;: form-data  &lt;strong&gt;KEY&lt;/strong&gt;: message  &lt;strong&gt;VALUE&lt;/strong&gt;: hello&lt;/p&gt;

&lt;p&gt;Finally click &lt;strong&gt;send&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;See Eclipse Console for messages:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;...  
...  
...  
2020-01-27 13:12:06.911  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.3.1  
2020-01-27 13:12:06.912  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 18a913733fb71c01  
2020-01-27 13:12:06.912  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1580148726911  
2020-01-27 13:12:06.947  INFO 31822 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: 6R8O95IPSfGoifR4zzwM6g  
Produced message: hello  
Consumed message: hello
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="big data" /><category term="apache kafka" /><category term="real time data pipelines" /><category term="java" /><category term="docker" /><category term="Spring Boot" /><category term="YAML" /><category term="Zookeeper" /><category term="Bioinformatics" /><category term="Emory University" /><summary type="html">Kafka is used for building real-time data pipelines and streaming apps.</summary></entry><entry><title type="html">Quantitative proteomics: TMT-based quantitation of proteins</title><link href="http://localhost:4000/posts/2020/01/blog-post-tmt/" rel="alternate" type="text/html" title="Quantitative proteomics: TMT-based quantitation of proteins" /><published>2020-04-04T00:00:00-07:00</published><updated>2020-04-04T00:00:00-07:00</updated><id>http://localhost:4000/posts/2020/01/blog-tmt</id><content type="html" xml:base="http://localhost:4000/posts/2020/01/blog-post-tmt/">&lt;p&gt;Quantification of proteins using isobaric labeling (tandem mass tag or TMT) starts with the reduction of disulfide bonds in proteins with Dithiothreitol (DTT).&lt;/p&gt;

&lt;p&gt;Alkylation with iodoacetamide (IAA) after cystine reduction results in the covalent addition of a carbamidomethyl group that prevents the formation of disulfide bonds. Then, overnight digestion of the proteins using trypsin or trypsin/LyC mixture, Tandem Mass Tag (TMT) labeling on Lysine residues, pooling of all samples and the fractionation of peptide mixture. Finally, LC-MS/MS data acquisition and database search for protein identification and quantification.&lt;/p&gt;

&lt;p&gt;Fractionation prior to LC-MS/MS analysis effectively detects low abundance proteins (&amp;lt;100ng/mL), which is the concentration range of most clinical biomarkers.&lt;/p&gt;

&lt;p&gt;TMT-based approach allows multiplexing of samples. TMT 6-plex reagents produces a series of different reporter ions with nominal masses from 126 to 131 Da at 1 Da intervals. Several TMT reagents are commercially available including TMTzero, TMT duplex, TMT 6-plex, and TMT 10-plex &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. They have the same chemical structure but contain different numbers and combinations of 13C and 15N isotopes in the reporter group. The overall calibrated intensities of reporter ions equal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Isobaric labeling reagent (TMT) structure&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;a. An amine-specific reactive group – an N-hydroxysuccinimide ester, which reacts with primary amines i.e unblocked N-terminals and lysine side chains.&lt;br /&gt;
b. A mass reporter group for quantification – the reporter groups are partially fragmented from the peptide during precursor fragmentation in the mass spectrometer.&lt;br /&gt;
c. A mass normalizer group to link the reactive and reporter groups – mass normalizer group ensures that the peptide complexity in the MS1 spectra does not increase with multiplexing.&lt;/p&gt;

&lt;p&gt;Unless otherwise noted, every analysis utilizes an MS3-based TMT-centric mass spectrometry method. MS2-based TMT yields the highest precision but lowest accuracy due to ratio compression, which MS3-based TMT can partly rescue &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. To 10 precursors selected for MS2/MS3 analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are the correction factors used for TMT?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TMT reporter ion signals need to be adjusted to account for isotopic impurities in each TMT variant.  For TMT-10plex labeling, different batches of TMT reagents have slightly different isotope impurities that need to be included for database search to correct the reporter ion ratio. The information of isotope impurity can be found in the reagent kit. However, I do not normally correct for isotopic impurities of TMT reagents, as this is typically a «1.2% shift.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Database search for protein identification and quantification&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;The resulting TMT-MS3 data (.raw files) are processed using MaxQuant with an integrated Andromeda search engine (v.1.6.7.0). Tandem mass spectra were searched against the Uniprot database.&lt;/p&gt;

&lt;p&gt;Download and install  &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src/master/&quot;&gt;MaxQuant&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Specification of Various Parameters in MaxQuant (All the other parameters in MaxQuant are set to the default values for processing orbitrap-type data)&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw data&lt;/strong&gt; pane&lt;br /&gt;
Using Load, select all raw files from all batches. Update Experiment (a unique value for each of the samples) and Fraction (if available) column values using set experiment and set fractions buttons.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Group-specific parameters&lt;/strong&gt; pane&lt;br /&gt;
Change Type as Reporter ion MS3 and then select “10plex TMT”. Update isobaric impurities values, if available. For Modifications, select Variable modifications as Oxidation (M), Acetyl (Protein N-term); Deamidation (NQ) and Fixed Modifications as Carbamidomethyl© and, TMT labeled N- terminus and lysine residue. Specify Trypsin/P as a cleavage enzyme using Digestion and allow up to 2 missing cleavages. Set Label-free quantification as None.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global parameters&lt;/strong&gt; pane&lt;br /&gt;
For Sequences select a reference proteome database Uniprot FASTA file and update Max. peptide mass [Da] as 6000. For Protein quantification select Use only unmodified peptides and a list of modifications such as Oxidation (M), Acetyl (Protein N-term) and Deamidation. Select Match between runs in Identification tab.&lt;/p&gt;

&lt;p&gt;Update the following two parameters for MS/MS analyzer:&lt;br /&gt;
a. FTMS MS/MS match tolerance: 0.05 Da&lt;br /&gt;
b. ITMS MS/MS match tolerance: 0.6 Da&lt;/p&gt;

&lt;p&gt;For Folder location select tmp directory (optional).&lt;/p&gt;

&lt;p&gt;Further data processing is performed using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Perseus v1.6.12.0&lt;/code&gt; &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. The search results in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ProteinGroups.txt&lt;/code&gt; generated by MaxQuant are directly processed by Perseus software. MaxQuant reports the TMT-MS3 quantitative relative abundance metrics in the columns titled “Reporter intensity corrected”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;br /&gt;
Data normalization and analysis in multiple TMT experimental designs &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
Multiplexed Protein Quantification Using the Isobaric TMT … &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
Isobaric matching between runs and novel PSM-level normalization in MaxQuant … &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;AWS Windows instance for &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src&quot;&gt;MaxQuant/Perseus&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Bąchor, R.; Waliczek, M.; Stefanowicz, P.; Szewczuk, Z. Trends in the Design of New Isobaric Labeling Reagents for Quantitative Proteomics. Molecules 2019, 24, 701. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;A. Hogrebe, L. von Stechow, D.B. Bekker-Jensen, B.T. Weinert, C.D. Kelstrup, J.V. Olsen Benchmarking common quantification strategies for large-scale phosphoproteomics Nat. Commun., 9 (2018), p. 1045. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=maxquant:start&quot;&gt;MaxQuant&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=perseus:start&quot;&gt;Perseus&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://pwilmart.github.io/TMT_analysis_examples/multiple_TMT_MQ.html&quot;&gt;TMT Batch Correction&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://assets.thermofisher.com/TFS-Assets/CMD/Reference-Materials/PP-TMT-Multiplexed-Protein-Quantification-HUPO2015-EN.pdf&quot;&gt;TMT analysis using Proteome Discoverer&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://doi.org/10.1101/2020.03.30.015487&quot;&gt;Isobaric matching between runs and novel PSM-level normalization&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="Proteomics" /><category term="TMT" /><category term="tandem mass tag" /><category term="MS3 spectra" /><category term="MaxQuant" /><category term="isobaric labeling" /><category term="Perseus" /><category term="TMT 10plex" /><category term="10plex" /><category term="Bioinformatics" /><category term="Emory University" /><summary type="html">Quantification of proteins using isobaric labeling (tandem mass tag or TMT) starts with the reduction of disulfide bonds in proteins with Dithiothreitol (DTT).</summary></entry><entry><title type="html">Quantitative proteomics: label-free quantitation of proteins</title><link href="http://localhost:4000/posts/2018/04/blog-post-lfq/" rel="alternate" type="text/html" title="Quantitative proteomics: label-free quantitation of proteins" /><published>2020-04-04T00:00:00-07:00</published><updated>2020-04-04T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/04/blog-lfq</id><content type="html" xml:base="http://localhost:4000/posts/2018/04/blog-post-lfq/">&lt;p&gt;Liquid chromatography (LC) coupled with mass spectrometry (MS) has been widely used for protein expression quantification.&lt;/p&gt;

&lt;p&gt;Protein quantification by tandem-MS (MS/MS) uses integrated peak intensity from the parent-ion mass (MS1) or features from fragment-ions (MS2). MS1 methods use the iBAQ (intensity Based Absolute Quantification) algorithm (a protein’s total non-normalised intensities are divided by the number of measurable tryptic peptides). Untargeted label-free quantitation (LFQ) of proteins, aims to determine the relative amount of proteins in two or more biological samples.&lt;/p&gt;

&lt;p&gt;Mass spectrometer generated &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;raw&lt;/code&gt; files are used for label-free quantitation of proteins. Base peak chromatograms are inspected visually using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RawMeat&lt;/code&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, which  is a data quality assessment tool designed for Thermo instruments. All raw files are processed together in a single run by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MaxQuant v1.6.14&lt;/code&gt; &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; with defaul parameters except the following&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Raw data&lt;/code&gt; pane&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load all raw data samples of a single run.&lt;/li&gt;
  &lt;li&gt;Select a sample file and edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Set experiment&lt;/code&gt; to assign a unique ID to each sample. If you don´t numerate the biological replicates MaxQuant will put them together in the output.&lt;/li&gt;
  &lt;li&gt;Select a sample file and edit &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Set fractions&lt;/code&gt; to assign fraction value. If you don’t have a fractionation set all file or samples with 1.&lt;/li&gt;
  &lt;li&gt;Number of processors: 4&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Group-specific parameters&lt;/code&gt; pane&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Type: Standard and Multiplicity: 1&lt;/li&gt;
  &lt;li&gt;Modifications:&lt;br /&gt;
  a. Variable modifications: Oxidation(M); Acetyl (Protein N-term); Deamidation (NQ)&lt;br /&gt;
  b. Fixed modifications: Carbamidomethyl ( C )&lt;/li&gt;
  &lt;li&gt;Digestion: trypsin&lt;/li&gt;
  &lt;li&gt;Instrument: Orbitrap&lt;/li&gt;
  &lt;li&gt;Label-free quantification: LFQ (LFQ min. ratio count: 2)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Global parameters&lt;/code&gt; pane&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sequences:&lt;br /&gt;
 a. Add D:\Proteomics\HUMAN.fasta (from UNIPROT)&lt;br /&gt;
 b. Identifier rule: Uniprot identifier&lt;br /&gt;
 c. Min. peptide length: 6&lt;br /&gt;
 d. Max. peptide mass [Da]: 6000&lt;/li&gt;
  &lt;li&gt;Protein quantification:&lt;br /&gt;
 a. Label min. ratio count: 1&lt;br /&gt;
 b. Peptides for quantification: Unique+razor&lt;br /&gt;
 c. Modifications used in protein quantification: Oxidation(M); Acetyl (Protein N-term); Deamidation (NQ)&lt;br /&gt;
 d. Discard unmodified counterpart peptides: FALSE&lt;/li&gt;
  &lt;li&gt;Tables&lt;br /&gt;
 a. Write msScans tabls: TRUE&lt;/li&gt;
  &lt;li&gt;MS/MS analyzer&lt;br /&gt;
 a. FTMS MS/MS match tolerance: 0.05 Da&lt;br /&gt;
 b. ITMS MS/MS match tolerance: 0.6 Da&lt;/li&gt;
  &lt;li&gt;Identification:&lt;br /&gt;
 a. Match between runs: TRUE&lt;br /&gt;
 b. Find dependent peptides: FALSE&lt;br /&gt;
 c. Razor protein FDR: TRUE&lt;/li&gt;
  &lt;li&gt;Label free quantification&lt;br /&gt;
 a. iBAQ: TRUE&lt;br /&gt;
 b. Separate LFQ in parameter groups: TRUE&lt;/li&gt;
  &lt;li&gt;Folder locations&lt;br /&gt;
 a. Temporary folder: D:\tmp&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Database searches are performed using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Andromeda&lt;/code&gt; search engine (a peptide search engine based on probabilistic scoring) with the UniProt-SwissProt human canonical database as a reference and a contaminants database of common laboratory contaminants. MaxQuant reports summed intensity for each protein, as well as its iBAQ value. Proteins that share all identified peptides are combined into a single protein group. Peptides that match multiple protein groups (“razor” peptides) are assigned to the protein group with the most unique peptides. MaxQuant employs the MaxLFQ algorithm for label-free quantitation (LFQ). Quantification will be performed using razor and unique peptides, including those modified by acetylation (protein N-terminal), oxidation (Met) and deamidation (NQ). &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PTXQC&lt;/code&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; is used for general quality control of proteomics data, which takes MaxQuant result files.&lt;/p&gt;

&lt;p&gt;Data processing is performed using `Perseus v1.6.12.0)&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. In brief, protein group LFQ intensities are log2-transformed to reduce the effect of outliers. To overcome the obstacle of missing LFQ values, missing values are imputed before fit the models. Hierarchical clustering is performed on Z-score normalized, log2-transformed LFQ intensities. Log ratios are calculated as the difference in average log2 LFQ intensity values between experimental and control groups. Two-tailed, Student’s t test calculations are used in statistical tests. A protein is considered statistically significant if its fold change is ≥ 2 and FDR ≤ 0.01. All the identified differentially expressed proteins are used in protein network or pathway analysis. In addition to the above analytical considerations, good experimental design helps effectively identify true differences in the presence of variability from various sources and also avoids bias during data acquisition.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;AWS Windows instance for &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src&quot;&gt;MaxQuant/Perseus&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proteomicsresource.washington.edu/protocols06/&quot;&gt;RawMeat&lt;/a&gt; is a nice Thermo raw file diagnostic tool developed by the now defunct Vast Scientific &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=maxquant:start&quot;&gt;MaxQuant&lt;/a&gt; is a quantitative proteomics software package designed for analyzing large mass-spectrometric data sets &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/cbielow/PTXQC&quot;&gt;PTXQC&lt;/a&gt;, an R-based quality control pipeline called Proteomics Quality Control &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=perseus:start&quot;&gt;Perseus&lt;/a&gt; is software package for shotgun proteomics data analyses &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="Proteomics" /><category term="LFQ" /><category term="label free quantitation" /><category term="iBAC" /><category term="MaxQuant" /><category term="Perseus" /><category term="Emory University" /><category term="Bioinformatics" /><summary type="html">Liquid chromatography (LC) coupled with mass spectrometry (MS) has been widely used for protein expression quantification.</summary></entry><entry><title type="html">Spatial RNA-seq data analysis using Space Ranger on SGE Cluster</title><link href="http://localhost:4000/posts/2020/03/blog-spatial-gene-expression/" rel="alternate" type="text/html" title="Spatial RNA-seq data analysis using Space Ranger on SGE Cluster" /><published>2020-03-06T00:00:00-08:00</published><updated>2020-03-06T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/03/blog-spatial-expression</id><content type="html" xml:base="http://localhost:4000/posts/2020/03/blog-spatial-gene-expression/">&lt;p&gt;Running &lt;a href=&quot;https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/what-is-space-ranger&quot;&gt;spaceranger&lt;/a&gt; as cluster mode that uses Sun Grid Engine (SGE) as queuing.&lt;/p&gt;

&lt;p&gt;There are 2 steps to analyze Spatial RNA-seq data&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger mkfastq&lt;/code&gt; demultiplexes raw base call (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BCL&lt;/code&gt;) files generated by Illumina sequencers into FASTQ files.&lt;br /&gt;
&lt;strong&gt;Step 2&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger count&lt;/code&gt; takes FASTQ files from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger mkfastq&lt;/code&gt; and performs alignment, filtering, barcode counting, and UMI counting.&lt;/p&gt;

&lt;p&gt;Running pipelines on cluster requires the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Load Space Ranger module (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger-1.0.0&lt;/code&gt;)&lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or, download and uncompress spaceranger at your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME&lt;/code&gt; directory and add PATH in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Update job config file (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger-1.0.0/external/martian/jobmanagers/config.json&lt;/code&gt;) for threads and memory. For example&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;threads_per_job&quot;: 8,&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;memGB_per_job&quot;: 64,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Update template file (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger-1.0.0/external/martian/jobmanagers/sge.template&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#!/bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -pe smp __MRO_THREADS__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;##$ -l mem_free=__MRO_MEM_GB__G&lt;/code&gt; (comment this line if your cluster do not support it!)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -q b.q&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -S /bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -m abe&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -M &amp;lt;e-mail&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd __MRO_JOB_WORKDIR__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source ../spaceranger-1.0.0/sourceme.bash&lt;/code&gt; (update with complete path)&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For clusters whose job managers do not support memory requests, it is possible to request memory 
in the form of cores via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mempercore&lt;/code&gt; command-line option. This option scales up the number 
of threads requested via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__MRO_THREADS__&lt;/code&gt; variable according to how much memory a stage requires&lt;/em&gt;.&lt;br /&gt;
Read more at &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&quot;&gt;Cluster Mode&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. Download spatial gene expression, image file and reference genome datasets from &lt;a href=&quot;https://www.10xgenomics.com/resources/datasets/&quot;&gt;10XGenomics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;. Create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sge.sh&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TR=&quot;$HOME/refdata-cellranger-mm10-3.0.0&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Output files will appear in the out/ subdirectory within this pipeline output directory.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd $HOME/10xgenomics/out&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For pipeline output directory, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--id&lt;/code&gt; argument is used i.e Adult_Mouse_Brain.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/V1_Adult_Mouse_Brain_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spaceranger count --disable-ui \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--id=Adult_Mouse_Brain \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--sample=V1_Adult_Mouse_Brain \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--image=$DATA_DIR/V1_Adult_Mouse_Brain_image.tif \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--slide=V19L01-041 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--area=C1 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobinterval=5000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--maxjobs=3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;. Execute a command in screen and, detach and reconnect&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen&lt;/code&gt; command to get in/out of the system while keeping the processes running.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -S screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash sge.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want to exit the terminal without killing the running process, simply press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl+A+D&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To reconnect to the screen: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -R screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7&lt;/strong&gt;. Monitor work progress through a web browser&lt;/p&gt;

&lt;p&gt;Open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_log&lt;/code&gt; file present in output folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Adult_Mouse_Brain&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see serving UI as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://cluster.university.edu:3600?auth=rlSdT_QLzQ9O7fxEo-INTj1nQManinD21RzTAzkDVJ8&lt;/code&gt;, then type the following from your laptop&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -NT -L 9000:cluster.university.edu:3600 user@cluster.university.edu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user@cluster.university.edu's password:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then access the UI using the following URL in your web browser
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:9000/&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/what-is-space-ranger&quot;&gt;10XGenomics- Visium spatial RNA-seq&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="10XGenomics" /><category term="SPATIAL GENE EXPRESSION" /><category term="Visium Spatial Gene Expression Solution" /><category term="brightfield microscope image" /><category term="Space Ranger" /><category term="SGE Cluster" /><category term="Cluster Computing" /><summary type="html">Running spaceranger as cluster mode that uses Sun Grid Engine (SGE) as queuing.</summary></entry><entry><title type="html">ATAC-seq peak calling with MACS2</title><link href="http://localhost:4000/posts/2019/12/blog-post-atacseq/" rel="alternate" type="text/html" title="ATAC-seq peak calling with MACS2" /><published>2019-12-05T00:00:00-08:00</published><updated>2019-12-05T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/12/blog-atacseq</id><content type="html" xml:base="http://localhost:4000/posts/2019/12/blog-post-atacseq/">&lt;p&gt;ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing) is a next-generation sequencing approach for the analysis of open chromatin regions to assess the genome-wise chromatin accessibility.&lt;/p&gt;

&lt;p&gt;ATAC-seq achieves this by simultaneously fragmenting and tagging genomic DNA with sequencing adapters using the hyperactive Tn5 transposase enzyme &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Other global chromatin accessibility methods include FAIRE-seq and DNase-seq. This document aims to provide accessibility.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pre-processing of raw sequencing reads&lt;/strong&gt; – before mapping the raw reads to the genome, trim the adapter sequences. Poor read quality or sequencing
errors often lead to low mapping rate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapping/alignment of sequencing reads to a reference genome&lt;/strong&gt; – use Burrows-Wheeler Aligner (BWA) for mapping of sequencing reads. The output alignment file will be saved as a sequence alignment/map (SAM) format or binary version of SAM called BAM. Mark the duplicate reads using Picard &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and exclude reads mapping to mitochondrial DNA and other chromosomes from analysis together with low quality reads (MAPQ&amp;lt;10 and reads in Encode black list regions) using SAMtools &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Filtering and shifting of the mapped reads&lt;/strong&gt; - shift the read position +4 and -5 bp in the BAM file before peak calling &lt;a href=&quot;https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/&quot;&gt;adjust the reads alignment&lt;/a&gt;. When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site. Picard CollectInsertSizeMetrics will be used to compute the fragment sizes on alignment shifted BAM files.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Identification and visualization of the ATAC-seq peaks&lt;/strong&gt; – use MACS2 for peak calling with the parameters nomodel or BAMPE &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and identify the differentially enriched peaks using the MACS2 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bdgdiff&lt;/code&gt; module. Individual peaks separated by &amp;lt;100 bp will be join together. For peak annotation and functional analysis use the R package ChIPpeakAnno or HOMER &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. First, ATAC-seq peaks will be categorized into different groups based on the nearest RefSeq gene i.e. promoter, untranslated regions (UTRs), intron and exon. Second, peaks that are within 5 kb upstream and 3 kb downstream of the Transcription Start Site (TSS) are associated to the nearest genes. Finally, these genes are then analyzed for over-represented gene ontology (GO) terms and KEGG pathways using ChIPpeakAnno. Visualize all sequencing tracks using the Integrated Genomic Viewer (IGV) &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Scripts are available for HPC &lt;a href=&quot;https://bitbucket.org/adinasarapu/atac_seq-data-analysis/src/master/&quot;&gt;Cluster&lt;/a&gt;.&lt;br /&gt;
For further reading: &lt;a href=&quot;https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/&quot;&gt;ATAC-seq-data-analysis-from-FASTQ-to-peaks&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/24097267&quot;&gt;ATAC-seq&lt;/a&gt; - Nature Methods. 2013; 10:1213–1218. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://broadinstitute.github.io/picard/&quot;&gt;PICARD&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.htslib.org&quot;&gt;HTSLIB&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/taoliu/MACS&quot;&gt;MACS&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-237&quot;&gt;ChIPpeakAnno&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://homer.ucsd.edu/homer/ngs/&quot;&gt;HOMER&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://software.broadinstitute.org/software/igv/home&quot;&gt;IGV&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing)" /><category term="MACS2" /><category term="BWA" /><category term="HOMER" /><summary type="html">ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing) is a next-generation sequencing approach for the analysis of open chromatin regions to assess the genome-wise chromatin accessibility.</summary></entry><entry><title type="html">Single cell RNA-seq data analysis using CellRanger and Seurat on Cluster</title><link href="http://localhost:4000/posts/2019/01/blog-post-sc-ranseq/" rel="alternate" type="text/html" title="Single cell RNA-seq data analysis using CellRanger and Seurat on Cluster" /><published>2019-11-18T00:00:00-08:00</published><updated>2019-11-18T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/01/blog-sc-rnaseq</id><content type="html" xml:base="http://localhost:4000/posts/2019/01/blog-post-sc-ranseq/">&lt;p&gt;Running &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger&quot;&gt;cellranger&lt;/a&gt; as cluster mode that uses Sun Grid Engine (SGE) as queuing system allows highly parallelizable jobs.&lt;/p&gt;

&lt;p&gt;There are 4 steps to analyze Chromium Single Cell data&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger mkfastq&lt;/code&gt; demultiplexes raw base call (BCL) files generated by Illumina sequencers into FASTQ files.&lt;br /&gt;
&lt;strong&gt;Step 2&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger count&lt;/code&gt; takes FASTQ files from cellranger mkfastq and performs alignment, filtering, barcode counting, and UMI counting. When doing large studies involving multiple GEM wells, run cellranger count on FASTQ data from each of the GEM wells individually, and then pool the results using cellranger aggr, as described &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
&lt;strong&gt;Step 3&lt;/strong&gt;: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger aggr&lt;/code&gt; aggregates outputs from multiple runs of cellranger count.&lt;br /&gt;
&lt;strong&gt;Step 4&lt;/strong&gt;: Downstream/Secondary analysis using R package &lt;a href=&quot;https://satijalab.org/seurat/&quot;&gt;Seurat v3.0&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Running pipelines on cluster requires the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Load Cell Ranger module (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger-3.1.0&lt;/code&gt;)&lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or, download and uncompress cellranger at your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$HOME&lt;/code&gt; directory and add PATH in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Update job config file (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger-3.1.0/martian-cs/v3.2.3/jobmanagers/config.json&lt;/code&gt;) for threads and memory. For example&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;threads_per_job&quot;: 20,&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&quot;memGB_per_job&quot;: 150,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Update template file (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger-3.1.0/martian-cs/v3.2.3/jobmanagers/sge.template&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#!/bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -pe smp __MRO_THREADS__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;##$ -l mem_free=__MRO_MEM_GB__G&lt;/code&gt; (comment this line if your cluster do not support it!)&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -q b.q&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -S /bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -m abe&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#$ -M &amp;lt;e-mail&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd __MRO_JOB_WORKDIR__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;source $HOME/cellranger-3.1.0/sourceme.bash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For clusters whose job managers do not support memory requests, it is possible to request memory 
in the form of cores via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mempercore&lt;/code&gt; command-line option. This option scales up the number 
of threads requested via the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__MRO_THREADS__&lt;/code&gt; variable according to how much memory a stage requires. 
see more at &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&quot;&gt;Cluster Mode&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. Download single cell gene expression and reference genome datasets from &lt;a href=&quot;https://www.10xgenomics.com/resources/datasets/&quot;&gt;10XGenomics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;. Create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sge.sh&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TR=&quot;$HOME/refdata-cellranger-GRCh38-3.0.0&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 3′ Gene expression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Output files will appear in the out/ subdirectory within this pipeline output directory.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cd $HOME/10xgenomics/out&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For pipeline output directory, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--id&lt;/code&gt; argument is used i.e 10XGTX_v3.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/pbmc_10k_v3_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger count --disable-ui \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--id=10XGTX_v3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--sample=pbmc_10k_v3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--expect-cells=10000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobinterval=5000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--maxjobs=3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 5′ gene expression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/vdj_v1_hs_nsclc_5gex_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-either-force-cells-or-expect-cells&quot;&gt;use either –force-cells or –expect-cells&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger count \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--id=10XGTX_v5 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--sample=vdj_v1_hs_nsclc_5gex \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--force-cells=7802 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--maxjobs=3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobinterval=2000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 5′ gene expression and cell surface protein (Feature Barcoding/Antibody Capture Assay)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LIBRARY=$HOME/vdj_v1_hs_pbmc2_5gex_protein_fastqs/vdj_v1_hs_pbmc2_5gex_protein_library.csv&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FEATURE_REF=$HOME/vdj_v1_hs_pbmc2_5gex_protein_fastqs/vdj_v1_hs_pbmc2_5gex_protein_feature_ref.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cellranger count \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--libraries=${LIBRARY} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--feature-ref=${FEATURE_REF} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--id=PBMC_5GEX \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--expect-cells=9000 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--maxjobs=3 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--jobinterval=5000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;. Execute a command in screen and, detach and reconnect&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen&lt;/code&gt; command to get in/out of the system while keeping the processes running.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -S screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bash sge.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want to exit the terminal without killing the running process, simply press &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Ctrl+A+D&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To reconnect to the screen: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;screen -R screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7&lt;/strong&gt;. Monitor work progress through a web browser&lt;/p&gt;

&lt;p&gt;Open &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_log&lt;/code&gt; file present in output folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PBMC_5GEX&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see serving UI as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://cluster.university.edu:3600?auth=rlSdT_QLzQ9O7fxEo-INTj1nQManinD21RzTAzkDVJ8&lt;/code&gt;, then type the following from your laptop&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh -NT -L 9000:cluster.university.edu:3600 user@cluster.university.edu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;user@cluster.university.edu's password:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then access the UI using the following URL in your web browser
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://localhost:9000/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8&lt;/strong&gt;. Single Cell Integration in Seurat v3.0&lt;/p&gt;

&lt;p&gt;Seurat is an R package designed for QC, analysis, and exploration of single cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. Seurat starts by reading cellranger data (barcodes.tsv.gz, features.tsv.gz and matrix.mtx.gz)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pbmc.data &amp;lt;- Read10X(data.dir = &quot;~/PBMC_5GEX/outs/filtered_feature_bc_matrix/&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/overview/welcome&quot;&gt;10XGenomics&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://satijalab.org/seurat/&quot;&gt;Seurat&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="10XGenomics" /><category term="Chromium Single Cell Gene Expression" /><category term="Single cell RNA-sequencing (scRNA-seq)" /><category term="Single Cell" /><category term="Cell Ranger" /><category term="Seurat" /><category term="Cluster Computing" /><category term="Feature Barcoding" /><summary type="html">Running cellranger as cluster mode that uses Sun Grid Engine (SGE) as queuing system allows highly parallelizable jobs.</summary></entry><entry><title type="html">Taxonomic and diversity profiling of the microbiome - 16S rRNA gene amplicon sequence data</title><link href="http://localhost:4000/posts/2019/01/blog-post-qiime2/" rel="alternate" type="text/html" title="Taxonomic and diversity profiling of the microbiome - 16S rRNA gene amplicon sequence data" /><published>2019-01-01T00:00:00-08:00</published><updated>2019-01-01T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/01/blog-qiime2</id><content type="html" xml:base="http://localhost:4000/posts/2019/01/blog-post-qiime2/">&lt;p&gt;The 16S ribosomal RNA (rRNA) gene of Bacteria codes for the RNA component of the 30S subunit. Different bacterial species have one to multiple copies of the 16S rRNA gene, and each with 9 hypervariable regions, V1-V9. High-throughput sequencing of 16S rRNA gene (a “marker gene”) amplicons has become a widely used method to study bacterial phylogeny and species classification.&lt;/p&gt;

&lt;p&gt;Quantitative Insights Into Microbial Ecology “QIIME” 2 (release 2018.11)&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is a widely used package to identity abundance of microbes using 16s rRNA. Briefly, feature table containing counts of each unique sequence in the samples will be constructed using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;qiime dada2 denoise-paired&lt;/code&gt; method. A feature is essentially any unit of observation, e.g., an OTU (Operational Taxonomic Unit), a sequence variant, a gene or a metabolite. In QIIME2 (currently), most features will be OTUs or sequence variants (alternatively, for OTUs, use QIIME2 plugin &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;q2-vsearch&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Data produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact typically has the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.qza&lt;/code&gt; file extension when output data stored in a file. Visualizations are another type of data (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.qzv&lt;/code&gt; file extension) generated by QIIME 2, which can be viewed using a web interface &lt;a href=&quot;https://view.qiime2.org&quot;&gt;https://view.qiime2.org&lt;/a&gt; (at Firefox web browser) without requiring a QIIME installation. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), we must create a QIIME 2 artifact by importing our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fastq.gz&lt;/code&gt; data files.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Scripts are available for AWS &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_qiime2/src&quot;&gt;Cloud&lt;/a&gt; and HPC &lt;a href=&quot;https://bitbucket.org/adinasarapu/cluster_qiime2/src&quot;&gt;Cluster&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://qiime2.org&quot;&gt;https://qiime2.org&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="microbiome" /><category term="computing" /><category term="qiime2" /><category term="16s rRNA amplicon" /><category term="DADA2" /><category term="OTU" /><summary type="html">The 16S ribosomal RNA (rRNA) gene of Bacteria codes for the RNA component of the 30S subunit. Different bacterial species have one to multiple copies of the 16S rRNA gene, and each with 9 hypervariable regions, V1-V9. High-throughput sequencing of 16S rRNA gene (a “marker gene”) amplicons has become a widely used method to study bacterial phylogeny and species classification.</summary></entry><entry><title type="html">Taxonomic and functional profiling of the microbiome - whole genome shotgun metagenomics</title><link href="http://localhost:4000/posts/2018/07/blog-post-metagenomics/" rel="alternate" type="text/html" title="Taxonomic and functional profiling of the microbiome - whole genome shotgun metagenomics" /><published>2018-07-27T00:00:00-07:00</published><updated>2018-07-27T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/07/blog-metagenomics</id><content type="html" xml:base="http://localhost:4000/posts/2018/07/blog-post-metagenomics/">&lt;p&gt;This workflow consists of taxonomic and functional profiling of shotgun metagenomics sequencing (MGS) reads using MetaPhlAn2 and HUMAnN2, respectively. To perform taxonomic (phyla, genera or species level) profiling of the MGS data, the MetaPhlAn2 pipeline was run on a high performance multicore cluster computing environment.&lt;/p&gt;

&lt;p&gt;MetaPhlAn2 provides microbial (bacterial, archaeal, viral, and eukaryotic) taxonomic profiling allowing the quantification of individual species across metagenomic samples. MetaPhlAn2 relies on ~1M unique clade-specific marker genes identified from ~17,000 reference genomes. Microbial reads, aligned by MetaPhlAn2, belonging to clades with no sequenced genomes available are reported as an “unclassified” subclade of the closest ancestor with available sequence data.
HUMAnN2 utilizes the MetaCyc database as well as the UniRef gene family catalog to characterize the microbial pathways present in samples. HUMAnN2 relies on programs such as BowTie (for accelerated nucleotide-level searches) and Diamond (for accelerated translated searches) to compute the abundance of gene families and metabolic pathways present. HUMAnN2 generates three outputs: 1) gene families based on UniRef proteins and their abundances reported in reads per kilobase, 2) MetaCyc pathways and their coverage, and 3) MetaCyc pathways and their abundances reported in reads per kilobase.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Scripts are available at &lt;a href=&quot;https://bitbucket.org/adinasarapu/shotgun_metagenomics/src&quot;&gt;shotgun_metagenomics&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name>Ashok R. Dinasarapu</name></author><category term="metagenomics" /><category term="MetaPhlAn2" /><category term="HUMAnN2" /><category term="taxonomic profiling" /><category term="functional profiling" /><category term="shotgun metagenomics sequencing" /><summary type="html">This workflow consists of taxonomic and functional profiling of shotgun metagenomics sequencing (MGS) reads using MetaPhlAn2 and HUMAnN2, respectively. To perform taxonomic (phyla, genera or species level) profiling of the MGS data, the MetaPhlAn2 pipeline was run on a high performance multicore cluster computing environment.</summary></entry></feed>