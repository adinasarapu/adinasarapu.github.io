<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-02-09T15:06:02-08:00</updated><id>http://localhost:4000/</id><title type="html">Ashok R. Dinasarapu</title><subtitle>personal description</subtitle><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><entry><title type="html">Building a real-time big data pipeline (part 2: Spark, Hadoop)</title><link href="http://localhost:4000/posts/2020/02/blog-post-spark/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 2: Spark, Hadoop)" /><published>2020-02-09T00:00:00-08:00</published><updated>2020-02-09T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/02/blog-spark</id><content type="html" xml:base="http://localhost:4000/posts/2020/02/blog-post-spark/">&lt;p&gt;Apache Spark&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is a unified analytics engine for large-scale data processing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What exactly is Apache Spark?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A Cluster computing engine and a set of libraries, application programming interfaces (APIs) together make apache spark. The spark core itself has two parts. 1. Computing engine and 2. Spark Core APIs (Scala, Java, Python and R).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Apache Spark Ecosystem&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+--------+-----------+-------+----------+
| SQL	 | Streaming | MLlib |	GraphX 	|
|---------------------------------------|
|	Spark Core API			|	
|---------------------------------------|
| Scala	| Python    |	Java |	R	|
|---------------------------------------|	
|	Compute Engine			|
+---------------------------------------+
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Computing Engine&lt;/strong&gt;&lt;br /&gt;
Apache Hadoop&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; offers distributed storage (HDFS), resource manager (YARN)  and computing framework (Map Reduce). Apache Spark is a distributed processing engine but it doesn’t come with inbuilt cluster resource manager and distributed storage system. We have to plugin a cluster manager and a storage system of our choice. We can use YARN, Mesos, and Kubernetes as a cluster manager for Apache Spark. Similarly, for the storage system we can use HDFS, Amazon S3, Google cloud storage or Cassandra File System (CFS) and more. The compute engine provides some basic functionality like memory management, task scheduling, fault recovery and most importantly interacting with the cluster manager and storage system.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Core APIs&lt;/strong&gt;
Spark core consists of structured API and unstructured API. Structured API consists of Data Frames and Data Sets. Unstructured APIs consists of RDDs, accumulators and broadcast variables&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. These core APIs are available as Scala, Java, Python and R.  &lt;br /&gt;
Outside of Spark Core we have 4 sets of libraries and packages, Spark SQL, Spark Streaming, MLlib and Graphx. They directly depend on Spark Core APIs to achieve distributed processing.&lt;/p&gt;

&lt;p&gt;Typical Spark Application Process Flow: Apache Spark reads some data from source and load it into a Spark. There are 3 alternatives to hold data in Spark. 1) Data Frame 2) Data Set and 3) RDD. Latest Spark release recommended to use Data Frame and Data Set. Both of them are compiled down in RDD.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RDD: Resilient Distributed Data Set&lt;/strong&gt; &lt;sup id=&quot;fnref:3:1&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Spark RDD is a resilient, partitioned, distributed and immutable collection of data.&lt;br /&gt;
&lt;strong&gt;Resilient&lt;/strong&gt; – RDDs are fault tolerant.&lt;br /&gt;
&lt;strong&gt;Partitioned&lt;/strong&gt; – Spark breaks the RDD into smaller chunks of data. These pieces are called partitions.&lt;br /&gt;
&lt;strong&gt;Distributed&lt;/strong&gt; – Instead of keeping these partitions on a single machine, Spark spreads them across the cluster.&lt;br /&gt;
&lt;strong&gt;Immutable&lt;/strong&gt; – Once defined, you can’t change them. So Spark RDD is a read-only data structure.&lt;/p&gt;

&lt;p&gt;For RDDs vs DataFrames and Datasets - When to use them and why. Read &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;We can create RDD using two methods. 1.Load some data from a source or 2. Create an RDD by transforming another RDD&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Hadoop installation&lt;/p&gt;

&lt;p&gt;See tutorial on &lt;a href=&quot;https://www.quickprogrammingtips.com/big-data/how-to-install-hadoop-on-mac-os-x-el-capitan.html&quot;&gt;How to Install and Configure Hadoop on Mac&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Update your &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bash_profile&lt;/code&gt; file, which is a configuration file for configuring user environments.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$vi ~/.bash_profile  
export HADOOP_HOME=/Users/adinasarapu/Documents/hadoop-3.1.3  
export PATH=$PATH:$HADOOP_HOME/bin  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Start Hadoop&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$bash sbin/start-dfs.sh 
Starting namenodes on [localhost]
Starting datanodes
Starting secondary namenodes [Ashoks-MacBook-Pro.local]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Verify Hadoop installation&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ jps
61073 ResourceManager
82025 SecondaryNameNode
61177 NodeManager
81882 DataNode
82303 Jps
81774 NameNode
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Create user&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$hadoop fs -mkdir -p /user/adinasarapu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Move file to HDFS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$hadoop fs -copyFromLocal samples.csv /user/adinasarapu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Now the data file is at real distributed storage. The file location at HDFS is &lt;code class=&quot;highlighter-rouge&quot;&gt;hdfs://localhost:9000/user/adinasarapu/ samples.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;List files moved&lt;/strong&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;$ hadoop fs -ls /user/adinasarapu&lt;/code&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Found 3 items
-rw-r--r--   1 adinasarapu supergroup  110252495 2020-02-08 17:04 /user/adinasarapu/flist.txt
-rw-r--r--   1 adinasarapu supergroup       1318 2020-02-09 14:47 /user/adinasarapu/samples.csv
-rw-r--r--   1 adinasarapu supergroup     303684 2020-02-09 08:21 /user/adinasarapu/survey.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Apache Spark installation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;See tutorial on &lt;a href=&quot;https://medium.com/luckspark/installing-spark-2-3-0-on-macos-high-sierra-276a127b8b85&quot;&gt;Installing Apache Spark … on macOS&lt;/a&gt;&lt;br /&gt;
Update your &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bash_profile&lt;/code&gt; file, which is a configuration file for configuring user environments.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$vi ~/.bash_profile  
export SPARK_HOME=/Users/adinasarapu/Documents/spark-3.0.0-preview2-bin-hadoop3.2  
export PATH=$PATH:$SPARK_HOME/bin  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Start Spark shell&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$spark-shell  
20/02/09 15:03:23 ..  
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties  
Setting default log level to &quot;WARN&quot;.  
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).  
Spark context Web UI available at http://192.168.0.5:4040  
Spark context available as 'sc' (master = local[*], app id = local-1581278612106).  
Spark session available as 'spark'.  
Welcome to  
      ____              __  
     / __/__  ___ _____/ /__  
    _\ \/ _ \/ _ `/ __/  '_/  
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0-preview2  
      /_/  
           
Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_102)  
Type in expressions to have them evaluated.  
Type :help for more information.  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Read CSV data&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df = spark.read.options(Map(  
	&quot;header&quot; -&amp;gt; &quot;true&quot;,  
	&quot;inferSchema&quot;-&amp;gt;&quot;true&quot;,  
	&quot;nullValue&quot;-&amp;gt;&quot;NA&quot;,  
	&quot;timestampFormat&quot;-&amp;gt;&quot;MM-dd-yyyy&quot;,  
	&quot;mode&quot;-&amp;gt;&quot;failfast&quot;)).csv(&quot;hdfs://localhost:9000/user/adinasarapu/samples.csv&quot;)  

df: org.apache.spark.sql.DataFrame = [Sample: string, p16: string ... 7 more fields]
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Check the number of partitions&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.rdd.getNumPartitions  
res4: Int = 1  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Set/increase Number of partitions to 3&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df2 = df.repartition(3).toDF  
df2: org.apache.spark.sql.DataFrame = [Sample: string, p16: string ... 7 more fields]  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Recheck the number of partitions&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df2.rdd.getNumPartitions  
res5: Int = 3  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;SQL like operation&lt;/strong&gt; &lt;br /&gt;
Data Frame follows row and column structure like a database table. Data Frame compiles down to RDDs. RDDs are immutable and once loaded you can’t modify it. However, you can perform the following operations a) Transformation and b) Actions. Spark Data Frames carries the same legacy from RDDs. Like RDDs, Spark Data Frames are immutable. You can perform transformation and actions on Data Frames.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; df.select(&quot;Sample&quot;,&quot;Age&quot;,&quot;Sex&quot;,&quot;Anatomy&quot;).filter(&quot;Age &amp;lt; 55&quot;).show  
  
+------+---+------+-------+  
|Sample|Age|   Sex|Anatomy|  
+------+---+------+-------+  
|GHN-57| 50|female|    BOT|  
|GHN-39| 51|  male|    BOT|  
|GHN-60| 41|  male|    BOT|  
|GHN-64| 49|  male|    BOT|  
|GHN-77| 53|  male|    BOT|  
|GHN-66| 52|  male| Tonsil|  
|GHN-67| 54|  male| Tonsil|  
|GHN-68| 51|  male| Tonsil|  
|GHN-80| 54|  male| Tonsil|  
|GHN-83| 54|  male| Tonsil|  
+------+---+------+-------+
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df1 = df.select($&quot;Sex&quot;,$&quot;Radiation&quot;)  
df1: org.apache.spark.sql.DataFrame = [Sex: string, Radiation: string]  

scala&amp;gt; df1.show  
+------+---------+  
|   Sex|Radiation|  
+------+---------+  
|female|        Y|  
|female|        Y|  
|  male|        Y|  
|  male|        N|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        N|  
|  male|        N|  
|  male|  Unknown|  
|  male|        Y|  
|female|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        Y|  
|  male|        N|  
+------+---------+  
only showing top 20 rows  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df2 = df1.select($&quot;Sex&quot;,   
		(when($&quot;Radiation&quot; === &quot;Y&quot;,1).otherwise(0)).alias(&quot;All-Yes&quot;),  
		(when($&quot;Radiation&quot; === &quot;N&quot;,1).otherwise(0)).alias(&quot;All-Nos&quot;),  
		(when($&quot;Radiation&quot; === &quot;Unknown&quot;,1).otherwise(0)).alias(&quot;All-Unknown&quot;))  
df2: org.apache.spark.sql.DataFrame = [Sex: string, All-Yes: int ... 2 more fields]  

scala&amp;gt; df2.show  
+------+-------+-------+-----------+  
|   Sex|All-Yes|All-Nos|All-Unknown|  
+------+-------+-------+-----------+  
|female|      1|      0|          0|  
|female|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      0|      1|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      0|      1|          0|  
|  male|      0|      1|          0|  
|  male|      0|      0|          1|  
|  male|      1|      0|          0|  
|female|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      1|      0|          0|  
|  male|      0|      1|          0|  
+------+-------+-------+-----------+  
only showing top 20 rows  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Scala user-defined function (UDF)&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def parseSex(g: String) = {  
 	g.toLowerCase match {   
			case &quot;male&quot;  =&amp;gt; “Male”  
			case &quot;female&quot; =&amp;gt; “Female”   
			case _ =&amp;gt; “Other”  
	}  
}   

scala&amp;gt; val parseSexUDF = udf(parseSex _)  

parseGenderUDF: org.apache.spark.sql.expressions.UserDefinedFunction = SparkUserDefinedFunction($Lambda$3628/1633612848@4ee80d4c,StringType,List(Some(Schema(StringType,true))),None,true,true)  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df3 = df2.select((parseSexUDF($&quot;Sex&quot;)).alias(&quot;Sex&quot;),$&quot;All-Yes&quot;,$&quot;All-Nos&quot;,$&quot;All-Unknown&quot;)  
df3: org.apache.spark.sql.DataFrame = [Sex: string, All-Yes: int ... 2 more fields]  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df4 = df3.groupBy(&quot;Sex&quot;).agg(sum($&quot;All-Yes&quot;), sum($&quot;All-Nos&quot;), sum($&quot;All-Unknown&quot;))  
df4: org.apache.spark.sql.DataFrame = [Sex: string, sum(All-Yes): bigint ... 2 more fields]  

scala&amp;gt; df4.show  
+------+------------+------------+----------------+  
|   Sex|sum(All-Yes)|sum(All-Nos)|sum(All-Unknown)|  
+------+------------+------------+----------------+  
|Female|           3|           0|               0|  
|  Male|          18|           5|               1|  
+------+------------+------------+----------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scala&amp;gt; val df5 = df4.filter($&quot;Sex&quot; =!= &quot;Unknown&quot;)  
df5: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Sex: string, sum(All-Yes): bigint ... 2 more fields]  
  
scala&amp;gt; df5.collect()  
res22: Array[org.apache.spark.sql.Row] = Array([Male,18,5,1])  

scala&amp;gt; df5.show  
+----+------------+------------+----------------+  
| Sex|sum(All-Yes)|sum(All-Nos)|sum(All-Unknown)|  
+----+------------+------------+----------------+  
|Male|          18|           5|               1|  
+----+------------+------------+----------------+  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://spark.apache.org&quot;&gt;Apache Spark&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://hadoop.apache.org&quot;&gt;Apache Hadoop&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.learningjournal.guru/courses/spark/spark-foundation-training/&quot;&gt;Learning Journal&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html&quot;&gt;RDDs vs Data Frames and Data Sets&lt;/a&gt; A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets - When to use them and why&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="big data" /><category term="apache spark" /><category term="real time data pipelines" /><category term="scala" /><category term="hadoop" /><summary type="html">Apache Spark1 is a unified analytics engine for large-scale data processing. Apache Spark&amp;nbsp;&amp;#8617;</summary></entry><entry><title type="html">Building a real-time big data pipeline (part 1: Apache Kafka)</title><link href="http://localhost:4000/posts/2020/01/blog-post-kafka/" rel="alternate" type="text/html" title="Building a real-time big data pipeline (part 1: Apache Kafka)" /><published>2020-01-26T00:00:00-08:00</published><updated>2020-01-26T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/01/blog-kafka</id><content type="html" xml:base="http://localhost:4000/posts/2020/01/blog-post-kafka/">&lt;p&gt;&lt;a href=&quot;https://kafka.apache.org&quot;&gt;Kafka&lt;/a&gt; is used for building real-time data pipelines and streaming apps.&lt;/p&gt;

&lt;p&gt;What is Kafka? &lt;a href=&quot;https://success.docker.com/article/getting-started-with-kafka&quot;&gt;Getting started with kafka&lt;/a&gt; says “Kafka is a distributed append log; in a simplistic view it is like a file on a filesystem. Producers can append data (echo ‘data’ » file.dat), and consumers subscribe to a certain file (tail -f file.dat). In addition, Kafka provides an ever-increasing counter and a timestamp for each consumed message. Kafka uses Zookeeper (simplified: solid, reliable, transactional key/value store) to keep track of the state of producers, topics, and consumers.”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Kafka for local development of applications&lt;/strong&gt;:&lt;br /&gt;
There are multiple ways of running Kafka locally for development of apps but the easiest method is by docker-compose. To download Docker Desktop, go to &lt;a href=&quot;https://hub.docker.com/&quot;&gt;Docker Hub&lt;/a&gt; and sign In with your Docker ID.&lt;/p&gt;

&lt;p&gt;Docker compose facilitates installing Kafka and Zookeeper with the help of &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;version: '2'  
services:  
  zookeeper:  
    image: wurstmeister/zookeeper  
    ports:  
      - &quot;2181:2181&quot;  
  kafka:  
   image: wurstmeister/kafka  
    ports:  
      - &quot;9092:9092&quot;  
    environment:  
     KAFKA_ADVERTISED_HOST_NAME: localhost  
     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181  
     KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;true&quot;  
    volumes:  
     - /var/run/docker.sock:/var/run/docker.sock
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;1. Start the Kafka service&lt;/strong&gt;&lt;br /&gt;
Open a terminal, go where you have the docker-compose.yml file, and execute the following command. This command starts the docker-compose engine, and it downloads the images and runs them.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose up -d  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Starting kafka_example_zookeeper_1 ... done  
Starting kafka_example_kafka_1     ... done  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;To list running docker containers, run the following command&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose ps  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Name				Command				State	Ports  
kafka_example_kafka_1		start-kafka.sh			Up	0.0.0.0:9092-&amp;gt;9092/tcp                              
kafka_example_zookeeper_1	/bin/sh -c /usr/sbin/sshd  ...	Up	0.0.0.0:2181-&amp;gt;2181/tcp, 22/tcp, 2888/tcp, 3888/tcp  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;You can shut down docker-compose by executing the following command in another terminal.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose down  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Stopping kafka_example_zookeeper_1 ... done  
Stopping kafka_example_kafka_1     ... done  
Removing kafka_example_zookeeper_1 ... done  
Removing kafka_example_kafka_1     ... done  
Removing network kafka_example_default  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Using the following command check the ZooKeeper logs to verify that ZooKeeper is working and healthy.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose logs zookeeper | grep -i binding  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Next, check the Kafka logs to verify that broker is working and healthy.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose logs kafka | grep -i started  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;2. Create a topic&lt;/strong&gt;&lt;br /&gt;
The Kafka cluster stores streams of records in categories called topics. A topic can have zero, one, or many consumers that subscribe to the data written to it. &lt;code class=&quot;highlighter-rouge&quot;&gt;docker-compose exec&lt;/code&gt; command by default allocates a TTY, so that you can use such a command to get an interactive prompt. Go into directory where docker-compose.yml file present, and execute it as&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$docker-compose exec kafka bash  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;(for zookeeper &lt;code class=&quot;highlighter-rouge&quot;&gt;$docker-compose exec zookeeper bash&lt;/code&gt;)&lt;/p&gt;

&lt;p&gt;Change the directory to /opt/kafka/bin where you find scripts such as &lt;code class=&quot;highlighter-rouge&quot;&gt;kafka-topics.sh&lt;/code&gt;.&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd /opt/kafka/bin&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Create, list or delete existing topics:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \  
 --create \  
 --topic test1 \  
 --partitions 1 \  
 --replication-factor 1 \  
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \
 --list \
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;If necessary, delete a topic using the following command.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-topics.sh \
 --delete \
 --topic test1 \
 --bootstrap-server localhost:9092  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3. Producer and Consumer&lt;/strong&gt;&lt;br /&gt;
A Kafka producer is an object that consists of a pool of buffer space that holds records that haven’t yet been transmitted to the server.&lt;/p&gt;

&lt;p&gt;The following is a producer command line to read data from standard input and write it to a Kafka topic.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test1  
&amp;gt;Hello  
&amp;gt;World  
^C
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The following is a command line to read data from a Kafka topic and write it to standard output.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test1 --from-beginning  
Hello  
World  
^CProcessed a total of 2 messages  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Another way of reading data from a Kafka topic is by simply using a Spring Boot application&lt;/strong&gt; (Here I call this project as SpringBootKafka).&lt;/p&gt;

&lt;p&gt;The following demonstrates how to receive messages from Kafka topic. First in this blog I create a Spring Kafka Consumer, which is able to listen the messages sent to a Kafka topic. Then in next blog I create a Spring Kafka Producer, which is able to send messages to a Kafka topic.&lt;/p&gt;

&lt;p&gt;The first step to create a simple Spring Boot maven Application is &lt;a href=&quot;https://spring.io/guides/gs/spring-boot/&quot;&gt;Starting with Spring Initializr&lt;/a&gt; and make sure to have spring-kafka dependency to &lt;code class=&quot;highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;  
  &amp;lt;groupId&amp;gt;org.springframework.kafka&amp;lt;/groupId&amp;gt;  
  &amp;lt;artifactId&amp;gt;spring-kafka&amp;lt;/artifactId&amp;gt;  
&amp;lt;/dependency&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;SpringBootKafkaApplication.java&lt;/code&gt; class (The Spring Initializr creates the following simple application class for you)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@SpringBootApplication  
public class SpringBootKafkaApplication {  
	public static void main(String[] args) {  
		SpringApplication.run(SpringBootApplication.class, args);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;In Spring Boot, properties are kept in the &lt;code class=&quot;highlighter-rouge&quot;&gt;application.properties&lt;/code&gt; file under the classpath. The application.properties file is located in the &lt;code class=&quot;highlighter-rouge&quot;&gt;src/main/resources&lt;/code&gt; directory. Change application.properties file to &lt;code class=&quot;highlighter-rouge&quot;&gt;application.yml&lt;/code&gt;, then add the following content.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spring:  
  kafka:  
consumer:  
  bootstrap-servers: localhost:9092  
  group-id: group_test1  
  auto-offset-reset: earliest  
  key-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
  value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Create a Spring Kafka Consumer class:&lt;/p&gt;

&lt;p&gt;Create a class called &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaConsumer.java&lt;/code&gt; and add a method with the @KakfaListener annotation.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Service  
public class KafkaConsumer {  
	@KafkaListener(id = &quot;group_test1&quot;, topics = &quot;test1&quot;)  
	public void consumeMessage(String message) {  
		System.out.println(&quot;Consumed message: &quot; + message);  
	}  
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;How to run Spring Boot web application in Eclipse?&lt;/p&gt;

&lt;p&gt;In eclipse Project Explorer, right click the project name -&amp;gt; select “Run As” -&amp;gt; “Maven Build…”&lt;br /&gt;
In the goals, enter &lt;code class=&quot;highlighter-rouge&quot;&gt;spring-boot:run&lt;/code&gt;&lt;br /&gt;
then click Run button.&lt;/p&gt;

&lt;p&gt;If you have Spring Tool Suite (STS) plug-in, you see a “Spring Boot App” option under Run As.&lt;/p&gt;

&lt;p&gt;Start Kafka and Zookeeper: &lt;code class=&quot;highlighter-rouge&quot;&gt;$docker-compose up -d&lt;/code&gt;&lt;br /&gt;
Produce message using the Kafka console producer: &lt;code class=&quot;highlighter-rouge&quot;&gt;$docker-compose exec kafka bash&lt;/code&gt;&lt;br /&gt;
Once inside the container &lt;code class=&quot;highlighter-rouge&quot;&gt;cd /opt/kafka/bin&lt;/code&gt;&lt;br /&gt;
Run the following console producer which will enable you to send messages to Kafka:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash-4.4# ./kafka-console-producer.sh --broker-list localhost:9092 --topic test1  
&amp;gt;Hello  
&amp;gt;World  
^C  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Try sending a few messages like above (Hello, World etc) and watch the application standard output in the Eclipse shell where you are running your Spring Boot application.&lt;/p&gt;

&lt;p&gt;Eclipse Console:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  .   ____          _            __ _ _  
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \  
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \  
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )  
  '  |____| .__|_| |_|_| |_\__, | / / / /  
 =========|_|==============|___/=/_/_/_/  

:: Spring Boot ::        (v2.2.4.RELEASE)  
2020-01-26 14:26:55.205  INFO 11137 --- [           main] c.e.d.SpringBootKafkaConsumerApplication : Starting   
…  
…  
…
o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=simpleconsumer] Setting newly assigned partitions: test1-0  
2020-01-26 14:26:56.384  INFO 11137 --- [econsumer-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-1, groupId=simpleconsumer] Found no committed offset for partition test1-0  
2020-01-26 14:26:56.408  INFO 11137 --- [econsumer-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-1, groupId=simpleconsumer] Resetting offset for partition test1-0 to offset 2.  
2020-01-26 14:26:56.477  INFO 11137 --- [econsumer-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : simpleconsumer: partitions assigned: [test1-0]  
Got message: hello  
Got message: world  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The following code demonstrates how to send and receive messages from Kafka topic. The above &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaConsumer.java&lt;/code&gt; receives messages that were sent to a Kafka topic. The followng &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaProducer.java&lt;/code&gt; send messages to a Kafka topic.&lt;/p&gt;

&lt;p&gt;Make sure to have spring-web dependency to &lt;code class=&quot;highlighter-rouge&quot;&gt;pom.xml&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;  
  &amp;lt;groupId&amp;gt;org.springframework.boot&amp;lt;/groupId&amp;gt;  
  &amp;lt;artifactId&amp;gt;spring-boot-starter-web&amp;lt;/artifactId&amp;gt;  
&amp;lt;/dependency&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Add two new java classes &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaController.java&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;KafkaProducer.java&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@RestController  
@RequestMapping(value=&quot;/kafka&quot;)  
public class KafkaController {  
	private final KafkaProducer producer;  

	@Autowired  
	KafkaController(KafkaProducer producer){  
		this.producer = producer;  
	}  

	@PostMapping(value=&quot;/publish&quot;)  
	public void messagePrint(@RequestParam(value=&quot;message&quot;, required = false) String message) {  
		this.producer.sendMessage(message);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@Service  
public class KafkaProducer {  
	private static final String TOPIC = &quot;test1&quot;;  

	@Autowired  
	private KafkaTemplate&amp;lt;String, String&amp;gt; kafkaTemplate;  
	
	public void sendMessage(String message) {  
		kafkaTemplate.send(TOPIC, message);  
		System.out.println(&quot;Produced message: &quot; + message);  
	}  
}  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Update &lt;code class=&quot;highlighter-rouge&quot;&gt;application.yml&lt;/code&gt; file.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;server:
  port: 8080
spring:
  kafka:
   consumer:
    bootstrap-servers: localhost:9092
    group-id: group_test1
    auto-offset-reset: earliest
    key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    value-deserializer: org.apache.kafka.common.serialization.StringDeserializer  
   producer:  
    bootstrap-servers: localhost:9092  
    key-serializer: org.apache.kafka.common.serialization.StringSerializer  
    value-serializer: org.apache.kafka.common.serialization.StringSerializer  
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Run Spring Boot web application (see How to run Spring Boot web application in Eclipse?)&lt;/p&gt;

&lt;p&gt;Make POST request using &lt;a href=&quot;https://www.getpostman.com&quot;&gt;Postman&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Select &lt;code class=&quot;highlighter-rouge&quot;&gt;POST&lt;/code&gt; and use the API &lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:8080/kafka/publish&lt;/code&gt;&lt;br /&gt;
&lt;strong&gt;Body&lt;/strong&gt;: form-data  &lt;strong&gt;KEY&lt;/strong&gt;: message  &lt;strong&gt;VALUE&lt;/strong&gt;: hello&lt;/p&gt;

&lt;p&gt;Finally click &lt;strong&gt;send&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;See Eclipse Console for messages:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;...  
...  
...  
2020-01-27 13:12:06.911  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 2.3.1  
2020-01-27 13:12:06.912  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 18a913733fb71c01  
2020-01-27 13:12:06.912  INFO 31822 --- [nio-8080-exec-2] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1580148726911  
2020-01-27 13:12:06.947  INFO 31822 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: 6R8O95IPSfGoifR4zzwM6g  
Produced message: hello  
Consumed message: hello
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="big data" /><category term="apache kafka" /><category term="real time data pipelines" /><category term="java" /><category term="docker" /><category term="Spring Boot" /><category term="YAML" /><category term="Zookeeper" /><summary type="html">Kafka is used for building real-time data pipelines and streaming apps.</summary></entry><entry><title type="html">Quantitative proteomics: TMT-based quantitation of proteins</title><link href="http://localhost:4000/posts/2020/01/blog-post-tmt/" rel="alternate" type="text/html" title="Quantitative proteomics: TMT-based quantitation of proteins" /><published>2020-01-09T00:00:00-08:00</published><updated>2020-01-09T00:00:00-08:00</updated><id>http://localhost:4000/posts/2020/01/blog-tmt</id><content type="html" xml:base="http://localhost:4000/posts/2020/01/blog-post-tmt/">&lt;p&gt;Here, I describe the general workflow of relative quantification of proteins using isobaric labeling (tandem mass tag or TMT). The methodology starts with the reduction of disulfide bonds in proteins with Dithiothreitol (DTT). Alkylation with iodoacetamide (IAA) after cystine reduction results in the covalent addition of a carbamidomethyl group that prevents the formation of disulfide bonds. Then, overnight digestion of the proteins using trypsin or trypsin/LyC mixture, Tandem Mass Tag (TMT) labeling on Lysine residues, pooling of all samples and the fractionation of peptide mixture. Finally, LC-MS/MS data acquisition and database search for protein identification and quantification.&lt;/p&gt;

&lt;p&gt;Fractionation prior to LC-MS/MS analysis effectively detects low abundance proteins (&amp;lt;100ng/mL), which is the concentration range of most clinical biomarkers.&lt;/p&gt;

&lt;p&gt;TMT-based approach allows multiplexing of samples. TMT 6-plex reagents produces a series of different reporter ions with nominal masses from 126 to 131 Da at 1 Da intervals. Several TMT reagents are commercially available including TMTzero, TMT duplex, TMT 6-plex, and TMT 10-plex &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. They have the same chemical structure but contain different numbers and combinations of 13C and 15N isotopes in the reporter group. The overall calibrated intensities of reporter ions equal.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Isobaric labeling reagent (TMT) structure&lt;/strong&gt;:&lt;br /&gt;
a. An amine-specific reactive group – an N-hydroxysuccinimide ester, which reacts with primary amines i.e unblocked N-terminals and lysine side chains.&lt;br /&gt;
b. A mass reporter group for quantification – the reporter groups are partially fragmented from the peptide during precursor fragmentation in the mass spectrometer.&lt;br /&gt;
c.A mass normalizer group to link the reactive and reporter groups – mass normalizer group ensures that the peptide complexity in the MS1 spectra does not increase with multiplexing.&lt;/p&gt;

&lt;p&gt;Unless otherwise noted, every analysis utilizes an MS3-based TMT-centric mass spectrometry method. MS2-based TMT yields the highest precision but lowest accuracy due to ratio compression, which MS3-based TMT can partly rescue &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. To 10 precursors selected for MS2/MS3 analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;What are the correction factors used for TMT?&lt;/strong&gt;&lt;br /&gt;
TMT reporter ion signals need to be adjusted to account for isotopic impurities in each TMT variant.  For TMT-10plex labeling, different batches of TMT reagents have slightly different isotope impurities that need to be included for database search to correct the reporter ion ratio. The information of isotope impurity can be found in the reagent kit. However, I do not normally correct for isotopic impurities of TMT reagents, as this is typically a «1.2% shift.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Database search for protein identification and quantification&lt;/strong&gt;:&lt;br /&gt;
The resulting TMT-MS3 data (.raw files) are processed using MaxQuant with an integrated Andromeda search engine (v.1.6.7.0). Tandem mass spectra were searched against the Uniprot database.&lt;/p&gt;

&lt;p&gt;Download and install  &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src/master/&quot;&gt;MaxQuant&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Specification of Various Parameters in MaxQuant (All the other parameters in MaxQuant are set to the default values for processing orbitrap-type data)&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Raw data&lt;/strong&gt; pane&lt;br /&gt;
Using Load, select all raw files from all batches. Update Experiment (a unique value for each of the samples) and Fraction (if available) column values using set experiment and set fractions buttons.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Group-specific parameters&lt;/strong&gt; pane&lt;br /&gt;
Change Type as Reporter ion MS3 and then select “10plex TMT”. Update isobaric impurities values, if available. For Modifications, select Variable modifications as Oxidation (M), Acetyl (Protein N-term); Deamidation (NQ) and Fixed Modifications as Carbamidomethyl© and, TMT labeled N- terminus and lysine residue. Specify Trypsin/P as a cleavage enzyme using Digestion and allow up to 2 missing cleavages. Set Label-free quantification as None.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Global parameters&lt;/strong&gt; pane&lt;br /&gt;
For Sequences select a reference proteome database Uniprot FASTA file and update Max. peptide mass [Da] as 6000. For Protein quantification select Use only unmodified peptides and a list of modifications such as Oxidation (M), Acetyl (Protein N-term) and Deamidation. Select Match between runs in Identification tab.&lt;/p&gt;

&lt;p&gt;Update the following two parameters for MS/MS analyzer:&lt;br /&gt;
a. FTMS MS/MS match tolerance: 0.05 Da&lt;br /&gt;
b.ITMS MS/MS match tolerance: 0.6 Da&lt;/p&gt;

&lt;p&gt;For Folder location select tmp directory (optional).&lt;/p&gt;

&lt;p&gt;Further data processing is performed using Perseus (version 1.6.7.0) &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. The search results in &lt;code class=&quot;highlighter-rouge&quot;&gt;ProteinGroups.txt&lt;/code&gt; generated by MaxQuant are directly processed by Perseus software. MaxQuant reports the TMT-MS3 quantitative relative abundance metrics in the columns titled “Reporter intensity corrected”.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Further reading&lt;/strong&gt;:&lt;br /&gt;
Data normalization and analysis in multiple TMT experimental designs &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;br /&gt;
Multiplexed Protein Quantification Using the Isobaric TMT … &lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;AWS Windows instance for &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src&quot;&gt;MaxQuant/Perseus&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;Bąchor, R.; Waliczek, M.; Stefanowicz, P.; Szewczuk, Z. Trends in the Design of New Isobaric Labeling Reagents for Quantitative Proteomics. Molecules 2019, 24, 701.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;A. Hogrebe, L. von Stechow, D.B. Bekker-Jensen, B.T. Weinert, C.D. Kelstrup, J.V. Olsen Benchmarking common quantification strategies for large-scale phosphoproteomics Nat. Commun., 9 (2018), p. 1045.&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=maxquant:start&quot;&gt;MaxQuant&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=perseus:start&quot;&gt;Perseus&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://pwilmart.github.io/TMT_analysis_examples/multiple_TMT_MQ.html&quot;&gt;TMT Batch Correction&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://assets.thermofisher.com/TFS-Assets/CMD/Reference-Materials/PP-TMT-Multiplexed-Protein-Quantification-HUPO2015-EN.pdf&quot;&gt;TMT analysis using Proteome Discoverer&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="Proteomics" /><category term="TMT" /><category term="tandem mass tag" /><category term="MS3 spectra" /><category term="MaxQuant" /><category term="isobaric labeling" /><category term="Perseus" /><category term="TMT 10plex" /><category term="10plex" /><summary type="html">Here, I describe the general workflow of relative quantification of proteins using isobaric labeling (tandem mass tag or TMT). The methodology starts with the reduction of disulfide bonds in proteins with Dithiothreitol (DTT). Alkylation with iodoacetamide (IAA) after cystine reduction results in the covalent addition of a carbamidomethyl group that prevents the formation of disulfide bonds. Then, overnight digestion of the proteins using trypsin or trypsin/LyC mixture, Tandem Mass Tag (TMT) labeling on Lysine residues, pooling of all samples and the fractionation of peptide mixture. Finally, LC-MS/MS data acquisition and database search for protein identification and quantification.</summary></entry><entry><title type="html">ATAC-seq peak calling with MACS2</title><link href="http://localhost:4000/posts/2019/12/blog-post-atacseq/" rel="alternate" type="text/html" title="ATAC-seq peak calling with MACS2" /><published>2019-12-05T00:00:00-08:00</published><updated>2019-12-05T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/12/blog-atacseq</id><content type="html" xml:base="http://localhost:4000/posts/2019/12/blog-post-atacseq/">&lt;p&gt;ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing) is a next-generation sequencing approach for the analysis of open chromatin regions to assess the genome-wise chromatin accessibility. ATAC-seq achieves this by simultaneously fragmenting and tagging genomic DNA with sequencing adapters using the hyperactive Tn5 transposase enzyme &lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Other global chromatin accessibility methods include FAIRE-seq and DNase-seq. This document aims to provide accessibility.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pre-processing of raw sequencing reads&lt;/strong&gt; – before mapping the raw reads to the genome, trim the adapter sequences. Poor read quality or sequencing
errors often lead to low mapping rate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mapping/alignment of sequencing reads to a reference genome&lt;/strong&gt; – use Burrows-Wheeler Aligner (BWA) for mapping of sequencing reads. The output alignment file will be saved as a sequence alignment/map (SAM) format or binary version of SAM called BAM. Mark the duplicate reads using Picard &lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and exclude reads mapping to mitochondrial DNA and other chromosomes from analysis together with low quality reads (MAPQ&amp;lt;10 and reads in Encode black list regions) using SAMtools &lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Filtering and shifting of the mapped reads&lt;/strong&gt; - shift the read position +4 and -5 bp in the BAM file before peak calling &lt;a href=&quot;https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/&quot;&gt;adjust the reads alignment&lt;/a&gt;. When the Tn5 transposase cuts open chromatin regions, it introduces two cuts that are separated by 9 bp. Therefore, ATAC-seq reads aligning to the positive and negative strands need to be adjusted by +4 bp and -5 bp respectively to represent the center of the transposase binding site. Picard CollectInsertSizeMetrics will be used to compute the fragment sizes on alignment shifted BAM files.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Identification and visualization of the ATAC-seq peaks&lt;/strong&gt; – use MACS2 for peak calling with the parameters nomodel or BAMPE &lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and identify the differentially enriched peaks using the MACS2 &lt;code class=&quot;highlighter-rouge&quot;&gt;bdgdiff&lt;/code&gt; module. Individual peaks separated by &amp;lt;100 bp will be join together. For peak annotation and functional analysis use the R package ChIPpeakAnno or HOMER &lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;,&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. First, ATAC-seq peaks will be categorized into different groups based on the nearest RefSeq gene i.e. promoter, untranslated regions (UTRs), intron and exon. Second, peaks that are within 5 kb upstream and 3 kb downstream of the Transcription Start Site (TSS) are associated to the nearest genes. Finally, these genes are then analyzed for over-represented gene ontology (GO) terms and KEGG pathways using ChIPpeakAnno. Visualize all sequencing tracks using the Integrated Genomic Viewer (IGV) &lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Scripts are available for HPC &lt;a href=&quot;https://bitbucket.org/adinasarapu/atac_seq-data-analysis/src/master/&quot;&gt;Cluster&lt;/a&gt;.&lt;br /&gt;
For further reading: &lt;a href=&quot;https://yiweiniu.github.io/blog/2019/03/ATAC-seq-data-analysis-from-FASTQ-to-peaks/&quot;&gt;ATAC-seq-data-analysis-from-FASTQ-to-peaks&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pubmed/24097267&quot;&gt;ATAC-seq&lt;/a&gt; - Nature Methods. 2013; 10:1213–1218.&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://broadinstitute.github.io/picard/&quot;&gt;PICARD&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.htslib.org&quot;&gt;HTSLIB&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/taoliu/MACS&quot;&gt;MACS&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-11-237&quot;&gt;ChIPpeakAnno&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://homer.ucsd.edu/homer/ngs/&quot;&gt;HOMER&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://software.broadinstitute.org/software/igv/home&quot;&gt;IGV&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing)" /><category term="MACS2" /><category term="BWA" /><category term="HOMER" /><summary type="html">ATAC-seq (Assay for Transposase Accessible Chromatin with high-throughput Sequencing) is a next-generation sequencing approach for the analysis of open chromatin regions to assess the genome-wise chromatin accessibility. ATAC-seq achieves this by simultaneously fragmenting and tagging genomic DNA with sequencing adapters using the hyperactive Tn5 transposase enzyme 1. Other global chromatin accessibility methods include FAIRE-seq and DNase-seq. This document aims to provide accessibility. ATAC-seq - Nature Methods. 2013; 10:1213–1218.&amp;nbsp;&amp;#8617;</summary></entry><entry><title type="html">Single cell RNA-seq data analysis using CellRanger and Seurat on Cluster</title><link href="http://localhost:4000/posts/2019/01/blog-post-sc-ranseq/" rel="alternate" type="text/html" title="Single cell RNA-seq data analysis using CellRanger and Seurat on Cluster" /><published>2019-11-18T00:00:00-08:00</published><updated>2019-11-18T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/01/blog-sc-rnaseq</id><content type="html" xml:base="http://localhost:4000/posts/2019/01/blog-post-sc-ranseq/">&lt;p&gt;Running &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger&quot;&gt;cellranger&lt;/a&gt; as cluster mode that uses Sun Grid Engine (SGE) as queuing system allows highly parallelizable jobs.&lt;/p&gt;

&lt;p&gt;There are 4 steps to analyze Chromium Single Cell data&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger mkfastq&lt;/code&gt; demultiplexes raw base call (BCL) files generated by Illumina sequencers into FASTQ files.&lt;br /&gt;
&lt;strong&gt;Step 2&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count&lt;/code&gt; takes FASTQ files from cellranger mkfastq and performs alignment, filtering, barcode counting, and UMI counting. When doing large studies involving multiple GEM wells, run cellranger count on FASTQ data from each of the GEM wells individually, and then pool the results using cellranger aggr, as described &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/aggregate&quot;&gt;here&lt;/a&gt;.&lt;br /&gt;
&lt;strong&gt;Step 3&lt;/strong&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger aggr&lt;/code&gt; aggregates outputs from multiple runs of cellranger count.&lt;br /&gt;
&lt;strong&gt;Step 4&lt;/strong&gt;: Downstream/Secondary analysis using R package &lt;a href=&quot;https://satijalab.org/seurat/&quot;&gt;Seurat v3.0&lt;/a&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Running pipelines on cluster requires the following:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1&lt;/strong&gt;. Load Cell Ranger module (&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger-3.1.0&lt;/code&gt;)&lt;sup id=&quot;fnref:1:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or, download and uncompress cellranger at your &lt;code class=&quot;highlighter-rouge&quot;&gt;$HOME&lt;/code&gt; directory and add PATH in &lt;code class=&quot;highlighter-rouge&quot;&gt;~/.bashrc&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2&lt;/strong&gt;. Update job config file (&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger-3.1.0/martian-cs/v3.2.3/jobmanagers/config.json&lt;/code&gt;) for threads and memory. For example&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;threads_per_job&quot;: 20,&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;memGB_per_job&quot;: 150,&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3&lt;/strong&gt;. Update template file (&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger-3.1.0/martian-cs/v3.2.3/jobmanagers/sge.template&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -pe smp __MRO_THREADS__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;##$ -l mem_free=__MRO_MEM_GB__G&lt;/code&gt; (comment this line if your cluster do not support it!)&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -q b.q&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -S /bin/bash&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -m abe&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;#$ -M &amp;lt;e-mail&amp;gt;&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;cd __MRO_JOB_WORKDIR__&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;source $HOME/cellranger-3.1.0/sourceme.bash&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For clusters whose job managers do not support memory requests, it is possible to request memory 
in the form of cores via the &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore&lt;/code&gt; command-line option. This option scales up the number 
of threads requested via the &lt;code class=&quot;highlighter-rouge&quot;&gt;__MRO_THREADS__&lt;/code&gt; variable according to how much memory a stage requires. 
see more at &lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/advanced/cluster-mode&quot;&gt;Cluster Mode&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4&lt;/strong&gt;. Download single cell gene expression and reference genome datasets from &lt;a href=&quot;https://www.10xgenomics.com/resources/datasets/&quot;&gt;10XGenomics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5&lt;/strong&gt;. Create &lt;code class=&quot;highlighter-rouge&quot;&gt;sge.sh&lt;/code&gt; file&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TR=&quot;$HOME/refdata-cellranger-GRCh38-3.0.0&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 3′ Gene expression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Output files will appear in the out/ subdirectory within this pipeline output directory.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cd $HOME/10xgenomics/out&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For pipeline output directory, the &lt;code class=&quot;highlighter-rouge&quot;&gt;--id&lt;/code&gt; argument is used i.e 10XGTX_v3.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/pbmc_10k_v3_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count --disable-ui \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=10XGTX_v3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--sample=pbmc_10k_v3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--expect-cells=10000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=5000 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 5′ gene expression&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FASTQS=&quot;$HOME/vdj_v1_hs_nsclc_5gex_fastqs&quot;&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-either-force-cells-or-expect-cells&quot;&gt;use either –force-cells or –expect-cells&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=10XGTX_v5 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--fastqs=${FASTQS} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--sample=vdj_v1_hs_nsclc_5gex \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--force-cells=7802 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3 \&lt;/code&gt;&lt;br /&gt;
  &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=2000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;for Single Cell 5′ gene expression and cell surface protein (Feature Barcoding/Antibody Capture Assay)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;LIBRARY=$HOME/vdj_v1_hs_pbmc2_5gex_protein_fastqs/vdj_v1_hs_pbmc2_5gex_protein_library.csv&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;FEATURE_REF=$HOME/vdj_v1_hs_pbmc2_5gex_protein_fastqs/vdj_v1_hs_pbmc2_5gex_protein_feature_ref.csv&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cellranger count \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--libraries=${LIBRARY} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--feature-ref=${FEATURE_REF} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--id=PBMC_5GEX \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--transcriptome=${TR} \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--expect-cells=9000 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobmode=sge \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--mempercore=8 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--maxjobs=3 \&lt;/code&gt;&lt;br /&gt;
 &lt;code class=&quot;highlighter-rouge&quot;&gt;--jobinterval=5000&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;6&lt;/strong&gt;. Execute a command in screen and, detach and reconnect&lt;/p&gt;

&lt;p&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;screen&lt;/code&gt; command to get in/out of the system while keeping the processes running.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;screen -S screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;bash sge.sh&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you want to exit the terminal without killing the running process, simply press &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl+A+D&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To reconnect to the screen: &lt;code class=&quot;highlighter-rouge&quot;&gt;screen -R screen_name&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7&lt;/strong&gt;. Monitor work progress through a web browser&lt;/p&gt;

&lt;p&gt;Open &lt;code class=&quot;highlighter-rouge&quot;&gt;_log&lt;/code&gt; file present in output folder &lt;code class=&quot;highlighter-rouge&quot;&gt;PBMC_5GEX&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If you see serving UI as &lt;code class=&quot;highlighter-rouge&quot;&gt;http://cluster.university.edu:3600?auth=rlSdT_QLzQ9O7fxEo-INTj1nQManinD21RzTAzkDVJ8&lt;/code&gt;, then type the following from your laptop&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ssh -NT -L 9000:cluster.university.edu:3600 user@cluster.university.edu&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;user@cluster.university.edu's password:&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Then access the UI using the following URL in your web browser
&lt;code class=&quot;highlighter-rouge&quot;&gt;http://localhost:9000/&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;8&lt;/strong&gt;. Single Cell Integration in Seurat v3.0&lt;/p&gt;

&lt;p&gt;Seurat is an R package designed for QC, analysis, and exploration of single cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single cell transcriptomic measurements, and to integrate diverse types of single cell data. Seurat starts by reading cellranger data (barcodes.tsv.gz, features.tsv.gz and matrix.mtx.gz)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pbmc.data &amp;lt;- Read10X(data.dir = &quot;~/PBMC_5GEX/outs/filtered_feature_bc_matrix/&quot;)&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://support.10xgenomics.com/single-cell-gene-expression/software/overview/welcome&quot;&gt;10XGenomics&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://satijalab.org/seurat/&quot;&gt;Seurat&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="10XGenomics" /><category term="Chromium Single Cell Gene Expression" /><category term="Single cell RNA-sequencing (scRNA-seq)" /><category term="Single Cell" /><category term="Cell Ranger" /><category term="Seurat" /><category term="Cluster Computing" /><category term="Feature Barcoding" /><summary type="html">Running cellranger as cluster mode that uses Sun Grid Engine (SGE) as queuing system allows highly parallelizable jobs.</summary></entry><entry><title type="html">Taxonomic and diversity profiling of the microbiome - 16S rRNA gene amplicon sequence data</title><link href="http://localhost:4000/posts/2019/01/blog-post-qiime2/" rel="alternate" type="text/html" title="Taxonomic and diversity profiling of the microbiome - 16S rRNA gene amplicon sequence data" /><published>2019-01-01T00:00:00-08:00</published><updated>2019-01-01T00:00:00-08:00</updated><id>http://localhost:4000/posts/2019/01/blog-qiime2</id><content type="html" xml:base="http://localhost:4000/posts/2019/01/blog-post-qiime2/">&lt;p&gt;The 16S ribosomal RNA (rRNA) gene of Bacteria codes for the RNA component of the 30S subunit. Different bacterial species have one to multiple copies of the 16S rRNA gene, and each with 9 hypervariable regions, V1-V9. High-throughput sequencing of 16S rRNA gene (a “marker gene”) amplicons has become a widely used method to study bacterial phylogeny and species classification.&lt;/p&gt;

&lt;p&gt;Quantitative Insights Into Microbial Ecology “QIIME” 2 (release 2018.11)&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; is a widely used package to identity abundance of microbes using 16s rRNA. Briefly, feature table containing counts of each unique sequence in the samples will be constructed using &lt;code class=&quot;highlighter-rouge&quot;&gt;qiime dada2 denoise-paired&lt;/code&gt; method. A feature is essentially any unit of observation, e.g., an OTU (Operational Taxonomic Unit), a sequence variant, a gene or a metabolite. In QIIME2 (currently), most features will be OTUs or sequence variants (alternatively, for OTUs, use QIIME2 plugin &lt;code class=&quot;highlighter-rouge&quot;&gt;q2-vsearch&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Data produced by QIIME 2 exist as QIIME 2 artifacts. A QIIME 2 artifact typically has the &lt;code class=&quot;highlighter-rouge&quot;&gt;.qza&lt;/code&gt; file extension when output data stored in a file. Visualizations are another type of data (&lt;code class=&quot;highlighter-rouge&quot;&gt;.qzv&lt;/code&gt; file extension) generated by QIIME 2, which can be viewed using a web interface &lt;a href=&quot;https://view.qiime2.org&quot;&gt;https://view.qiime2.org&lt;/a&gt; (at Firefox web browser) without requiring a QIIME installation. Since QIIME 2 works with artifacts instead of data files (e.g. FASTA files), we must create a QIIME 2 artifact by importing our &lt;code class=&quot;highlighter-rouge&quot;&gt;fastq.gz&lt;/code&gt; data files.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Scripts are available for AWS &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_qiime2/src&quot;&gt;Cloud&lt;/a&gt; and HPC &lt;a href=&quot;https://bitbucket.org/adinasarapu/cluster_qiime2/src&quot;&gt;Cluster&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://qiime2.org&quot;&gt;https://qiime2.org&lt;/a&gt;&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="microbiome" /><category term="computing" /><category term="qiime2" /><category term="16s rRNA amplicon" /><category term="DADA2" /><category term="OTU" /><summary type="html">The 16S ribosomal RNA (rRNA) gene of Bacteria codes for the RNA component of the 30S subunit. Different bacterial species have one to multiple copies of the 16S rRNA gene, and each with 9 hypervariable regions, V1-V9. High-throughput sequencing of 16S rRNA gene (a “marker gene”) amplicons has become a widely used method to study bacterial phylogeny and species classification.</summary></entry><entry><title type="html">Taxonomic and functional profiling of the microbiome - whole genome shotgun metagenomics</title><link href="http://localhost:4000/posts/2018/07/blog-post-metagenomics/" rel="alternate" type="text/html" title="Taxonomic and functional profiling of the microbiome - whole genome shotgun metagenomics" /><published>2018-07-27T00:00:00-07:00</published><updated>2018-07-27T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/07/blog-metagenomics</id><content type="html" xml:base="http://localhost:4000/posts/2018/07/blog-post-metagenomics/">&lt;p&gt;This workflow consists of taxonomic and functional profiling of shotgun metagenomics sequencing (MGS) reads using MetaPhlAn2 and HUMAnN2, respectively. To perform taxonomic (phyla, genera or species level) profiling of the MGS data, the MetaPhlAn2 pipeline was run on a high performance multicore cluster computing environment.&lt;/p&gt;

&lt;p&gt;MetaPhlAn2 provides microbial (bacterial, archaeal, viral, and eukaryotic) taxonomic profiling allowing the quantification of individual species across metagenomic samples. MetaPhlAn2 relies on ~1M unique clade-specific marker genes identified from ~17,000 reference genomes. Microbial reads, aligned by MetaPhlAn2, belonging to clades with no sequenced genomes available are reported as an “unclassified” subclade of the closest ancestor with available sequence data.
HUMAnN2 utilizes the MetaCyc database as well as the UniRef gene family catalog to characterize the microbial pathways present in samples. HUMAnN2 relies on programs such as BowTie (for accelerated nucleotide-level searches) and Diamond (for accelerated translated searches) to compute the abundance of gene families and metabolic pathways present. HUMAnN2 generates three outputs: 1) gene families based on UniRef proteins and their abundances reported in reads per kilobase, 2) MetaCyc pathways and their coverage, and 3) MetaCyc pathways and their abundances reported in reads per kilobase.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Scripts are available at &lt;a href=&quot;https://bitbucket.org/adinasarapu/shotgun_metagenomics/src&quot;&gt;shotgun_metagenomics&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="metagenomics" /><category term="MetaPhlAn2" /><category term="HUMAnN2" /><category term="taxonomic profiling" /><category term="functional profiling" /><category term="shotgun metagenomics sequencing" /><summary type="html">This workflow consists of taxonomic and functional profiling of shotgun metagenomics sequencing (MGS) reads using MetaPhlAn2 and HUMAnN2, respectively. To perform taxonomic (phyla, genera or species level) profiling of the MGS data, the MetaPhlAn2 pipeline was run on a high performance multicore cluster computing environment.</summary></entry><entry><title type="html">Quantitative proteomics: label-free quantitation of proteins</title><link href="http://localhost:4000/posts/2018/04/blog-post-lfq/" rel="alternate" type="text/html" title="Quantitative proteomics: label-free quantitation of proteins" /><published>2018-04-07T00:00:00-07:00</published><updated>2018-04-07T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/04/blog-lfq</id><content type="html" xml:base="http://localhost:4000/posts/2018/04/blog-post-lfq/">&lt;p&gt;Liquid chromatography (LC) coupled with mass spectrometry (MS) has been widely used for protein expression quantification. Protein quantification by tandem-MS (MS/MS) uses integrated peak intensity from the parent-ion mass (MS1) or features from fragment-ions (MS2). MS1 methods use the iBAQ (intensity Based Absolute Quantification) algorithm (a protein’s total non-normalised intensities are divided by the number of measurable tryptic peptides). Untargeted label-free quantitation (LFQ) of proteins, aims to determine the relative amount of proteins in two or more biological samples.&lt;/p&gt;

&lt;p&gt;Mass spectrometer generated &lt;code class=&quot;highlighter-rouge&quot;&gt;raw&lt;/code&gt; files are used for label-free quantitation of proteins. Base peak chromatograms are inspected visually using &lt;code class=&quot;highlighter-rouge&quot;&gt;RawMeat&lt;/code&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, which  is a data quality assessment tool designed for Thermo instruments. All raw files are processed together in a single run by &lt;code class=&quot;highlighter-rouge&quot;&gt;MaxQuant&lt;/code&gt; (version 1.6.0.16)&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; with default parameters. Database searches are performed using the &lt;code class=&quot;highlighter-rouge&quot;&gt;Andromeda&lt;/code&gt; search engine (a peptide search engine based on probabilistic scoring) with the UniProt-SwissProt human canonical database as a reference and a contaminants database of common laboratory contaminants. MaxQuant reports summed intensity for each protein, as well as its iBAQ value. Proteins that share all identified peptides are combined into a single protein group. Peptides that match multiple protein groups (“razor” peptides) are assigned to the protein group with the most unique peptides. MaxQuant employs the MaxLFQ algorithm for label-free quantitation (LFQ). Quantification will be performed using razor and unique peptides, including those modified by acetylation (protein N-terminal), oxidation (Met) and deamidation (NQ). &lt;code class=&quot;highlighter-rouge&quot;&gt;PTXQC&lt;/code&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; is used for general quality control of proteomics data, which takes MaxQuant result files.&lt;/p&gt;

&lt;p&gt;Data processing is performed using &lt;code class=&quot;highlighter-rouge&quot;&gt;Perseus&lt;/code&gt; (version 1.5.0.31)&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. In brief, protein group LFQ intensities are log2-transformed to reduce the effect of outliers. To overcome the obstacle of missing LFQ values, missing values are imputed before fit the models. Hierarchical clustering is performed on Z-score normalized, log2-transformed LFQ intensities. Log ratios are calculated as the difference in average log2 LFQ intensity values between experimental and control groups. Two-tailed, Student’s t test calculations are used in statistical tests. A protein is considered statistically significant if its fold change is ≥ 2 and FDR ≤ 0.01. All the identified differentially expressed proteins are used in protein network or pathway analysis. In addition to the above analytical considerations, good experimental design helps effectively identify true differences in the presence of variability from various sources and also avoids bias during data acquisition.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;AWS Windows instance for &lt;a href=&quot;https://bitbucket.org/adinasarapu/aws_maxquant_persues/src&quot;&gt;MaxQuant/Perseus&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proteomicsresource.washington.edu/protocols06/&quot;&gt;RawMeat&lt;/a&gt; is a nice Thermo raw file diagnostic tool developed by the now defunct Vast Scientific&amp;nbsp;&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=maxquant:start&quot;&gt;MaxQuant&lt;/a&gt; is a quantitative proteomics software package designed for analyzing large mass-spectrometric data sets&amp;nbsp;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://github.com/cbielow/PTXQC&quot;&gt;PTXQC&lt;/a&gt;, an R-based quality control pipeline called Proteomics Quality Control&amp;nbsp;&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.coxdocs.org/doku.php?id=perseus:start&quot;&gt;Perseus&lt;/a&gt; is software package for shotgun proteomics data analyses&amp;nbsp;&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="Proteomics" /><category term="RawMeat" /><category term="LFQ" /><category term="iBAC" /><category term="MaxQuant" /><category term="PTXQC" /><category term="Perseus" /><summary type="html">Liquid chromatography (LC) coupled with mass spectrometry (MS) has been widely used for protein expression quantification. Protein quantification by tandem-MS (MS/MS) uses integrated peak intensity from the parent-ion mass (MS1) or features from fragment-ions (MS2). MS1 methods use the iBAQ (intensity Based Absolute Quantification) algorithm (a protein’s total non-normalised intensities are divided by the number of measurable tryptic peptides). Untargeted label-free quantitation (LFQ) of proteins, aims to determine the relative amount of proteins in two or more biological samples.</summary></entry><entry><title type="html">RNA-Seq: raw reads to differential expression</title><link href="http://localhost:4000/posts/2018/03/blog-post-rnaseq/" rel="alternate" type="text/html" title="RNA-Seq: raw reads to differential expression" /><published>2018-03-20T00:00:00-07:00</published><updated>2018-03-20T00:00:00-07:00</updated><id>http://localhost:4000/posts/2018/03/blog-rnaseq</id><content type="html" xml:base="http://localhost:4000/posts/2018/03/blog-post-rnaseq/">&lt;p&gt;A simple RNA-Seq differential expression analysis using High Performance Computing (HPC).&lt;/p&gt;

&lt;p&gt;The goal is to analyze RNA-Seq data using HPC. The software which will be used in this session is listed below.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.bioinformatics.babraham.ac.uk/projects/fastqc/&quot;&gt;FastQC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;STAR&lt;/li&gt;
  &lt;li&gt;HTSeq&lt;/li&gt;
  &lt;li&gt;EdgeR&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This pipeline is available as &lt;a href=&quot;https://bitbucket.org/adinasarapu/ibs_class/src&quot;&gt;RNA-Seq Exercise&lt;/a&gt;&lt;/p&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="RNASeq" /><category term="RNA-Seq" /><category term="FastQC" /><category term="STAR" /><category term="HTSeq" /><category term="edgeR" /><summary type="html">A simple RNA-Seq differential expression analysis using High Performance Computing (HPC).</summary></entry><entry><title type="html">Genomic variants from RNA-Seq data</title><link href="http://localhost:4000/posts/2018/01/blog-post-gatk/" rel="alternate" type="text/html" title="Genomic variants from RNA-Seq data" /><published>2018-01-05T00:00:00-08:00</published><updated>2018-01-05T00:00:00-08:00</updated><id>http://localhost:4000/posts/2018/01/blog-gatk</id><content type="html" xml:base="http://localhost:4000/posts/2018/01/blog-post-gatk/">&lt;p&gt;RNA-Seq allows the detection and quantification of known and rare RNA transcripts within a sample. In addition to differential expression and detection of novel transcripts, RNA-seq also supports the detection of genomic variation in expressed regions.&lt;/p&gt;

&lt;p&gt;Currently few workflows exist for detecting SNPs in RNA-seq data, including &lt;a href=&quot;http://bioinformaticstools.mayo.edu/research/esnv-detect/&quot;&gt;eSNV-detect&lt;/a&gt;, &lt;a href=&quot;https://github.com/rpiskol/SNPiR&quot;&gt;SNPiR&lt;/a&gt; and &lt;a href=&quot;https://github.com/pysam-developers/pysam&quot;&gt;Opossum&lt;/a&gt;. Here, I have employed &lt;a href=&quot;https://software.broadinstitute.org/gatk/documentation/article.php?id=3891&quot;&gt;GATK workflow for SNP and indel calling on RNAseq data&lt;/a&gt;, which is based on the following steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Reference (hg38) based read mapping using &lt;code class=&quot;highlighter-rouge&quot;&gt;STAR&lt;/code&gt; aligner. This is a 2-pass approach with the suggested parameters. In this STAR 2-pass approach, splice junctions detected in a first alignment run are used to guide the final alignment (reads which have been mapped across splice junctions must be split to remove intronic parts).&lt;/li&gt;
  &lt;li&gt;Add read group information, sort, mark the duplicates and index with &lt;code class=&quot;highlighter-rouge&quot;&gt;picard.jar&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;GATK’s &lt;code class=&quot;highlighter-rouge&quot;&gt;SplitNCigarReads&lt;/code&gt; split the reads into exon segments (removing Ns but maintaining grouping information) and reassigning mapping qualities.&lt;/li&gt;
  &lt;li&gt;Indel realignment and recalibration of Base qualities and&lt;/li&gt;
  &lt;li&gt;Variant calling with GATK’s &lt;code class=&quot;highlighter-rouge&quot;&gt;HaplotypeCaller&lt;/code&gt;, and finally filtering the variants with GATK’s &lt;code class=&quot;highlighter-rouge&quot;&gt;VariantFiltration&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My qsub-based pipeline is available at &lt;a href=&quot;https://bitbucket.org/adinasarapu/clustercomputing/src/3e9a2b3881ea0e2afbe58df325ca693ecfac4fbc/job_rnaseq_variant_caller.sh&quot;&gt;bitbucket.org&lt;/a&gt;&lt;/p&gt;</content><author><name>Scientist, Bioinformatics</name><email>ashok.reddy.dinasarapu@emory.edu</email></author><category term="GATK" /><category term="RNA-Seq" /><category term="STAR" /><category term="SNP" /><category term="Indel" /><summary type="html">RNA-Seq allows the detection and quantification of known and rare RNA transcripts within a sample. In addition to differential expression and detection of novel transcripts, RNA-seq also supports the detection of genomic variation in expressed regions.</summary></entry></feed>